[
  {
    "objectID": "why_quarto.html",
    "href": "why_quarto.html",
    "title": "Why Quarto?",
    "section": "",
    "text": "As described in the quarto documentation: Quarto is a new, open-source, scientific, and technical publishing system. It is a multi-language, next generation version of R Markdown from RStudio, with many new features and capabilities. Like R Markdown, Quarto uses Knitr to execute R code, and is therefore able to render most existing Rmd files without modification.\nData scientists are pretty excited about the introduction of Quarto, and since it represents the future of R Markdown, we will conduct SDS 264 using Quarto. Intriguing Quarto features that have been cited include:\nFrom Posit and the developers of Rstudio and Quarto https://charlotte.quarto.pub/cascadia/, Quarto (compared to RMarkdown):\nIn other words, Quarto unifies and extends R Markdown:\nHere’s a cool example from the Quarto documentation, showing features like cross-referencing of figures, chunk options using the hash-pipe format, collapsed code, and easy figure legends:",
    "crumbs": [
      "Why Quarto?"
    ]
  },
  {
    "objectID": "why_quarto.html#air-quality",
    "href": "why_quarto.html#air-quality",
    "title": "Why Quarto?",
    "section": "Air Quality",
    "text": "Air Quality\nFigure 1 further explores the impact of temperature on ozone level.\n\n\nCode\nlibrary(ggplot2)\n\nggplot(airquality, aes(Temp, Ozone)) + \n  geom_point() + \n  geom_smooth(method = \"loess\")\n\n\n\n\n\n\n\n\nFigure 1: Temperature and ozone level.",
    "crumbs": [
      "Why Quarto?"
    ]
  },
  {
    "objectID": "rtipoftheday.html",
    "href": "rtipoftheday.html",
    "title": "R Tip of the Day",
    "section": "",
    "text": "Signup Sheet\nCool presentations using Quarto\nRTD rubric"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MSCS 264: Data Science 2 (Fall 2024)",
    "section": "",
    "text": "Quarto\n\n\n\n\n\n\n\ntidyverse\n\n\n\n\n\n\nKey links for SDS 264\n\nCourse syllabus\nRStudio server\nmoodle\nGitHub source code for this website"
  },
  {
    "objectID": "github_intro.html",
    "href": "github_intro.html",
    "title": "Intro to GitHub",
    "section": "",
    "text": "Version Control (GitHub)\nIn order to collaborate on an R project (or any coding project in general), data scientists typically use a version control system like GitHub. With GitHub, you never have to save files as Final.docx, Final2.docx, Newfinal.docx, RealFinal.docx, nothisisreallyit.docx, etc. You update code like .qmd and .Rmd and data files, recording descriptions of any changes. Then if you ever want to go back to an earlier version, GitHub can facilitate that. Or if you want to make your work public, others can see it and even suggest changes, but you are ultimately in control of any changes that get made.\nAnd, you can have multiple collaborators with access to the same set of files. While it can be dicey if multiple people have the file open and make changes at the same time; if you do this with GitHub, it is at least POSSIBLE to get it straightened out, and the second person who tries to save will get warned. If you are both just using a common folder on RStudio, you can easily write over and erase each other’s work. (If you use a common folder, be sure that one person is editing at a time to prevent this).\nIn order to begin to get familiar with GitHub, we will use it to create a course folder for you.\n\n\nGetting started on GitHub and connecting to RStudio\n\nCreate a GitHub account at github.com. It’s usually okay to hit “Skip Personalization” at the bottom of the screen after entering an email, username, and password (you might have to enable 2-factor authentication as well). There are a few bonuses you can get as a student that you might consider.\n\nYou may choose to use a non-St. Olaf email address to ensure you’ll have access to your GitHub account after you graduate.\n\nObtain a personal access token (PAT) in GitHub using the following steps:\n\nClick your profile picture/symbol in the upper right of your GitHub page. Then select Settings &gt; Developer settings &gt; Personal access tokens &gt; Tokens (classic).\nClick “Generate new token (classic)” and give a descriptive name like “My PAT for RStudio”. Note that the default expiration is 30 days; I did a Custom setting through the end of the semester.\nSelect scopes and permissions; I often select: repo, workflow, gist, and user.\nClick “Generate token”. Copy your personal access token and store it somewhere.\n\nStore your credentials in RStudio using the following steps:\n\nIn the console, type library(credentials). You might have to install the credentials package first.\nThen type set_github_pat() and hit Return. You can sign in with the browser, or you can choose the Token option, where you can copy in your personal access token\n\n\nAlert! If the steps in (3) don’t work, you may have to install Git on your computer first. This chapter in “Happy Git with R” provides nice guidance for installing Git. Installing Git for Windows seems to work well on most Windows machines, using this site seems to work well for macOS, and a command like sudo apt-get install git can often work nicely in Linux. Once Git is installed, restart RStudio, and it will usually magically find Git. If not, there’s some good advice in this chapter of “Happy Git with R”. If you ever get frustrated with Git, remember that No one is giving out Git Nerd merit badges! Just muddle through until you figure out something that works for you!\n\n\nCreating an R project (local) that’s connected to GitHub (cloud)\n\nIn your GitHub account, click the \\(+ \\nabla\\) (+down arrow) button near the top right and select New Repository (repo). Put something like “SDS264_F24” for your repository (repo) name; use simple but descriptive names that avoid spaces. Check Private for now; you can turn a repository Public if you want later. Check Add a ReadMe File. Finally hit Create Repository and copy the URL once your repo has been created; the URL should be something like github.com/username/SDS264_F24.\nGo into your RStudio and select File &gt; New Project &gt; Version Control &gt; Git. For the repository URL paste in the URL for the repository you just created. A project directory named “SDS264_F24” will be created in your specified folder (which you can navigate to).\n\nNotice that you are now starting with a blank slate! Nothing in the environment or history. Also note where it says your project name in the top right corner.\nAt this point your should have a GitHub repo called “SDS264_F24” connected to an R project named “SDS264_F24”. The basic framework is set!\nHere is an illustration (source) of the process of using GitHub to manage version control and collaboration, followed by greater detail about each step:\n\n\n\nCreating new files in RStudio (local)\n\nYou can download our first file with in-class exercises here. Just hit the Download Raw File button and note where the file is saved on your computer. Use File &gt; Open File in RStudio to open up 01_review164.qmd. Then, use File &gt; Save As to navigate to the SDS264_F24 folder on your computer and save a copy there. You can even add your name or a few answers and re-save your file.\n\n\n\nMaking sure the connection to GitHub is ready\n\n[If necessary] You may need one of these two steps when using GitHub for the first name in a new R Project, even though you likely did them while installing GitHub.\n\nIn the console, type library(credentials). Then type set_github_pat(), hit Return, and copy in your personal access token.\nIn the Terminal window (this should be the tab next to Console), type the following two lines (be precise with the dashes and spaces!!):\n\n\n\ngit config --global user.name \"YOUR USER NAME\"\n\n\ngit config --global user.email \"YOUR EMAIL ASSOCIATED WITH GITHUB\"\n\n\n\nPushing your work to GitHub (cloud)\n\nWe now want to “push” the changes made in 01_review164.qmd to your GitHub repo (the changes have only been made in your local RStudio for now).\n\nunder the Git tab in the Environment panel, check the boxes in front of all modified files to “stage” your changes. To select a large number of files, check the top box, scroll down to the bottom of the list, and then shift-click the final box\nclick the Commit tab to “commit” your changes (like saving a file locally) along with a message describing the changes you made. GitHub guides you by by showing your old code (red) and new code (green), to make sure you approve of the changes. This first time, all code should be green since we’ve only added new things rather than modifying previously saved files.\n“push” your changes to GitHub (an external website) by clicking the green Up arrow. Refresh your GitHub account to see new files in the user_name.github.io repo!\n\n\n\n\nModifying files that have already been pushed to GitHub\n\nMake and save a change (anything) to your file 01_review164.qmd in RStudio. Now go back under the Git tab and “push” these new changes to GitHub. You’ll have to go through the same process of Stage, Commit, and Push, although this time you’ll see only your newest changes in green when you Commit. Confirm that your changes appear in GitHub.\n\n\n\nPulling work from GitHub\nBefore you start a new session of working on a project in RStudio, you should always Pull changes from GitHub first. Most of the time there will be nothing new, but if a collaborator made changes since the last time you worked on a file, you want to make sure you’re working with the latest and greatest version. If not, you’ll end up trying to Push changes made to an old version, and GitHub will balk and produce Merge Conflict messages. We’ll see how to handle Merge Conflicts later, but it’s a bit of a pain and best avoided!\n\nGo into 01_review164.qmd on GitHub and hit the Edit icon. Add a line anywhere, and then scroll down to hit Commit Changes. (This is not recommended and for illustrative purposes only! You will likely never edit directly in GitHub, but we’re emulating what might happen if a collaborator makes changes since the last time you worked on a document.) Now go back to RStudio and, under the Git tab, “Pull” the changes from GitHub into your R project folder. (Use the blue Down arrow). Confirm that your changes now appear in RStudio. Before you start working on the R server, you should always Pull any changes that might have been made on GitHub (especially if you’re working on a team!), since things can get dicey if you try to merge new changes from RStudio with new changes on GitHub.\n\n\n\nA bit more about R projects\n\nTo see the power of projects, select File &gt; Close Project and Don’t Save the workspace image. Then, select File &gt; Recent Projects &gt; SDS264_F24; you will get a clean Environment and Console once again, but History shows the commands you ran by hand, active Rmd and qmd files appear in the Source panel, and Files contains the Rmd, qmd, html, and csv files produced by your last session. And you can stage, commit and push the changes and the new file to GitHub!",
    "crumbs": [
      "Intro to GitHub"
    ]
  },
  {
    "objectID": "05_iteration.html",
    "href": "05_iteration.html",
    "title": "Iteration",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\nThis leans on parts of R4DS Chapter 26: Iteration, in addition to parts of the first edition of R4DS.\n# Initial packages required\nlibrary(tidyverse)",
    "crumbs": [
      "Iteration"
    ]
  },
  {
    "objectID": "05_iteration.html#iteration",
    "href": "05_iteration.html#iteration",
    "title": "Iteration",
    "section": "Iteration",
    "text": "Iteration\nReducing duplication of code will reduce errors and make debugging much easier. We’ve already seen how functions (Ch 25) can help reduce duplication by extracting repeated patterns of code. Another tool is iteration, when you find you’re doing the same thing to multiple inputs – repeating the same operation on different columns or datasets.\nHere we’ll see two important iteration paradigms: imperative programming and functional programming.",
    "crumbs": [
      "Iteration"
    ]
  },
  {
    "objectID": "05_iteration.html#imperation-programming-for-iteration",
    "href": "05_iteration.html#imperation-programming-for-iteration",
    "title": "Iteration",
    "section": "Imperation programming for iteration",
    "text": "Imperation programming for iteration\nExamples: for loops and while loops\nPros: relatively easy to learn, make iteration very explicit so it’s obvious what’s happening, not as inefficient as some people believe\nCons: require lots of bookkeeping code that’s duplicated for every loop\nEvery for loop has three components:\n\noutput - plan ahead and allocate enough space for output\nsequence - determines what to loop over; cycles through different values of \\(i\\)\nbody - code that does the work; run repeatedly with different values of \\(i\\)\n\n\ndf &lt;- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\ndf\n\n# A tibble: 10 × 4\n        a       b      c        d\n    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1  0.605 -0.313   1.61   0.471  \n 2  0.192 -0.655   1.39   1.21   \n 3  0.274  0.0471 -0.958 -0.688  \n 4 -0.739  0.751   2.87   0.619  \n 5  0.446 -0.0546 -1.00  -0.00687\n 6 -1.35   1.43    0.180  2.21   \n 7 -0.262 -2.16    1.16  -1.58   \n 8 -0.248 -0.473   0.614  0.121  \n 9  0.536  0.0188  1.07  -1.59   \n10  0.974 -1.63    0.513 -1.42   \n\n# want median of each column (w/o cutting and pasting)\n#   Be careful using square brackets vs double square brackets when\n#   selecting elements\nmedian(df[[1]])\n\n[1] 0.233093\n\nmedian(df[1])\n\nError in median.default(df[1]): need numeric data\n\ndf[1]\n\n# A tibble: 10 × 1\n        a\n    &lt;dbl&gt;\n 1  0.605\n 2  0.192\n 3  0.274\n 4 -0.739\n 5  0.446\n 6 -1.35 \n 7 -0.262\n 8 -0.248\n 9  0.536\n10  0.974\n\ndf[[1]]\n\n [1]  0.6051526  0.1920690  0.2741170 -0.7388195  0.4463717 -1.3505509\n [7] -0.2620253 -0.2481437  0.5358468  0.9743978\n\nclass(df[1])\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nclass(df[[1]])\n\n[1] \"numeric\"\n\n# basic for loop to take median of each column\noutput &lt;- vector(\"double\", ncol(df))  # 1. output\nfor (i in 1:4) {                      # 2. sequence\n  output[[i]] &lt;- median(df[[i]])      # 3. body\n}\noutput\n\n[1]  0.23309303 -0.18358778  0.84029064  0.05690053\n\n# ?seq_along - a safer option if had zero length vectors\noutput &lt;- vector(\"double\", ncol(df))  # 1. output\nfor (i in seq_along(df)) {            # 2. sequence\n  output[[i]] &lt;- median(df[[i]])      # 3. body\n}\noutput\n\n[1]  0.23309303 -0.18358778  0.84029064  0.05690053\n\n# use [[.]] even if don't have to to signal working with single elements\n\n# alternative solution - don't hardcode in \"4\"\noutput &lt;- vector(\"double\", ncol(df))  # 1. output\nfor(i in 1:ncol(df)) {                # 2. sequence\n  output[i] &lt;- median(df[[i]])        # 3. body\n}\noutput\n\n[1]  0.23309303 -0.18358778  0.84029064  0.05690053\n\n# another approach - no double square brackets since df not a tibble\ndf &lt;- as.data.frame(df)\noutput &lt;- vector(\"double\", ncol(df))  # 1. output\nfor(i in 1:ncol(df)) {                # 2. sequence\n  output[i] &lt;- median(df[,i])         # 3. body\n}\noutput\n\n[1]  0.23309303 -0.18358778  0.84029064  0.05690053\n\n\nOne advantage of seq_along(): works with unknown output length. However, the second approach below is much more efficient, since each iteration doesn’t copy all data from previous iterations.\n[Pause to Ponder:] What does the code below do? Be prepared to explain both chunks line-by-line!\n\n# for loop: unknown output length\n\nmeans &lt;- c(0, 1, 2)\noutput &lt;- double()\nfor (i in seq_along(means)) {\n  n &lt;- sample(100, 1)\n  output &lt;- c(output, rnorm(n, means[[i]]))\n}\nstr(output)        ## inefficient\n\n num [1:142] 0.0162 -1.5003 -2.7678 -1.3815 -1.5616 ...\n\nout &lt;- vector(\"list\", length(means))\nfor (i in seq_along(means)) {\n  n &lt;- sample(100, 1)\n  out[[i]] &lt;- rnorm(n, means[[i]])\n}\nstr(out)           ## more efficient\n\nList of 3\n $ : num [1:73] 1.991 1.136 -0.219 -0.348 1.951 ...\n $ : num [1:5] 1.27 2.06 3.06 2.05 1.11\n $ : num [1:91] 2.69 1.37 2.24 1.67 2.07 ...\n\nstr(unlist(out))   ## flatten list of vectors into single vector\n\n num [1:169] 1.991 1.136 -0.219 -0.348 1.951 ...\n\n\nFinally, the while() loop can be used with unknown sequence length. This is used more in simulation than in data analysis.\n[Pause to Ponder:] What does the following code do?\n\nflip &lt;- function() sample(c(\"T\", \"H\"), 1)\nflips &lt;- 0\nnheads &lt;- 0\nwhile (nheads &lt; 3) {\n  if (flip() == \"H\") {\n    nheads &lt;- nheads + 1\n  } else {\n    nheads &lt;- 0\n  }\n  flips &lt;- flips + 1\n}\nflips\n\n[1] 9",
    "crumbs": [
      "Iteration"
    ]
  },
  {
    "objectID": "05_iteration.html#using-iteration-for-simulation",
    "href": "05_iteration.html#using-iteration-for-simulation",
    "title": "Iteration",
    "section": "Using iteration for simulation",
    "text": "Using iteration for simulation\nThis applet contains data from a 2005 study on the use of dolphin-facilitated therapy on the treatment of depression. In that study, 10 of the 15 subjects (67%) assigned to dolphin therapy showed improvement, compared to only 3 of the 15 subjects (20%) assigned to the control group. But with such small sample sizes, is this significant evidence that the dolphin group had greater improvement of their depressive symptoms? To answer that question, we can use simulation to conduct a randomization test.\nWe will simulate behavior in the “null world” where there is no real effect of treatment. In that case, the 13 total improvers would have improved no matter the treatment assigned, and the 17 total non-improvers would have not improved no matter the treatment assigned. So in the “null world”, treatment is a meaningless label that can be just as easily shuffled among subjects without any effect. In that world, the fact we observed a 47 percentage point difference in success rates (67 - 20) was just random luck. But we should ask: how often would we expect a difference as large as 47% by chance, assuming we’re living in the null world where there is no effect of treatment?\nYou could think about simulating this situation with the following steps:\n\nwrite code to calculate the difference in success rates in the observed data\nwrite a loop to calculate the differences in success rates from 1000 simulated data sets from the null world. Store those 1000 simulated differences\ncalculate how often we found a difference in the null world as large as that found in the observed data. In statistics, when this probability is below .05, we typically reject the null world, and conclude that there is likely a real difference between the two groups (i.e. a “statistically significant” difference)\n\n[Pause to Ponder:] Fill in Step 2 in the second R chunk below to carry out the three steps above. (The first R chunk provides some preliminary code.) Then describe what you can conclude from this study based on your plot and “p_value” from Step 3.\n\n### Preliminary code ###\n\n# generate a tibble with our observed data\ndolphin_data &lt;- tibble(treatment = rep(c(\"Dolphin\", \"Control\"), each = 15),\n                       improve = c(rep(\"Yes\", 10), rep(\"No\", 5), \n                                   rep(\"Yes\", 3), rep(\"No\", 12)))\nprint(dolphin_data, n = Inf)\n\n# A tibble: 30 × 2\n   treatment improve\n   &lt;chr&gt;     &lt;chr&gt;  \n 1 Dolphin   Yes    \n 2 Dolphin   Yes    \n 3 Dolphin   Yes    \n 4 Dolphin   Yes    \n 5 Dolphin   Yes    \n 6 Dolphin   Yes    \n 7 Dolphin   Yes    \n 8 Dolphin   Yes    \n 9 Dolphin   Yes    \n10 Dolphin   Yes    \n11 Dolphin   No     \n12 Dolphin   No     \n13 Dolphin   No     \n14 Dolphin   No     \n15 Dolphin   No     \n16 Control   Yes    \n17 Control   Yes    \n18 Control   Yes    \n19 Control   No     \n20 Control   No     \n21 Control   No     \n22 Control   No     \n23 Control   No     \n24 Control   No     \n25 Control   No     \n26 Control   No     \n27 Control   No     \n28 Control   No     \n29 Control   No     \n30 Control   No     \n\n# `sample()` can be used to shuffle the treatments among the 30 subjects\nsample(dolphin_data$treatment)\n\n [1] \"Dolphin\" \"Control\" \"Dolphin\" \"Control\" \"Control\" \"Control\" \"Dolphin\"\n [8] \"Control\" \"Dolphin\" \"Dolphin\" \"Control\" \"Dolphin\" \"Dolphin\" \"Control\"\n[15] \"Control\" \"Dolphin\" \"Dolphin\" \"Control\" \"Control\" \"Dolphin\" \"Dolphin\"\n[22] \"Control\" \"Control\" \"Dolphin\" \"Dolphin\" \"Dolphin\" \"Control\" \"Control\"\n[29] \"Dolphin\" \"Control\"\n\n\n\n### Fill in Step 2 and remove \"eval: FALSE\" ###\n\n# Step 1\ndolphin_summary &lt;- dolphin_data |&gt;\n  group_by(treatment) |&gt;\n  summarize(prop_yes = mean(improve == \"Yes\"))\ndolphin_summary\nobserved_diff &lt;- dolphin_summary[[2]][2] - dolphin_summary[[2]][1]\n\n# Step 2\n\n### Write a loop to create 1000 simulated differences from the null world\n\n# Step 3\nnull_world &lt;- tibble(simulated_diffs = simulated_diffs)\nggplot(null_world, aes(x = simulated_diffs)) +\n  geom_histogram() +\n  geom_vline(xintercept = observed_diff, color = \"red\")\n\np_value &lt;- sum(abs(simulated_diffs) &gt;= abs(observed_diff)) / 1000\np_value\n\nYou have written code to conduct a randomization test for the difference in two proportions, a powerful test of statistical significance that is demonstrated in the original applet!",
    "crumbs": [
      "Iteration"
    ]
  },
  {
    "objectID": "05_iteration.html#functional-programming-for-iteration",
    "href": "05_iteration.html#functional-programming-for-iteration",
    "title": "Iteration",
    "section": "Functional programming for iteration",
    "text": "Functional programming for iteration\nExamples: map functions and across()\nPros: less code, fewer errors, code that’s easier to read; takes advantage of fact that R is a functional programming language\nCons: little more complicated to master vocabulary and use – a step up in abstraction\nR is a functional programming language. This means that it’s possible to wrap up for loops in a function, and call that function instead of using the for loop directly. Passing one function to another is a very powerful coding approach!!\n\n# Below you can avoid writing separate functions for mean, median, \n#   SD, etc. by column\ndf &lt;- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\ncol_summary &lt;- function(df, fun) {\n  out &lt;- vector(\"double\", length(df))\n  for (i in seq_along(df)) {\n    out[i] &lt;- fun(df[[i]])\n  }\n  out\n}\ncol_summary(df, median)\n\n[1]  0.53670886 -0.92232782  0.30365532  0.08235539\n\ncol_summary(df, mean)\n\n[1]  0.3673695 -0.7268323  0.3309512 -0.2743300\n\ncol_summary(df, IQR)\n\n[1] 0.8121097 1.7068574 1.3734111 0.9246885\n\n\nThe purrr package provides map functions to eliminate need for for loops, plus it makes code easier to read!\n\n# using map functions for summary stats by column as above\nmap_dbl(df, mean)\n\n         a          b          c          d \n 0.3673695 -0.7268323  0.3309512 -0.2743300 \n\nmap_dbl(df, median)\n\n          a           b           c           d \n 0.53670886 -0.92232782  0.30365532  0.08235539 \n\nmap_dbl(df, sd)\n\n        a         b         c         d \n1.0615755 0.9040397 0.9190574 1.1750165 \n\nmap_dbl(df, mean, trim = 0.5)\n\n          a           b           c           d \n 0.53670886 -0.92232782  0.30365532  0.08235539 \n\n# map_dbl means make a double vector\n# can also do map() for list, map_lgl(), map_int(), and map_chr()\n\n# even more clear\ndf |&gt; map_dbl(mean)\n\n         a          b          c          d \n 0.3673695 -0.7268323  0.3309512 -0.2743300 \n\ndf |&gt; map_dbl(median)\n\n          a           b           c           d \n 0.53670886 -0.92232782  0.30365532  0.08235539 \n\ndf |&gt; map_dbl(sd)\n\n        a         b         c         d \n1.0615755 0.9040397 0.9190574 1.1750165 \n\n\nThe across() function from dplyr also works well:\n\ndf |&gt; summarize(\n  n = n(),\n  across(.cols = a:d, .fns = median, .names = \"median_{.col}\")\n)\n\n# A tibble: 1 × 5\n      n median_a median_b median_c median_d\n  &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1    10    0.537   -0.922    0.304   0.0824\n\n# other ways to repeat across the numeric columns of df:\ndf |&gt; summarize(\n  n = n(),\n  across(everything(), median, .names = \"median_{.col}\")\n)\n\n# A tibble: 1 × 6\n      n median_a median_b median_c median_d median_n\n  &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;int&gt;\n1    10    0.537   -0.922    0.304   0.0824       10\n\ndf |&gt; summarize(\n  n = n(),\n  across(where(is.numeric), median, .names = \"median_{.col}\")\n)\n\n# A tibble: 1 × 6\n      n median_a median_b median_c median_d median_n\n  &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;int&gt;\n1    10    0.537   -0.922    0.304   0.0824       10\n\n# Here \"across\" effectively expands to the following code.  Note that \n#   across() will write over old columns unless you change the name!\ndf |&gt; \n  summarize(\n    median_a = median(a),\n    median_b = median(b),\n    median_c = median(c),\n    median_d = median(d),\n    n = n()\n  )\n\n# A tibble: 1 × 5\n  median_a median_b median_c median_d     n\n     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n1    0.537   -0.922    0.304   0.0824    10\n\n# And if we're worried about NAs, we can't call median directly, we\n#   must create a new function that we can pass options into\ndf_miss &lt;- df\ndf_miss[2, 1] &lt;- NA\ndf_miss[4:5, 2] &lt;- NA\ndf_miss\n\n# A tibble: 10 × 4\n           a      b      c       d\n       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1 -1.80     -1.69  -0.808 -1.93  \n 2 NA        -1.36  -0.399 -2.66  \n 3  0.771     0.292 -0.346  0.387 \n 4  1.75     NA      0.951  0.186 \n 5  0.478    NA      1.59   0.417 \n 6  0.596    -0.684  1.00  -0.621 \n 7  0.275    -1.87   0.896 -0.198 \n 8  1.53     -1.53   1.41   0.561 \n 9  0.000985  0.384 -0.690  1.13  \n10  0.919     0.274 -0.289 -0.0215\n\ndf_miss |&gt; \n  summarize(\n    across(\n      a:d,\n      list(\n        median = \\(x) median(x, na.rm = TRUE),\n        n_miss = \\(x) sum(is.na(x))\n      ),\n      .names = \"{.fn}_{.col}\"\n    ),\n    n = n(),\n  )\n\n# A tibble: 1 × 9\n  median_a n_miss_a median_b n_miss_b median_c n_miss_c median_d n_miss_d     n\n     &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt; &lt;int&gt;\n1    0.596        1    -1.02        2    0.304        0   0.0824        0    10\n\n# where \\ is shorthand for an anonymous function - i.e. you could\n#   replace \"\\\" with \"function\" if you like typing more letters :)\n\n# across-like functions can also be used with filter():\n\n# same as df_miss |&gt; filter(is.na(a) | is.na(b) | is.na(c) | is.na(d))\ndf_miss |&gt; filter(if_any(a:d, is.na))\n\n# A tibble: 3 × 4\n       a     b      c      d\n   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 NA     -1.36 -0.399 -2.66 \n2  1.75  NA     0.951  0.186\n3  0.478 NA     1.59   0.417\n\n# same as df_miss |&gt; filter(is.na(a) & is.na(b) & is.na(c) & is.na(d))\ndf_miss |&gt; filter(if_all(a:d, is.na))\n\n# A tibble: 0 × 4\n# ℹ 4 variables: a &lt;dbl&gt;, b &lt;dbl&gt;, c &lt;dbl&gt;, d &lt;dbl&gt;\n\n\nWhen you input a list of functions (like the lubridate functions below), across() assigns default names as columnname_functionname:\n\nlibrary(lubridate)\nexpand_dates &lt;- function(df) {\n  df |&gt; \n    mutate(\n      across(where(is.Date), list(year = year, month = month, day = mday))\n    )\n}\n\ndf_date &lt;- tibble(\n  name = c(\"Amy\", \"Bob\"),\n  date = ymd(c(\"2009-08-03\", \"2010-01-16\"))\n)\n\ndf_date |&gt; \n  expand_dates()\n\n# A tibble: 2 × 5\n  name  date       date_year date_month date_day\n  &lt;chr&gt; &lt;date&gt;         &lt;dbl&gt;      &lt;dbl&gt;    &lt;int&gt;\n1 Amy   2009-08-03      2009          8        3\n2 Bob   2010-01-16      2010          1       16\n\n\nHere is default is to summarize all numeric columns, but as with all functions, we can override the default if we choose:\n\nsummarize_means &lt;- function(df, summary_vars = where(is.numeric)) {\n  df |&gt; \n    summarize(\n      across({{ summary_vars }}, \\(x) mean(x, na.rm = TRUE)),\n      n = n(),\n      .groups = \"drop\"\n    )\n}\ndiamonds |&gt; \n  group_by(cut) |&gt; \n  summarize_means()\n\n# A tibble: 5 × 9\n  cut       carat depth table price     x     y     z     n\n  &lt;ord&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 Fair      1.05   64.0  59.1 4359.  6.25  6.18  3.98  1610\n2 Good      0.849  62.4  58.7 3929.  5.84  5.85  3.64  4906\n3 Very Good 0.806  61.8  58.0 3982.  5.74  5.77  3.56 12082\n4 Premium   0.892  61.3  58.7 4584.  5.97  5.94  3.65 13791\n5 Ideal     0.703  61.7  56.0 3458.  5.51  5.52  3.40 21551\n\ndiamonds |&gt; \n  group_by(cut) |&gt; \n  summarize_means(c(carat, x:z))\n\n# A tibble: 5 × 6\n  cut       carat     x     y     z     n\n  &lt;ord&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 Fair      1.05   6.25  6.18  3.98  1610\n2 Good      0.849  5.84  5.85  3.64  4906\n3 Very Good 0.806  5.74  5.77  3.56 12082\n4 Premium   0.892  5.97  5.94  3.65 13791\n5 Ideal     0.703  5.51  5.52  3.40 21551\n\n\npivot_longer() with group_by() and summarize() also provides a nice solution:\n\nlong &lt;- df |&gt; \n  pivot_longer(a:d) |&gt; \n  group_by(name) |&gt; \n  summarize(\n    median = median(value),\n    mean = mean(value)\n  )\nlong\n\n# A tibble: 4 × 3\n  name   median   mean\n  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 a      0.537   0.367\n2 b     -0.922  -0.727\n3 c      0.304   0.331\n4 d      0.0824 -0.274\n\n\nHere are a couple of other nice features of map functions: - perform analyses (like fitting a line) by subgroup - extracting components from a model or elements by position\n\n# fit linear model to each group based on cylinder\n#   - split designed to split into new dfs (unlike group_by)\n#   - map returns a vector or list, which can be limiting\nmap = purrr::map\nmodels &lt;- split(mtcars, mtcars$cyl) |&gt;\n  map(function(df) lm(mpg ~ wt, data = df))\nmodels\n\n$`4`\n\nCall:\nlm(formula = mpg ~ wt, data = df)\n\nCoefficients:\n(Intercept)           wt  \n     39.571       -5.647  \n\n\n$`6`\n\nCall:\nlm(formula = mpg ~ wt, data = df)\n\nCoefficients:\n(Intercept)           wt  \n      28.41        -2.78  \n\n\n$`8`\n\nCall:\nlm(formula = mpg ~ wt, data = df)\n\nCoefficients:\n(Intercept)           wt  \n     23.868       -2.192  \n\nmodels[[1]]\n\n\nCall:\nlm(formula = mpg ~ wt, data = df)\n\nCoefficients:\n(Intercept)           wt  \n     39.571       -5.647  \n\n# shortcut using purrr - 1-sided formulas\nmodels &lt;- split(mtcars, mtcars$cyl) |&gt; \n  map(~lm(mpg ~ wt, data = .))\nmodels\n\n$`4`\n\nCall:\nlm(formula = mpg ~ wt, data = .)\n\nCoefficients:\n(Intercept)           wt  \n     39.571       -5.647  \n\n\n$`6`\n\nCall:\nlm(formula = mpg ~ wt, data = .)\n\nCoefficients:\n(Intercept)           wt  \n      28.41        -2.78  \n\n\n$`8`\n\nCall:\nlm(formula = mpg ~ wt, data = .)\n\nCoefficients:\n(Intercept)           wt  \n     23.868       -2.192  \n\n# extract named components from each model\nstr(models)\n\nList of 3\n $ 4:List of 12\n  ..$ coefficients : Named num [1:2] 39.57 -5.65\n  .. ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"wt\"\n  ..$ residuals    : Named num [1:11] -3.6701 2.8428 1.0169 5.2523 -0.0513 ...\n  .. ..- attr(*, \"names\")= chr [1:11] \"Datsun 710\" \"Merc 240D\" \"Merc 230\" \"Fiat 128\" ...\n  ..$ effects      : Named num [1:11] -88.433 10.171 0.695 6.231 1.728 ...\n  .. ..- attr(*, \"names\")= chr [1:11] \"(Intercept)\" \"wt\" \"\" \"\" ...\n  ..$ rank         : int 2\n  ..$ fitted.values: Named num [1:11] 26.5 21.6 21.8 27.1 30.5 ...\n  .. ..- attr(*, \"names\")= chr [1:11] \"Datsun 710\" \"Merc 240D\" \"Merc 230\" \"Fiat 128\" ...\n  ..$ assign       : int [1:2] 0 1\n  ..$ qr           :List of 5\n  .. ..$ qr   : num [1:11, 1:2] -3.317 0.302 0.302 0.302 0.302 ...\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:11] \"Datsun 710\" \"Merc 240D\" \"Merc 230\" \"Fiat 128\" ...\n  .. .. .. ..$ : chr [1:2] \"(Intercept)\" \"wt\"\n  .. .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  .. ..$ qraux: num [1:2] 1.3 1.5\n  .. ..$ pivot: int [1:2] 1 2\n  .. ..$ tol  : num 1e-07\n  .. ..$ rank : int 2\n  .. ..- attr(*, \"class\")= chr \"qr\"\n  ..$ df.residual  : int 9\n  ..$ xlevels      : Named list()\n  ..$ call         : language lm(formula = mpg ~ wt, data = .)\n  ..$ terms        :Classes 'terms', 'formula'  language mpg ~ wt\n  .. .. ..- attr(*, \"variables\")= language list(mpg, wt)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"mpg\" \"wt\"\n  .. .. .. .. ..$ : chr \"wt\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"wt\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: 0x000001831fe36c70&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(mpg, wt)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"wt\"\n  ..$ model        :'data.frame':   11 obs. of  2 variables:\n  .. ..$ mpg: num [1:11] 22.8 24.4 22.8 32.4 30.4 33.9 21.5 27.3 26 30.4 ...\n  .. ..$ wt : num [1:11] 2.32 3.19 3.15 2.2 1.61 ...\n  .. ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language mpg ~ wt\n  .. .. .. ..- attr(*, \"variables\")= language list(mpg, wt)\n  .. .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. .. ..$ : chr [1:2] \"mpg\" \"wt\"\n  .. .. .. .. .. ..$ : chr \"wt\"\n  .. .. .. ..- attr(*, \"term.labels\")= chr \"wt\"\n  .. .. .. ..- attr(*, \"order\")= int 1\n  .. .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. .. ..- attr(*, \"response\")= int 1\n  .. .. .. ..- attr(*, \".Environment\")=&lt;environment: 0x000001831fe36c70&gt; \n  .. .. .. ..- attr(*, \"predvars\")= language list(mpg, wt)\n  .. .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"wt\"\n  ..- attr(*, \"class\")= chr \"lm\"\n $ 6:List of 12\n  ..$ coefficients : Named num [1:2] 28.41 -2.78\n  .. ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"wt\"\n  ..$ residuals    : Named num [1:7] -0.125 0.584 1.929 -0.69 0.355 ...\n  .. ..- attr(*, \"names\")= chr [1:7] \"Mazda RX4\" \"Mazda RX4 Wag\" \"Hornet 4 Drive\" \"Valiant\" ...\n  ..$ effects      : Named num [1:7] -52.235 -2.427 2.111 -0.353 0.679 ...\n  .. ..- attr(*, \"names\")= chr [1:7] \"(Intercept)\" \"wt\" \"\" \"\" ...\n  ..$ rank         : int 2\n  ..$ fitted.values: Named num [1:7] 21.1 20.4 19.5 18.8 18.8 ...\n  .. ..- attr(*, \"names\")= chr [1:7] \"Mazda RX4\" \"Mazda RX4 Wag\" \"Hornet 4 Drive\" \"Valiant\" ...\n  ..$ assign       : int [1:2] 0 1\n  ..$ qr           :List of 5\n  .. ..$ qr   : num [1:7, 1:2] -2.646 0.378 0.378 0.378 0.378 ...\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:7] \"Mazda RX4\" \"Mazda RX4 Wag\" \"Hornet 4 Drive\" \"Valiant\" ...\n  .. .. .. ..$ : chr [1:2] \"(Intercept)\" \"wt\"\n  .. .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  .. ..$ qraux: num [1:2] 1.38 1.12\n  .. ..$ pivot: int [1:2] 1 2\n  .. ..$ tol  : num 1e-07\n  .. ..$ rank : int 2\n  .. ..- attr(*, \"class\")= chr \"qr\"\n  ..$ df.residual  : int 5\n  ..$ xlevels      : Named list()\n  ..$ call         : language lm(formula = mpg ~ wt, data = .)\n  ..$ terms        :Classes 'terms', 'formula'  language mpg ~ wt\n  .. .. ..- attr(*, \"variables\")= language list(mpg, wt)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"mpg\" \"wt\"\n  .. .. .. .. ..$ : chr \"wt\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"wt\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: 0x000001831fe874c0&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(mpg, wt)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"wt\"\n  ..$ model        :'data.frame':   7 obs. of  2 variables:\n  .. ..$ mpg: num [1:7] 21 21 21.4 18.1 19.2 17.8 19.7\n  .. ..$ wt : num [1:7] 2.62 2.88 3.21 3.46 3.44 ...\n  .. ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language mpg ~ wt\n  .. .. .. ..- attr(*, \"variables\")= language list(mpg, wt)\n  .. .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. .. ..$ : chr [1:2] \"mpg\" \"wt\"\n  .. .. .. .. .. ..$ : chr \"wt\"\n  .. .. .. ..- attr(*, \"term.labels\")= chr \"wt\"\n  .. .. .. ..- attr(*, \"order\")= int 1\n  .. .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. .. ..- attr(*, \"response\")= int 1\n  .. .. .. ..- attr(*, \".Environment\")=&lt;environment: 0x000001831fe874c0&gt; \n  .. .. .. ..- attr(*, \"predvars\")= language list(mpg, wt)\n  .. .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"wt\"\n  ..- attr(*, \"class\")= chr \"lm\"\n $ 8:List of 12\n  ..$ coefficients : Named num [1:2] 23.87 -2.19\n  .. ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"wt\"\n  ..$ residuals    : Named num [1:14] 2.374 -1.741 1.455 1.61 -0.381 ...\n  .. ..- attr(*, \"names\")= chr [1:14] \"Hornet Sportabout\" \"Duster 360\" \"Merc 450SE\" \"Merc 450SL\" ...\n  ..$ effects      : Named num [1:14] -56.499 -6.003 0.816 1.22 -0.807 ...\n  .. ..- attr(*, \"names\")= chr [1:14] \"(Intercept)\" \"wt\" \"\" \"\" ...\n  ..$ rank         : int 2\n  ..$ fitted.values: Named num [1:14] 16.3 16 14.9 15.7 15.6 ...\n  .. ..- attr(*, \"names\")= chr [1:14] \"Hornet Sportabout\" \"Duster 360\" \"Merc 450SE\" \"Merc 450SL\" ...\n  ..$ assign       : int [1:2] 0 1\n  ..$ qr           :List of 5\n  .. ..$ qr   : num [1:14, 1:2] -3.742 0.267 0.267 0.267 0.267 ...\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:14] \"Hornet Sportabout\" \"Duster 360\" \"Merc 450SE\" \"Merc 450SL\" ...\n  .. .. .. ..$ : chr [1:2] \"(Intercept)\" \"wt\"\n  .. .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  .. ..$ qraux: num [1:2] 1.27 1.11\n  .. ..$ pivot: int [1:2] 1 2\n  .. ..$ tol  : num 1e-07\n  .. ..$ rank : int 2\n  .. ..- attr(*, \"class\")= chr \"qr\"\n  ..$ df.residual  : int 12\n  ..$ xlevels      : Named list()\n  ..$ call         : language lm(formula = mpg ~ wt, data = .)\n  ..$ terms        :Classes 'terms', 'formula'  language mpg ~ wt\n  .. .. ..- attr(*, \"variables\")= language list(mpg, wt)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"mpg\" \"wt\"\n  .. .. .. .. ..$ : chr \"wt\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"wt\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: 0x000001832005f308&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(mpg, wt)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"wt\"\n  ..$ model        :'data.frame':   14 obs. of  2 variables:\n  .. ..$ mpg: num [1:14] 18.7 14.3 16.4 17.3 15.2 10.4 10.4 14.7 15.5 15.2 ...\n  .. ..$ wt : num [1:14] 3.44 3.57 4.07 3.73 3.78 ...\n  .. ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language mpg ~ wt\n  .. .. .. ..- attr(*, \"variables\")= language list(mpg, wt)\n  .. .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. .. ..$ : chr [1:2] \"mpg\" \"wt\"\n  .. .. .. .. .. ..$ : chr \"wt\"\n  .. .. .. ..- attr(*, \"term.labels\")= chr \"wt\"\n  .. .. .. ..- attr(*, \"order\")= int 1\n  .. .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. .. ..- attr(*, \"response\")= int 1\n  .. .. .. ..- attr(*, \".Environment\")=&lt;environment: 0x000001832005f308&gt; \n  .. .. .. ..- attr(*, \"predvars\")= language list(mpg, wt)\n  .. .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"wt\"\n  ..- attr(*, \"class\")= chr \"lm\"\n\nstr(models[[1]])\n\nList of 12\n $ coefficients : Named num [1:2] 39.57 -5.65\n  ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"wt\"\n $ residuals    : Named num [1:11] -3.6701 2.8428 1.0169 5.2523 -0.0513 ...\n  ..- attr(*, \"names\")= chr [1:11] \"Datsun 710\" \"Merc 240D\" \"Merc 230\" \"Fiat 128\" ...\n $ effects      : Named num [1:11] -88.433 10.171 0.695 6.231 1.728 ...\n  ..- attr(*, \"names\")= chr [1:11] \"(Intercept)\" \"wt\" \"\" \"\" ...\n $ rank         : int 2\n $ fitted.values: Named num [1:11] 26.5 21.6 21.8 27.1 30.5 ...\n  ..- attr(*, \"names\")= chr [1:11] \"Datsun 710\" \"Merc 240D\" \"Merc 230\" \"Fiat 128\" ...\n $ assign       : int [1:2] 0 1\n $ qr           :List of 5\n  ..$ qr   : num [1:11, 1:2] -3.317 0.302 0.302 0.302 0.302 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:11] \"Datsun 710\" \"Merc 240D\" \"Merc 230\" \"Fiat 128\" ...\n  .. .. ..$ : chr [1:2] \"(Intercept)\" \"wt\"\n  .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  ..$ qraux: num [1:2] 1.3 1.5\n  ..$ pivot: int [1:2] 1 2\n  ..$ tol  : num 1e-07\n  ..$ rank : int 2\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 9\n $ xlevels      : Named list()\n $ call         : language lm(formula = mpg ~ wt, data = .)\n $ terms        :Classes 'terms', 'formula'  language mpg ~ wt\n  .. ..- attr(*, \"variables\")= language list(mpg, wt)\n  .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:2] \"mpg\" \"wt\"\n  .. .. .. ..$ : chr \"wt\"\n  .. ..- attr(*, \"term.labels\")= chr \"wt\"\n  .. ..- attr(*, \"order\")= int 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=&lt;environment: 0x000001831fe36c70&gt; \n  .. ..- attr(*, \"predvars\")= language list(mpg, wt)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"wt\"\n $ model        :'data.frame':  11 obs. of  2 variables:\n  ..$ mpg: num [1:11] 22.8 24.4 22.8 32.4 30.4 33.9 21.5 27.3 26 30.4 ...\n  ..$ wt : num [1:11] 2.32 3.19 3.15 2.2 1.61 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language mpg ~ wt\n  .. .. ..- attr(*, \"variables\")= language list(mpg, wt)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"mpg\" \"wt\"\n  .. .. .. .. ..$ : chr \"wt\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"wt\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: 0x000001831fe36c70&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(mpg, wt)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"wt\"\n - attr(*, \"class\")= chr \"lm\"\n\nstr(summary(models[[1]]))\n\nList of 11\n $ call         : language lm(formula = mpg ~ wt, data = .)\n $ terms        :Classes 'terms', 'formula'  language mpg ~ wt\n  .. ..- attr(*, \"variables\")= language list(mpg, wt)\n  .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:2] \"mpg\" \"wt\"\n  .. .. .. ..$ : chr \"wt\"\n  .. ..- attr(*, \"term.labels\")= chr \"wt\"\n  .. ..- attr(*, \"order\")= int 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=&lt;environment: 0x000001831fe36c70&gt; \n  .. ..- attr(*, \"predvars\")= language list(mpg, wt)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"wt\"\n $ residuals    : Named num [1:11] -3.6701 2.8428 1.0169 5.2523 -0.0513 ...\n  ..- attr(*, \"names\")= chr [1:11] \"Datsun 710\" \"Merc 240D\" \"Merc 230\" \"Fiat 128\" ...\n $ coefficients : num [1:2, 1:4] 39.57 -5.65 4.35 1.85 9.1 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:2] \"(Intercept)\" \"wt\"\n  .. ..$ : chr [1:4] \"Estimate\" \"Std. Error\" \"t value\" \"Pr(&gt;|t|)\"\n $ aliased      : Named logi [1:2] FALSE FALSE\n  ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"wt\"\n $ sigma        : num 3.33\n $ df           : int [1:3] 2 9 2\n $ r.squared    : num 0.509\n $ adj.r.squared: num 0.454\n $ fstatistic   : Named num [1:3] 9.32 1 9\n  ..- attr(*, \"names\")= chr [1:3] \"value\" \"numdf\" \"dendf\"\n $ cov.unscaled : num [1:2, 1:2] 1.701 -0.705 -0.705 0.308\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:2] \"(Intercept)\" \"wt\"\n  .. ..$ : chr [1:2] \"(Intercept)\" \"wt\"\n - attr(*, \"class\")= chr \"summary.lm\"\n\nmodels |&gt;\n  map(summary) |&gt; \n  map_dbl(\"r.squared\")\n\n        4         6         8 \n0.5086326 0.4645102 0.4229655 \n\n# can use integer to select elements by position\nx &lt;- list(list(1, 2, 3), list(4, 5, 6), list(7, 8, 9))\nx |&gt; map_dbl(2)\n\n[1] 2 5 8",
    "crumbs": [
      "Iteration"
    ]
  },
  {
    "objectID": "05_iteration.html#iterative-techniques-for-reading-multiple-files",
    "href": "05_iteration.html#iterative-techniques-for-reading-multiple-files",
    "title": "Iteration",
    "section": "Iterative techniques for reading multiple files",
    "text": "Iterative techniques for reading multiple files\n\nlibrary(readxl)\n\n# our usual path doesn't work with excel files\n# read_excel(\"https://proback.github.io/264_fall_2024/Data/gapminder/1952.xlsx\")\n\n# this will work if .xlsx files live in a Data folder that's at the same\n#   level as your .qmd file\ngap1952 &lt;- read_excel(\"Data/gapminder/1952.xlsx\")\ngap1957 &lt;- read_excel(\"Data/gapminder/1957.xlsx\")\n\nSince the 1952 and 1957 data have the same 5 columns, if we want to combine this data into a single data set showing time trends, we could simply bind_rows() (after adding a 6th column for year)\n\ngap1952 &lt;- gap1952 |&gt;\n  mutate(year = 1952)\ngap1957 &lt;- gap1957 |&gt;\n  mutate(year = 1957)\ngap_data &lt;- bind_rows(gap1952, gap1957)\n\nOf course, with 10 more years worth of data still left to read in and merge, this process could get pretty onerous. Section 26.3 shows how to automate this process in 3 steps:\n\nuse list.files() to list all the files in a directory\n\n\npaths &lt;- list.files(\"Data/gapminder\", pattern = \"[.]xlsx$\", full.names = TRUE)\npaths\n\n [1] \"Data/gapminder/1952.xlsx\" \"Data/gapminder/1957.xlsx\"\n [3] \"Data/gapminder/1962.xlsx\" \"Data/gapminder/1967.xlsx\"\n [5] \"Data/gapminder/1972.xlsx\" \"Data/gapminder/1977.xlsx\"\n [7] \"Data/gapminder/1982.xlsx\" \"Data/gapminder/1987.xlsx\"\n [9] \"Data/gapminder/1992.xlsx\" \"Data/gapminder/1997.xlsx\"\n[11] \"Data/gapminder/2002.xlsx\" \"Data/gapminder/2007.xlsx\"\n\n\n\nuse purrr::map() to read each of them into a list (we will discuss lists more in 06_data_types.qmd)\n\n\ngap_files &lt;- map(paths, readxl::read_excel)\nlength(gap_files)\n\n[1] 12\n\nstr(gap_files)\n\nList of 12\n $ : tibble [142 × 5] (S3: tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 28.8 55.2 43.1 30 62.5 ...\n  ..$ pop      : num [1:142] 8425333 1282697 9279525 4232095 17876956 ...\n  ..$ gdpPercap: num [1:142] 779 1601 2449 3521 5911 ...\n $ : tibble [142 × 5] (S3: tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 30.3 59.3 45.7 32 64.4 ...\n  ..$ pop      : num [1:142] 9240934 1476505 10270856 4561361 19610538 ...\n  ..$ gdpPercap: num [1:142] 821 1942 3014 3828 6857 ...\n $ : tibble [142 × 5] (S3: tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 32 64.8 48.3 34 65.1 ...\n  ..$ pop      : num [1:142] 10267083 1728137 11000948 4826015 21283783 ...\n  ..$ gdpPercap: num [1:142] 853 2313 2551 4269 7133 ...\n $ : tibble [142 × 5] (S3: tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 34 66.2 51.4 36 65.6 ...\n  ..$ pop      : num [1:142] 11537966 1984060 12760499 5247469 22934225 ...\n  ..$ gdpPercap: num [1:142] 836 2760 3247 5523 8053 ...\n $ : tibble [142 × 5] (S3: tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 36.1 67.7 54.5 37.9 67.1 ...\n  ..$ pop      : num [1:142] 13079460 2263554 14760787 5894858 24779799 ...\n  ..$ gdpPercap: num [1:142] 740 3313 4183 5473 9443 ...\n $ : tibble [142 × 5] (S3: tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 38.4 68.9 58 39.5 68.5 ...\n  ..$ pop      : num [1:142] 14880372 2509048 17152804 6162675 26983828 ...\n  ..$ gdpPercap: num [1:142] 786 3533 4910 3009 10079 ...\n $ : tibble [142 × 5] (S3: tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 39.9 70.4 61.4 39.9 69.9 ...\n  ..$ pop      : num [1:142] 12881816 2780097 20033753 7016384 29341374 ...\n  ..$ gdpPercap: num [1:142] 978 3631 5745 2757 8998 ...\n $ : tibble [142 × 5] (S3: tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 40.8 72 65.8 39.9 70.8 ...\n  ..$ pop      : num [1:142] 13867957 3075321 23254956 7874230 31620918 ...\n  ..$ gdpPercap: num [1:142] 852 3739 5681 2430 9140 ...\n $ : tibble [142 × 5] (S3: tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 41.7 71.6 67.7 40.6 71.9 ...\n  ..$ pop      : num [1:142] 16317921 3326498 26298373 8735988 33958947 ...\n  ..$ gdpPercap: num [1:142] 649 2497 5023 2628 9308 ...\n $ : tibble [142 × 5] (S3: tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 41.8 73 69.2 41 73.3 ...\n  ..$ pop      : num [1:142] 22227415 3428038 29072015 9875024 36203463 ...\n  ..$ gdpPercap: num [1:142] 635 3193 4797 2277 10967 ...\n $ : tibble [142 × 5] (S3: tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 42.1 75.7 71 41 74.3 ...\n  ..$ pop      : num [1:142] 25268405 3508512 31287142 10866106 38331121 ...\n  ..$ gdpPercap: num [1:142] 727 4604 5288 2773 8798 ...\n $ : tibble [142 × 5] (S3: tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 43.8 76.4 72.3 42.7 75.3 ...\n  ..$ pop      : num [1:142] 31889923 3600523 33333216 12420476 40301927 ...\n  ..$ gdpPercap: num [1:142] 975 5937 6223 4797 12779 ...\n\ngap_files[[1]]   # pull off the first object in the list (i.e. 1952 data)\n\n# A tibble: 142 × 5\n   country     continent lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia         28.8  8425333      779.\n 2 Albania     Europe       55.2  1282697     1601.\n 3 Algeria     Africa       43.1  9279525     2449.\n 4 Angola      Africa       30.0  4232095     3521.\n 5 Argentina   Americas     62.5 17876956     5911.\n 6 Australia   Oceania      69.1  8691212    10040.\n 7 Austria     Europe       66.8  6927772     6137.\n 8 Bahrain     Asia         50.9   120447     9867.\n 9 Bangladesh  Asia         37.5 46886859      684.\n10 Belgium     Europe       68    8730405     8343.\n# ℹ 132 more rows\n\n\n\nuse purrr::list_rbind() to combine them into a single data frame\n\n\ngap_tidy &lt;- list_rbind(gap_files)\nclass(gap_tidy)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\ngap_tidy\n\n# A tibble: 1,704 × 5\n   country     continent lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia         28.8  8425333      779.\n 2 Albania     Europe       55.2  1282697     1601.\n 3 Algeria     Africa       43.1  9279525     2449.\n 4 Angola      Africa       30.0  4232095     3521.\n 5 Argentina   Americas     62.5 17876956     5911.\n 6 Australia   Oceania      69.1  8691212    10040.\n 7 Austria     Europe       66.8  6927772     6137.\n 8 Bahrain     Asia         50.9   120447     9867.\n 9 Bangladesh  Asia         37.5 46886859      684.\n10 Belgium     Europe       68    8730405     8343.\n# ℹ 1,694 more rows\n\n\nWe could even do all steps in a single pipeline:\n\nlist.files(\"Data/gapminder\", pattern = \"[.]xlsx$\", full.names = TRUE) |&gt;\n  map(readxl::read_excel) |&gt;\n  list_rbind()\n\n# A tibble: 1,704 × 5\n   country     continent lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia         28.8  8425333      779.\n 2 Albania     Europe       55.2  1282697     1601.\n 3 Algeria     Africa       43.1  9279525     2449.\n 4 Angola      Africa       30.0  4232095     3521.\n 5 Argentina   Americas     62.5 17876956     5911.\n 6 Australia   Oceania      69.1  8691212    10040.\n 7 Austria     Europe       66.8  6927772     6137.\n 8 Bahrain     Asia         50.9   120447     9867.\n 9 Bangladesh  Asia         37.5 46886859      684.\n10 Belgium     Europe       68    8730405     8343.\n# ℹ 1,694 more rows\n\n\nNote that we are lacking a 6th column with the year represented by each row of data. Here is one way to solve that issue:\n\n# This extracts file names, which are carried along as data frame names by\n#   map functions\npaths |&gt; set_names(basename)\n\n                 1952.xlsx                  1957.xlsx \n\"Data/gapminder/1952.xlsx\" \"Data/gapminder/1957.xlsx\" \n                 1962.xlsx                  1967.xlsx \n\"Data/gapminder/1962.xlsx\" \"Data/gapminder/1967.xlsx\" \n                 1972.xlsx                  1977.xlsx \n\"Data/gapminder/1972.xlsx\" \"Data/gapminder/1977.xlsx\" \n                 1982.xlsx                  1987.xlsx \n\"Data/gapminder/1982.xlsx\" \"Data/gapminder/1987.xlsx\" \n                 1992.xlsx                  1997.xlsx \n\"Data/gapminder/1992.xlsx\" \"Data/gapminder/1997.xlsx\" \n                 2002.xlsx                  2007.xlsx \n\"Data/gapminder/2002.xlsx\" \"Data/gapminder/2007.xlsx\" \n\n# The middle line ensures that each of the 12 data frames in the list for\n#   gap_files has a name determined by its filepath, unlike the gap_files\n#   we created in step 2 above, which had no names (we could only identify\n#   data frames by their position)\ngap_files &lt;- paths |&gt; \n  set_names(basename) |&gt;  \n  map(readxl::read_excel)\n\n# Now we can extract a particular year by its name:\ngap_files[[\"1962.xlsx\"]]\n\n# A tibble: 142 × 5\n   country     continent lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia         32.0 10267083      853.\n 2 Albania     Europe       64.8  1728137     2313.\n 3 Algeria     Africa       48.3 11000948     2551.\n 4 Angola      Africa       34    4826015     4269.\n 5 Argentina   Americas     65.1 21283783     7133.\n 6 Australia   Oceania      70.9 10794968    12217.\n 7 Austria     Europe       69.5  7129864    10751.\n 8 Bahrain     Asia         56.9   171863    12753.\n 9 Bangladesh  Asia         41.2 56839289      686.\n10 Belgium     Europe       70.2  9218400    10991.\n# ℹ 132 more rows\n\n# Finally, take advantage of the `names_to` argument in list_rbind to \n#   create that 6th column with `year`\ngap_tidy &lt;- paths |&gt; \n  set_names(basename) |&gt; \n  map(readxl::read_excel) |&gt; \n  list_rbind(names_to = \"year\") |&gt; \n  mutate(year = parse_number(year))\n\nYou could then save your result using write_csv so you don’t have to run the reading and wrangling code every time!",
    "crumbs": [
      "Iteration"
    ]
  },
  {
    "objectID": "05_iteration.html#on-your-own",
    "href": "05_iteration.html#on-your-own",
    "title": "Iteration",
    "section": "On Your Own",
    "text": "On Your Own\n\nCompute the mean of every column of the mtcars data set using (a) a for loop, (b) a map function, (c) the across() function, and (d) pivot_longer().\nWrite a function that prints the mean of each numeric column in a data frame. Try it on the iris data set. (Hint: keep(is.numeric))\nEliminate the for loop in each of the following examples by taking advantage of an existing function that works with vectors:\n\n\nout &lt;- \"\"\nfor (x in letters) {\n  out &lt;- stringr::str_c(out, x)\n}\nout\n\n[1] \"abcdefghijklmnopqrstuvwxyz\"\n\nx &lt;- runif(100)\nout &lt;- vector(\"numeric\", length(x))\nout[1] &lt;- x[1]\nfor (i in 2:length(x)) {\n  out[i] &lt;- out[i - 1] + x[i]\n}\nout\n\n  [1]  0.4363580  0.6163114  1.2547968  2.0925272  2.5482991  2.9061889\n  [7]  3.1792396  3.4801376  4.3323516  5.1758081  5.9077938  6.3636480\n [13]  6.9713586  7.8547863  8.8505223  9.3018292 10.1461415 10.6780068\n [19] 11.2364895 11.4196895 11.6830372 12.0431903 12.9037666 13.7713163\n [25] 14.2788006 14.9264675 15.6075275 16.4128426 17.2000999 17.5380862\n [31] 18.4035165 19.2263177 19.4371666 19.6111271 20.1410954 20.4032972\n [37] 21.0143504 21.2417582 22.0500671 22.4558197 23.0901000 23.9480034\n [43] 24.3488660 25.3264980 26.1345959 26.4749237 26.8314345 27.7029153\n [49] 28.0788223 28.7635352 29.5819036 29.6324120 30.3804216 31.2764039\n [55] 31.8694696 31.9873297 32.4037579 33.2375845 33.9450734 33.9758665\n [61] 34.4840275 35.1644055 35.5737545 35.7371849 36.3416225 37.1803655\n [67] 37.7797960 38.1944555 38.4163835 38.4721142 38.5460223 38.7286420\n [73] 39.2164878 39.3166065 40.1072741 40.3050525 40.8310915 40.8454806\n [79] 41.6906755 42.1600349 42.2317290 42.7587245 43.6998448 44.4663339\n [85] 44.6525455 44.9054612 45.2130954 45.6687264 45.7193277 46.6936858\n [91] 47.6153619 48.2713683 48.2984388 48.9174845 49.1291271 49.5138485\n [97] 50.1872974 50.6112216 50.6366930 51.0128397\n\n\n\nCompute the number of unique values in each column of the iris data set using at least 2 of your favorite iteration methods. Bonus points if you can use pivot_longer()!\nCarefully explain each step in the pipeline below:\n\n\nshow_missing &lt;- function(df, group_vars, summary_vars = everything()) {\n  df |&gt; \n    group_by(pick({{ group_vars }})) |&gt; \n    summarize(\n      across({{ summary_vars }}, \\(x) sum(is.na(x))),\n      .groups = \"drop\"\n    ) |&gt;\n    select(where(\\(x) any(x &gt; 0)))\n}\nnycflights13::flights |&gt; show_missing(c(year, month, day))\n\n# A tibble: 365 × 9\n    year month   day dep_time dep_delay arr_time arr_delay tailnum air_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;     &lt;int&gt;    &lt;int&gt;     &lt;int&gt;   &lt;int&gt;    &lt;int&gt;\n 1  2013     1     1        4         4        5        11       0       11\n 2  2013     1     2        8         8       10        15       2       15\n 3  2013     1     3       10        10       10        14       2       14\n 4  2013     1     4        6         6        6         7       2        7\n 5  2013     1     5        3         3        3         3       1        3\n 6  2013     1     6        1         1        1         3       0        3\n 7  2013     1     7        3         3        3         3       1        3\n 8  2013     1     8        4         4        4         7       1        7\n 9  2013     1     9        5         5        7         9       2        9\n10  2013     1    10        3         3        3         3       2        3\n# ℹ 355 more rows\n\n\n\nWrite a function called summary_stats() that allows a user to input a tibble, numeric variables in that tibble, and summary statistics that they would like to see for each variable. Using across(), the function’s output should look like the example below.\n\n\nsummary_stats(mtcars, \n              vars = c(mpg, hp, wt), \n              stat_fcts = list(mean = mean, \n                               median = median, \n                               sd = sd, \n                               IQR = IQR))\n\n#  mpg_mean mpg_median   mpg_sd mpg_IQR  hp_mean hp_median    hp_sd hp_IQR\n#1 20.09062       19.2 6.026948   7.375 146.6875       123 68.56287   83.5\n#  wt_mean wt_median     wt_sd  wt_IQR  n\n#1 3.21725     3.325 0.9784574 1.02875 32\n\n\nThe power of a statistical test is the probability that it rejects the null hypothesis when the null hypothesis is false. In other words, it’s the probability that a statistical test can detect when a true difference exists. The power depends on a number of factors, including:\n\n\nsample size\ntype I error level (probability of declaring there is a statistically significant difference when there really isn’t)\nvariability in the data\nsize of the true difference\n\nThe following steps can be followed to simulate a power calculation using iteration techniques:\n\ngenerate simulated data where is a true difference or effect\nrun your desired test on the simulated data and record if the null hypothesis was rejected or not (i.e. if the p-value was below .05)\nrepeat (a)-(b) a large number of times and record the total proportion of times that the null hypothesis was rejected; that proportion is the power of your test under those conditions\n\nCreate a power curve for a two-sample t-test by filling in Step C below and then removing eval: FALSE:\n\n# Step A\n\n# set parameters for two-sample t-test\nmean1 &lt;- 100   # mean response in Group 1\ntruediff &lt;- 5    # true mean difference between Groups 1 and 2\nmean2 &lt;- mean1 + truediff   # mean response in Group 2\nsd1 &lt;- 10   # standard deviation in Group 1\nsd2 &lt;- 10   # standard deviation in Group 2\nn1 &lt;- 20    # sample size in Group 1\nn2 &lt;- 20    # sample size in Group 2\nnumsims &lt;- 1000   # number of simulations (iterations) to run\n\n# generate sample data for Groups 1 and 2 based on normal distributions\n#   with the parameters above (note that there is truly a difference in means!)\nsamp1 &lt;- rnorm(n1, mean1, sd1)\nsamp2 &lt;- rnorm(n2, mean2, sd2)\n\n# organize the simulated data into a tibble\nsim_data &lt;- tibble(response = c(samp1, samp2), \n       group = c(rep(\"Group 1\", n1), rep(\"Group 2\", n2)))\nsim_data\n\n# Step B\n\n# exploratory analysis of the simulated data\nmosaic::favstats(response ~ group, data = sim_data)\nggplot(sim_data, aes(x = response, y = group)) +\n  geom_boxplot()\n\n# run a two-sample t-test to see if there is a significant difference\n#   in means between Groups 1 and 2 (i.e. is the p-value &lt; .05?)\np_value &lt;- t.test(x = samp1, y = samp2)$p.value\np_value\np_value &lt; .05   # if TRUE, then we reject the null hypothesis and conclude\n                #   there is a statistically significant difference\n\n# Step C\n\n# find the power = proportion of time null is rejected when\n#   true difference is not 0 (i.e. number of simulated data sets that\n#   result in p-values below .05)",
    "crumbs": [
      "Iteration"
    ]
  },
  {
    "objectID": "03_functions.html",
    "href": "03_functions.html",
    "title": "Functions and tidy evaluation",
    "section": "",
    "text": "Based on Chapter 25 from R for Data Science\nYou can download this .qmd file from here. Just hit the Download Raw File button.",
    "crumbs": [
      "Functions and tidy evaluation"
    ]
  },
  {
    "objectID": "03_functions.html#vector-functions",
    "href": "03_functions.html#vector-functions",
    "title": "Functions and tidy evaluation",
    "section": "Vector functions",
    "text": "Vector functions\n\nExample 1: Rescale variables from 0 to 1.\nThis code creates a 10 x 4 tibble filled with random values taken from a normal distribution with mean 0 and SD 1\n\ndf &lt;- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\ndf\n\n# A tibble: 10 × 4\n         a       b      c       d\n     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1  0.290  -0.0972  0.349  0.765 \n 2  2.82    0.284   1.95  -1.63  \n 3  0.301   0.140   0.490 -0.0328\n 4  1.08   -0.742   0.198 -0.120 \n 5 -0.0682  0.627  -0.411 -0.494 \n 6  0.399  -0.921   0.708 -0.214 \n 7 -1.55   -0.556   1.71  -0.138 \n 8 -0.451   1.86    1.18  -1.00  \n 9  0.212  -0.679   1.37   0.841 \n10  0.147   0.274   1.08  -0.324 \n\n\nThis code below for rescaling variables from 0 to 1 is ripe for functions… we did it four times!\nIt’s easiest to start with working code and turn it into a function.\n\ndf$a &lt;- (df$a - min(df$a)) / (max(df$a) - min(df$a))\ndf$b &lt;- (df$b - min(df$b)) / (max(df$b) - min(df$b))\ndf$c &lt;- (df$c - min(df$c)) / (max(df$c) - min(df$c))\ndf$d &lt;- (df$d - min(df$d)) / (max(df$d) - min(df$d))\ndf\n\n# A tibble: 10 × 4\n       a      b     c     d\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 0.421 0.296  0.322 0.970\n 2 1     0.434  1     0    \n 3 0.424 0.382  0.381 0.647\n 4 0.601 0.0643 0.258 0.612\n 5 0.339 0.557  0     0.460\n 6 0.446 0      0.474 0.574\n 7 0     0.131  0.900 0.604\n 8 0.252 1      0.675 0.255\n 9 0.403 0.0872 0.755 1    \n10 0.388 0.430  0.629 0.529\n\n\nNotice first what changes and what stays the same in each line. Then, if we look at the first line above, we see we have one value we’re using over and over: df$a. So our function will have one input. We’ll start with our code from that line, then replace the input (df$a) with x. We should give our function a name that explains what it does. The name should be a verb.\n\n# I'm going to show you how to write the function in class! \n# I have it in the code already below, but don't look yet!\n# Let's try to write it together first!\n\n. . . . . . . . .\n\n# Our function (first draft!)\nrescale01 &lt;- function(x) {\n  (x - min(x)) / (max(x) - min(x))\n}\n\nNote the general form of a function:\n\nname &lt;- function(arguments) {\n  body\n}\n\nEvery function contains 3 essential components:\n\nA name. The name should clearly evoke what the function does; hence, it is often a verb (action). Here we’ll use rescale01 because this function rescales a vector to lie between 0 and 1. snake_case is good; CamelCase is just okay.\nThe arguments. The arguments are things that vary across calls and they are usually nouns - first the data, then other details. Our analysis above tells us that we have just one; we’ll call it x because this is the conventional name for a numeric vector, but you can use any word.\nThe body. The body is the code that’s repeated across all the calls. By default a function will return the last statement; use return() to specify a return value\n\nSummary: Functions should be written for both humans and computers!\nOnce we have written a function we like, then we need to test it with different inputs!\n\ntemp &lt;- c(4, 6, 8, 9)\nrescale01(temp)\n\n[1] 0.0 0.4 0.8 1.0\n\ntemp0 &lt;- c(4, 6, 8, 9, NA)\nrescale01(temp0)\n\n[1] NA NA NA NA NA\n\n\nOK, so NA’s don’t work the way we want them to.\n\nrescale01 &lt;- function(x) {\n  (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))\n}\nrescale01(temp)\n\n[1] 0.0 0.4 0.8 1.0\n\nrescale01(temp0)\n\n[1] 0.0 0.4 0.8 1.0  NA\n\n\nWe can continue to improve our function. Here is another method, which uses the existing range function within R to avoid 3 max/min executions:\n\nrescale01 &lt;- function(x) {\n  rng &lt;- range(x, na.rm = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\nrescale01(temp)\n\n[1] 0.0 0.4 0.8 1.0\n\nrescale01(c(0, 5, 10))\n\n[1] 0.0 0.5 1.0\n\nrescale01(c(-10, 0, 10))\n\n[1] 0.0 0.5 1.0\n\nrescale01(c(1, 2, 3, NA, 5))\n\n[1] 0.00 0.25 0.50   NA 1.00\n\n\nWe should continue testing unusual inputs. Think carefully about how you want this function to behave… the current behavior is to include the Inf (infinity) value when calculating the range. You get strange output everywhere, but it’s pretty clear that there is a problem right away when you use the function. In the example below (rescale1), you ignore the infinity value when calculating the range. The function returns Inf for one value, and sensible stuff for the rest. In many cases this may be useful, but it could also hide a problem until you get deeper into an analysis.\n\nx &lt;- c(1:10, Inf)\nrescale01(x)\n\n [1]   0   0   0   0   0   0   0   0   0   0 NaN\n\nrescale1 &lt;- function(x) {\n  rng &lt;- range(x, na.rm = TRUE, finite = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\nrescale1(x)\n\n [1] 0.0000000 0.1111111 0.2222222 0.3333333 0.4444444 0.5555556 0.6666667\n [8] 0.7777778 0.8888889 1.0000000       Inf\n\n\nNow we’ve used functions to simplify original example. We will learn to simplify further in iterations (Ch 26)\n\ndf &lt;- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n# add a little noise\ndf$a[5] = NA\ndf$b[6] = Inf\ndf\n\n# A tibble: 10 × 4\n         a       b       c      d\n     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 -1.14    -0.306  0.125  -1.13 \n 2 -0.592    1.20   0.509  -1.37 \n 3 -0.0533  -1.77  -1.53   -0.359\n 4 -0.0517  -1.48  -0.209  -2.41 \n 5 NA        0.597 -0.664  -0.979\n 6 -0.575  Inf      0.929   0.294\n 7 -0.134   -0.817 -0.160  -0.816\n 8 -0.895   -0.547  0.0797 -0.792\n 9 -0.955   -1.23  -0.766   0.718\n10  0.589    0.550 -0.867  -1.82 \n\ndf$a_new &lt;- rescale1(df$a)\ndf$b_new &lt;- rescale1(df$b)\ndf$c_new &lt;- rescale1(df$c)\ndf$d_new &lt;- rescale1(df$d)\ndf\n\n# A tibble: 10 × 8\n         a       b       c      d  a_new    b_new c_new d_new\n     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 -1.14    -0.306  0.125  -1.13   0       0.493  0.673 0.410\n 2 -0.592    1.20   0.509  -1.37   0.316   1      0.829 0.331\n 3 -0.0533  -1.77  -1.53   -0.359  0.628   0      0     0.655\n 4 -0.0517  -1.48  -0.209  -2.41   0.629   0.0970 0.537 0    \n 5 NA        0.597 -0.664  -0.979 NA       0.798  0.351 0.457\n 6 -0.575  Inf      0.929   0.294  0.325 Inf      1     0.864\n 7 -0.134   -0.817 -0.160  -0.816  0.581   0.321  0.557 0.509\n 8 -0.895   -0.547  0.0797 -0.792  0.140   0.412  0.654 0.517\n 9 -0.955   -1.23  -0.766   0.718  0.105   0.181  0.310 1    \n10  0.589    0.550 -0.867  -1.82   1       0.782  0.269 0.187\n\ndf |&gt;\n  select(1:4) |&gt;\n  mutate(a_new = rescale1(a),\n         b_new = rescale1(b),\n         c_new = rescale1(c),\n         d_new = rescale1(d))\n\n# A tibble: 10 × 8\n         a       b       c      d  a_new    b_new c_new d_new\n     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 -1.14    -0.306  0.125  -1.13   0       0.493  0.673 0.410\n 2 -0.592    1.20   0.509  -1.37   0.316   1      0.829 0.331\n 3 -0.0533  -1.77  -1.53   -0.359  0.628   0      0     0.655\n 4 -0.0517  -1.48  -0.209  -2.41   0.629   0.0970 0.537 0    \n 5 NA        0.597 -0.664  -0.979 NA       0.798  0.351 0.457\n 6 -0.575  Inf      0.929   0.294  0.325 Inf      1     0.864\n 7 -0.134   -0.817 -0.160  -0.816  0.581   0.321  0.557 0.509\n 8 -0.895   -0.547  0.0797 -0.792  0.140   0.412  0.654 0.517\n 9 -0.955   -1.23  -0.766   0.718  0.105   0.181  0.310 1    \n10  0.589    0.550 -0.867  -1.82   1       0.782  0.269 0.187\n\n# Even better - from Chapter 26\ndf |&gt; \n  select(1:4) |&gt;\n  mutate(across(a:d, rescale1, .names = \"{.col}_new\"))\n\n# A tibble: 10 × 8\n         a       b       c      d  a_new    b_new c_new d_new\n     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 -1.14    -0.306  0.125  -1.13   0       0.493  0.673 0.410\n 2 -0.592    1.20   0.509  -1.37   0.316   1      0.829 0.331\n 3 -0.0533  -1.77  -1.53   -0.359  0.628   0      0     0.655\n 4 -0.0517  -1.48  -0.209  -2.41   0.629   0.0970 0.537 0    \n 5 NA        0.597 -0.664  -0.979 NA       0.798  0.351 0.457\n 6 -0.575  Inf      0.929   0.294  0.325 Inf      1     0.864\n 7 -0.134   -0.817 -0.160  -0.816  0.581   0.321  0.557 0.509\n 8 -0.895   -0.547  0.0797 -0.792  0.140   0.412  0.654 0.517\n 9 -0.955   -1.23  -0.766   0.718  0.105   0.181  0.310 1    \n10  0.589    0.550 -0.867  -1.82   1       0.782  0.269 0.187\n\n\n\n\nOptions for handling NAs in functions\nBefore we try some practice problems, let’s consider various options for handling NAs in functions. We used the na.rm option within functions like min, max, and range in order to take care of missing values. But there are alternative approaches:\n\nfilter/remove the NA values before rescaling\ncreate an if statement to check if there are NAs; return an error if NAs exist\ncreate a removeNAs option in the function we are creating\n\nLet’s take a look at each alternative approach in turn:\n\nFilter/remove the NA values before rescaling\n\ndf &lt;- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\ndf$a[5] = NA\ndf\n\n# A tibble: 10 × 4\n         a       b       c       d\n     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1  0.586   0.0624 -0.488   0.592 \n 2 -0.0311  0.598  -1.25   -2.44  \n 3 -0.275  -0.299  -0.516   0.692 \n 4 -1.29    0.459   0.613   0.412 \n 5 NA      -0.705   1.28   -0.0105\n 6  1.58   -0.199   0.303  -0.453 \n 7  0.387   0.999   0.633   1.05  \n 8  0.488  -0.564   0.834  -0.736 \n 9  0.617   1.12   -0.0360 -0.313 \n10  0.147   0.211  -0.781   1.32  \n\nrescale_basic &lt;- function(x) {\n  (x - min(x)) / (max(x) - min(x))\n}\n\ndf %&gt;%\n  filter(!is.na(a)) %&gt;%\n  mutate(new_a = rescale_basic(a))\n\n# A tibble: 9 × 5\n        a       b       c      d new_a\n    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1  0.586   0.0624 -0.488   0.592 0.653\n2 -0.0311  0.598  -1.25   -2.44  0.438\n3 -0.275  -0.299  -0.516   0.692 0.353\n4 -1.29    0.459   0.613   0.412 0    \n5  1.58   -0.199   0.303  -0.453 1    \n6  0.387   0.999   0.633   1.05  0.584\n7  0.488  -0.564   0.834  -0.736 0.619\n8  0.617   1.12   -0.0360 -0.313 0.664\n9  0.147   0.211  -0.781   1.32  0.500\n\n\n[Pause to Ponder:] Do you notice anything in the output above that gives you pause?\n\n\nCreate an if statement to check if there are NAs; return an error if NAs exist\nFirst, here’s an example involving weighted means:\n\n# Create function to calculate weighted mean\nwt_mean &lt;- function(x, w) {\n  sum(x * w) / sum(w)\n}\nwt_mean(c(1, 10), c(1/3, 2/3))\n\n[1] 7\n\nwt_mean(1:6, 1:3)\n\n[1] 7.666667\n\n\n[Pause to Ponder:] Why is the answer to the last call above 7.67? Aren’t we taking a weighted mean of 1-6, all of which are below 7?\n\n# update function to handle cases where data and weights of unequal length\nwt_mean &lt;- function(x, w) {\n  if (length(x) != length(w)) {\n    stop(\"`x` and `w` must be the same length\", call. = FALSE)\n  } else {\n  sum(w * x) / sum(w)\n  }  \n}\nwt_mean(1:6, 1:3) \n\nError: `x` and `w` must be the same length\n\n# should produce an error now if weights and data different lengths\n#  - nice example of if and else\n\n[Pause to Ponder:] What does the call. option do?\nNow let’s apply this to our rescaling function\n\nrescale_w_error &lt;- function(x) {\n  if (is.na(sum(x))) {\n    stop(\"`x` cannot have NAs\", call. = FALSE)\n  } else {\n    (x - min(x)) / (max(x) - min(x))\n  }  \n}\n\ntemp &lt;- c(4, 6, 8, 9)\nrescale_w_error(temp)\n\n[1] 0.0 0.4 0.8 1.0\n\ntemp &lt;- c(4, 6, 8, 9, NA)\nrescale_w_error(temp)\n\nError: `x` cannot have NAs\n\n\n[Pause to Ponder:] Why can’t we just use if (is.na(x)) instead of is.na(sum(x))?\n\n\nCreate a removeNAs option in the function we are creating\n\nrescale_NAoption &lt;- function(x, removeNAs = FALSE) {\n  (x - min(x, na.rm = removeNAs)) / \n    (max(x, na.rm = removeNAs) - min(x, na.rm = removeNAs))\n} \n\ntemp &lt;- c(4, 6, 8, 9)\nrescale_NAoption(temp)\n\n[1] 0.0 0.4 0.8 1.0\n\ntemp &lt;- c(4, 6, 8, 9, NA)\nrescale_NAoption(temp, removeNAs = TRUE)\n\n[1] 0.0 0.4 0.8 1.0  NA\n\n\nOK, but all the other summary stats functions use na.rm as the input, so to be consistent, it’s probably better to do something slightly awkward like this:\n\nrescale_NAoption &lt;- function(x, na.rm = FALSE) {\n  (x - min(x, na.rm = na.rm)) / \n    (max(x, na.rm = na.rm) - min(x, na.rm = na.rm))\n} \n\ntemp &lt;- c(4, 6, 8, 9, NA)\nrescale_NAoption(temp, na.rm = TRUE)\n\n[1] 0.0 0.4 0.8 1.0  NA\n\n\nwt_mean() is an example of a “summary function (single value output) instead of a”mutate function” (vector output) like rescale01(). Here’s another summary function to produce the mean absolute percentage error:\n\nmape &lt;- function(actual, predicted) {\n  sum(abs((actual - predicted) / actual)) / length(actual)\n}\n\ny &lt;- c(2,6,3,8,5)\nyhat &lt;- c(2.5, 5.1, 4.4, 7.8, 6.1)\nmape(actual = y, predicted = yhat)\n\n[1] 0.2223333",
    "crumbs": [
      "Functions and tidy evaluation"
    ]
  },
  {
    "objectID": "03_functions.html#data-frame-functions",
    "href": "03_functions.html#data-frame-functions",
    "title": "Functions and tidy evaluation",
    "section": "Data frame functions",
    "text": "Data frame functions\nThese work like dplyr verbs, taking a data frame as the first argument, and then returning a data frame or a vector.\n\nDemonstration of tidy evaluation in functions\n\n# Start with working code then functionize\nggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +\n  geom_point(size = 0.75) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\nmake_plot &lt;- function(dataset, xvar, yvar, pt_size = 0.75)  {\n  ggplot(data = dataset, mapping = aes(x = xvar, y = yvar)) +\n    geom_point(size = pt_size) +\n    geom_smooth()\n}\n\nmake_plot(dataset = mpg, xvar = cty, yvar = hwy)  # Error!\n\nError in `geom_point()`:\n! Problem while computing aesthetics.\nℹ Error occurred in the 1st layer.\nCaused by error:\n! object 'cty' not found\n\n\nThe problem is tidy evaluation, which makes most common coding easier, but makes some less common things harder. Key terms to understand tidy evaluation:\n\nenv-variables = live in the environment (mpg)\ndata-variables = live in data frame or tibble (cty)\ndata masking = tidyverse use data-variables as if they are env-variables. That is, you don’t always need mpg$cty to access cty in tidyverse\n\nThe key idea behind data masking is that it blurs the line between the two different meanings of the word “variable”:\n\nenv-variables are “programming” variables that live in an environment. They are usually created with &lt;-.\ndata-variables are “statistical” variables that live in a data frame. They usually come from data files (e.g. .csv, .xls), or are created manipulating existing variables.\n\nThe solution is to embrace {{ }} data-variables which are user inputs into functions. One way to remember what’s happening, as suggested by our book authors, is to think of {{ }} as looking down a tunnel — {{ var }} will make a dplyr function look inside of var rather than looking for a variable called var. Thus, embracing a variable tells dplyr to use the value stored inside the argument, not the argument as the literal variable name.\nSee Section 25.3 of R4DS for more details (and there are plenty!).\n\n# This will work to make our plot!\nmake_plot &lt;- function(dataset, xvar, yvar, pt_size = 0.75)  {\n  ggplot(data = dataset, mapping = aes(x = {{ xvar }}, y = {{ yvar }})) +\n    geom_point(size = pt_size) +\n    geom_smooth()\n}\n\nmake_plot(dataset = mpg, xvar = cty, yvar = hwy)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nI often wish it were easier to get my own custom summary statistics for numeric variables in EDA rather than using mosaic::favstats(). Using group_by() and summarise() from the tidyverse reads clearly but takes so many lines, but if I only had to write the code once…\n\nsummary6 &lt;- function(data, var) {\n  data |&gt; summarize(\n    min = min({{ var }}, na.rm = TRUE),\n    mean = mean({{ var }}, na.rm = TRUE),\n    median = median({{ var }}, na.rm = TRUE),\n    max = max({{ var }}, na.rm = TRUE),\n    n = n(),\n    n_miss = sum(is.na({{ var }})),\n    .groups = \"drop\"    # to leave the data in an ungrouped state\n  )\n}\n\nmpg |&gt; summary6(hwy)\n\n# A tibble: 1 × 6\n    min  mean median   max     n n_miss\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n1    12  23.4     24    44   234      0\n\n\nEven cooler, I can use my new function with group_by()!\n\nmpg |&gt; \n  group_by(drv) |&gt;\n  summary6(hwy)\n\n# A tibble: 3 × 7\n  drv     min  mean median   max     n n_miss\n  &lt;chr&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n1 4        12  19.2     18    28   103      0\n2 f        17  28.2     28    44   106      0\n3 r        15  21       21    26    25      0\n\n\nYou can even pass conditions into a function using the embrace:\n[Pause to Ponder:] Predict what the code below will do, and (only) then run it to check. Think about: why do we have sort = sort? why not embrace df? why didn’t we need n in the arguments?\n\nnew_function &lt;- function(df, var, condition, sort = TRUE) {\n  df |&gt;\n    filter({{ condition }}) |&gt;\n    count({{ var }}, sort = sort) |&gt;\n    mutate(prop = n / sum(n))\n}\n\nmpg |&gt; new_function(var = manufacturer, \n                    condition = manufacturer %in% c(\"audi\", \n                                                    \"honda\", \n                                                    \"hyundai\", \n                                                    \"nissan\", \n                                                    \"subaru\", \n                                                    \"toyota\", \n                                                    \"volkswagen\")\n                    )\n\n\n\nData-masking vs. tidy-selection (Section 25.3.4)\nWhy doesn’t the following code work?\n\ncount_missing &lt;- function(df, group_vars, x_var) {\n  df |&gt; \n    group_by({{ group_vars }}) |&gt; \n    summarize(\n      n_miss = sum(is.na({{ x_var }})),\n      .groups = \"drop\"\n    )\n}\n\nflights |&gt; \n  count_missing(c(year, month, day), dep_time)\n\nError in `group_by()`:\nℹ In argument: `c(year, month, day)`.\nCaused by error:\n! `c(year, month, day)` must be size 336776 or 1, not 1010328.\n\n\nThe problem is that group_by() uses data-masking rather than tidy-selection; it is selecting certain variables rather than evaluating values of those variables. These are the two most common subtypes of tidy evaluation:\n\nData-masking is used in functions like arrange(), filter(), mutate(), and summarize() that compute with variables. Data masking is an R feature that blends programming variables that live inside environments (env-variables) with statistical variables stored in data frames (data-variables).\n\nTidy-selection is used for functions like select(), relocate(), and rename() that select variables. Tidy selection provides a concise dialect of R for selecting variables based on their names or properties.\n\nMore detail can be found here.\nThe error above can be solved by using the pick() function, which uses tidy selection inside of data masking:\n\ncount_missing &lt;- function(df, group_vars, x_var) {\n  df |&gt; \n    group_by(pick({{ group_vars }})) |&gt; \n    summarize(\n      n_miss = sum(is.na({{ x_var }})),\n      .groups = \"drop\"\n  )\n}\n\nflights |&gt; \n  count_missing(c(year, month, day), dep_time)\n\n# A tibble: 365 × 4\n    year month   day n_miss\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1  2013     1     1      4\n 2  2013     1     2      8\n 3  2013     1     3     10\n 4  2013     1     4      6\n 5  2013     1     5      3\n 6  2013     1     6      1\n 7  2013     1     7      3\n 8  2013     1     8      4\n 9  2013     1     9      5\n10  2013     1    10      3\n# ℹ 355 more rows\n\n\n[Pause to Ponder:] Here’s another nice use of pick(). Predict what the function will do, then run the code to see if you are correct.\n\n# Source: https://twitter.com/pollicipes/status/1571606508944719876\nnew_function &lt;- function(data, rows, cols) {\n  data |&gt; \n    count(pick(c({{ rows }}, {{ cols }}))) |&gt; \n    pivot_wider(\n      names_from = {{ cols }}, \n      values_from = n,\n      names_sort = TRUE,\n      values_fill = 0\n    )\n}\n\nmpg |&gt; new_function(c(manufacturer, model), cyl)",
    "crumbs": [
      "Functions and tidy evaluation"
    ]
  },
  {
    "objectID": "03_functions.html#plot-functions",
    "href": "03_functions.html#plot-functions",
    "title": "Functions and tidy evaluation",
    "section": "Plot functions",
    "text": "Plot functions\nLet’s say you find yourself making a lot of histograms:\n\nflights |&gt; \n  ggplot(aes(x = dep_time)) +\n  geom_histogram(bins = 25)\n\n\n\n\n\n\n\nflights |&gt; \n  ggplot(aes(x = air_time)) +\n  geom_histogram(bins = 35)\n\n\n\n\n\n\n\n\nJust use embrace to create a histogram-making function\n\nhistogram &lt;- function(df, var, bins = NULL) {\n  df |&gt; \n    ggplot(aes(x = {{ var }})) + \n    geom_histogram(bins = bins)\n}\n\nflights |&gt; histogram(air_time, 35)\n\n\n\n\n\n\n\n\nSince histogram() returns a ggplot, you can add any layers you want\n\nflights |&gt; \n  histogram(air_time, 35) +\n  labs(x = \"Flight time (minutes)\", y = \"Number of flights\")\n\n\n\n\n\n\n\n\nYou can also combine data wrangling with plotting. Note that we need the “walrus operator” (:=) since the variable name on the left is being generated with user-supplied data.\n\n# sort counts with highest values at top and counts on x-axis\nsorted_bars &lt;- function(df, var) {\n  df |&gt; \n    mutate({{ var }} := fct_rev(fct_infreq({{ var }})))  |&gt;\n    ggplot(aes(y = {{ var }})) +\n    geom_bar()\n}\n\nflights |&gt; sorted_bars(carrier)\n\n\n\n\n\n\n\n\nFinally, it would be really helpful to label plots based on user inputs. This is a bit more complicated, but still do-able!\nFor this, we’ll need the rlang package. rlang is a low-level package that’s used by just about every other package in the tidyverse because it implements tidy evaluation (as well as many other useful tools).\nCheck out the following update of our histogram() function which uses the englue() function from the rlang package:\n\nhistogram &lt;- function(df, var, bins) {\n  label &lt;- rlang::englue(\"A histogram of {{var}} with binwidth {bins}\")\n  \n  df |&gt; \n    ggplot(aes(x = {{ var }})) + \n    geom_histogram(bins = bins) + \n    labs(title = label)\n}\n\nflights |&gt; histogram(air_time, 35)\n\n\n\n\n\n\n\n\n\nOn Your Own\n\nRewrite this code snippet as a function: x / sum(x, na.rm = TRUE). This code creates weights which sum to 1, where NA values are ignored. Test it for at least two different vectors. (Make sure at least one has NAs!)\nCreate a function to calculate the standard error of a variable, where SE = square root of the variance divided by the sample size. Hint: start with a vector like x &lt;- 0:5 or x &lt;- gss_cat$age and write code to find the SE of x, then turn it into a function to handle any vector x. Note: var is the function to find variance in R and sqrt does square root. length may also be handy. Test your function on two vectors that do not include NAs (i.e. do not worry about removing NAs at this point).\nUse your se function within summarize to get a table of the mean and s.e. of hwy and cty by class in the mpg dataset.\nUse your se function within summarize to get a table of the mean and s.e. of arr_delay and dep_delay by carrier in the flights dataset. Why does the output look like this?\nMake your se function handle NAs with an na.rm option. Test your new function (you can call it se again) on a vector that doesn’t include NA and on the same vector with an added NA. Be sure to check that it gives the expected output with na.rm = TRUE and na.rm = FALSE. Make na.rm = FALSE the default value. Repeat #4. (Hint: be sure when you divide by sample size you don’t count any NAs)\nCreate both_na(), a function that takes two vectors of the same length and returns how many positions have an NA in both vectors. Hint: create two vectors like test_x &lt;- c(1, 2, 3, NA, NA) and test_y &lt;- c(NA, 1, 2, 3, NA) and write code that works for test_x and test_y, then turn it into a function that can handle any x and y. (In this case, the answer would be 1, since both vectors have NA in the 5th position.) Test it for at least one more combination of x and y.\nRun your code from (6) with the following two vectors: test_x &lt;- c(1, 2, 3, NA, NA, NA) and test_y &lt;- c(NA, 1, 2, 3, NA). Did you get the output you wanted or expected? Modify your function using if, else, and stop to print an error if x and y are not the same length. Then test again with test_x, test_y and the sets of vectors you used in (6).\nHere is a way to get not_cancelled flights in the flights dataset:\n\n\nnot_cancelled &lt;- flights %&gt;% \n  filter(!is.na(dep_delay), !is.na(arr_delay))\n\nIs it necessary to check is.na for both departure and arrival? Using summarize, find the number of flights missing departure delay, arrival delay, and both. (Use your new function!)\n\nRead the code for each of the following three functions, puzzle out what they do, and then brainstorm better names.\n\n\nf1 &lt;- function(time1, time2) {\n  hour1 &lt;- time1 %/% 100\n  min1 &lt;- time1 %% 100\n  hour2 &lt;- time2 %/% 100\n  min2 &lt;- time2 %% 100\n  \n  (hour2 - hour1)*60 + (min2 - min1)\n}\n\n\nf2 &lt;- function(lengthcm, widthcm) {\n  (lengthcm / 2.54) * (widthcm / 2.54)\n}\n\n\nf3 &lt;- function(x) {\n  fct_collapse(x, \"non answer\" = c(\"No answer\", \"Refused\", \n                                   \"Don't know\", \"Not applicable\"))\n}\n\n\nExplain what the following function does and demonstrate by running foo1(x) with a few appropriately chosen vectors x. (Hint: set x and run the “guts” of the function piece by piece.)\n\n\nfoo1 &lt;- function(x) {\n  diff &lt;- x[-1] - x[1:(length(x) - 1)]\n  sum(diff &lt; 0)\n}\n\n\nThe foo1() function doesn’t perform well if a vector has missing values. Amend foo1() so that it produces a helpful error message and stops if there are any missing values in the input vector. Show that it works with appropriately chosen vectors x. Be sure you add error = TRUE to your R chunk, or else knitting will fail!\nWrite a function called greet using if, else if, and else to print out “good morning” if it’s before 12 PM, “good afternoon” if it’s between 12 PM and 5 PM, and “good evening” if it’s after 5 PM. Your function should work if you input a time like: greet(time = \"2018-05-03 17:38:01 CDT\") or if you input the current time with greet(time = Sys.time()). [Hint: check out the hour function in the lubridate package]\nModify the summary6() function from earlier to add an argument that gives the user an option to remove missing values, if any exist. Show that your function works for (a) the hwy variable in mpg_tbl &lt;- as_tibble(mpg), and (b) the age variable in gss_cat.\nAdd an argument to (13) to produce summary statistics by group for a second variable (you should now have 4 possible inputs to your function). Show that your function works for (a) the hwy variable in mpg_tbl &lt;- as_tibble(mpg) grouped by drv, and (b) the age variable in gss_cat grouped by partyid.\nCreate a function that has a vector as the input and returns the last value. (Note: Be sure to use a name that does not write over an existing function!)\nSave your final table from (14) and write a function to draw a scatterplot of a measure of center (mean or median - user can choose) vs. a measure of spread (sd or IQR - user can choose), with points sized by sample size, to see if there is constant variance. Each point should be labeled with partyid, and the plot title should reflect the variables chosen by the user.",
    "crumbs": [
      "Functions and tidy evaluation"
    ]
  },
  {
    "objectID": "01_review164.html",
    "href": "01_review164.html",
    "title": "Review of Data Science 1",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\n\nDeterminants of COVID vaccination rates\nFirst, a little detour to describe several alternatives for reading in data:\nIf you navigate to my Github account, and find the 264_fall_2024 repo, there is a Data folder inside. You can then click on vacc_Mar21.csv to see the data we want to download. This link should also get you there, but it’s good to be able to navigate there yourself.\n\n# Approach 1\n1vaccine_data &lt;- read_csv(\"Data/vaccinations_2021.csv\")\n\n# Approach 2\n2vaccine_data &lt;- read_csv(\"~/264_fall_2024/Data/vaccinations_2021.csv\")\n\n# Approach 3\n3vaccine_data &lt;- read_csv(\"https://proback.github.io/264_fall_2024/Data/vaccinations_2021.csv\")\n\n# Approach 4\n4vaccine_data &lt;- read_csv(\"https://raw.githubusercontent.com/proback/264_fall_2024/main/Data/vaccinations_2021.csv\")\n\n\n1\n\nApproach 1: create a Data folder in the same location where this .qmd file resides, and then store vaccinations_2021.csv in that Data folder\n\n2\n\nApproach 2: give R the complete path to the location of vaccinations_2021.csv, starting with Home (~)\n\n3\n\nApproach 3: link to our course webpage, and then know we have a Data folder containing all our csvs\n\n4\n\nApproach 4: navigate to the data in GitHub, hit the Raw button, and copy that link\n\n\n\n\nA recent Stat 272 project examined determinants of covid vaccination rates at the county level. Our data set contains 3053 rows (1 for each county in the US) and 14 columns; here is a quick description of the variables we’ll be using:\n\nstate = state the county is located in\ncounty = name of the county\nregion = region the state is located in\nmetro_status = Is the county considered “Metro” or “Non-metro”?\nrural_urban_code = from 1 (most urban) to 9 (most rural)\nperc_complete_vac = percent of county completely vaccinated as of 11/9/21\ntot_pop = total population in the county\nvotes_Trump = number of votes for Trump in the county in 2020\nvotes_Biden = number of votes for Biden in the county in 2020\nperc_Biden = percent of votes for Biden in the county in 2020\ned_somecol_perc = percent with some education beyond high school (but not a Bachelor’s degree)\ned_bachormore_perc = percent with a Bachelor’s degree or more\nunemployment_rate_2020 = county unemployment rate in 2020\nmedian_HHincome_2019 = county’s median household income in 2019\n\n\nConsider only Minnesota and its surrounding states (Iowa, Wisconsin, North Dakota, and South Dakota). We want to examine the relationship between the percentage who voted for Biden and the percentage of complete vaccinations by state. Generate two plots to examine this relationship:\n\n\nA scatterplot with points and smoothers colored by state. Make sure the legend is ordered in a meaningful way, and include good labels on your axes and your legend. Also leave off the error bars from your smoothers.\nOne plot per state containing a scatterplot and a smoother.\n\nDescribe which plot you prefer and why. What can you learn from your preferred plot?\n\nWe wish to compare the proportions of counties in each region with median household income above the national median ($69,560).\n\n\nFill in the blanks below to produce a segmented bar plot with regions ordered from highest proportion above the median to lowest.\nCreate a table of proportions by region to illustrate that your bar plot in (a) is in the correct order (you should find two regions that are really close when you just try to eyeball differences).\nExplain why we can replace fct_relevel(region, FILL IN CODE) with\n\nmutate(region_sort = fct_reorder(region, median_HHincome_2019 &lt; 69560, .fun = mean))\nbut not\nmutate(region_sort = fct_reorder(region, median_HHincome_2019 &lt; 69560))\n\nvaccine_data |&gt;\n  mutate(HHincome_vs_national = ifelse(median_HHincome_2019 &lt; 69560, FILL IN CODE)) |&gt;\n  mutate(region_sort = fct_relevel(region, FILL IN CODE)) |&gt;\n  ggplot(mapping = aes(x = region_sort, fill = HHincome_vs_national)) +\n    geom_bar(position = \"fill\")\n\n\nWe want to examine the distribution of total county populations and then see how it’s related to vaccination rates.\n\n\nCarefully and thoroughly explain why the two histograms below provide different plots.\n\n\nvaccine_data |&gt;\n  mutate(tot_pop_millions = tot_pop / 1000000) |&gt;\n  ggplot(mapping = aes(x = tot_pop_millions)) +\n    geom_histogram(bins = 40) +\n    labs(x = \"Total population in millions\")\n\n\n\n\n\n\n\nvaccine_data |&gt;\n  mutate(tot_pop_millions = tot_pop %/% 1000000) |&gt;\n  ggplot(mapping = aes(x = tot_pop_millions)) +\n    geom_histogram(bins = 40) +\n    labs(x = \"Total population in millions\")\n\n\n\n\n\n\n\n\n\nFind the top 5 counties in terms of total population.\nPlot a histogram of logged population and describe this distribution.\nPlot the relationship between log population and percent vaccinated using separate colors for Metro and Non-metro counties (be sure there’s no 3rd color used for NAs). Reduce the size and transparency of each point to make the plot more readable. Describe what you can learn from this plot.\n\n\nProduce 3 different plots for illustrating the relationship between the rural_urban_code and percent vaccinated. Hint: you can sometimes turn numeric variables into categorical variables for plotting purposes (e.g. as.factor(), ifelse()).\n\nState your favorite plot, why you like it better than the other two, and what you can learn from your favorite plot. Create an alt text description of your favorite plot, using the Four Ingredient Model. See this link for reminders and references about alt text.\n\nBEFORE running the code below, sketch the plot that will be produced by R. AFTER running the code, describe what conclusion(s) can we draw from this plot?\n\n\nvaccine_data |&gt;\n  filter(!is.na(perc_Biden)) |&gt;\n  mutate(big_states = fct_lump(state, n = 10)) |&gt;\n  group_by(big_states) |&gt;\n  summarize(IQR_Biden = IQR(perc_Biden)) |&gt;\n  mutate(big_states = fct_reorder(big_states, IQR_Biden)) |&gt;\n  ggplot() + \n    geom_point(aes(x = IQR_Biden, y = big_states))\n\n\nIn this question we will focus only on the 12 states in the Midwest (i.e. where region == “Midwest”).\n\n\nCreate a tibble with the following information for each state. Order states from least to greatest state population.\n\n\nnumber of different rural_urban_codes represented among the state’s counties (there are 9 possible)\ntotal state population\nproportion of Metro counties\nmedian unemployment rate\n\n\nUse your tibble in (a) to produce a plot of the relationship between proportion of Metro counties and median unemployment rate. Points should be colored by the number of different rural_urban_codes in a state, but a single linear trend should be fit to all points. What can you conclude from the plot?\n\n\nGenerate an appropriate plot to compare vaccination rates between two subregions of the US: New England (which contains the states Maine, Vermont, New Hampshire, Massachusetts, Connecticut, Rhode Island) and the Upper Midwest (which, according to the USGS, contains the states Minnesota, Wisconsin, Michigan, Illinois, Indiana, and Iowa). What can you conclude from your plot?\n\nIn this next section, we consider a few variables that could have been included in our data set, but were NOT. Thus, you won’t be able to write and test code, but you nevertheless should be able to use your knowledge of the tidyverse to answer these questions.\nHere are the hypothetical variables:\n\nHR_party = party of that county’s US Representative (Republican, Democrat, Independent, Green, or Libertarian)\npeople_per_MD = number of residents per doctor (higher values = fewer doctors)\nperc_over_65 = percent of residents over 65 years old\nperc_white = percent of residents who identify as white\n\n\nHypothetical R chunk #1:\n\n\n# Hypothetical R chunk 1\ntemp &lt;- vaccine_data |&gt;\n  mutate(new_perc_vac = ifelse(perc_complete_vac &gt; 95, NA, perc_complete_vac),\n         MD_group = cut_number(people_per_MD, 3)) |&gt;\n  group_by(MD_group) |&gt;\n  summarise(n = n(),\n            mean_perc_vac = mean(new_perc_vac, na.rm = TRUE),\n            mean_white = mean(perc_white, na.rm = TRUE))\n\n\nDescribe the tibble temp created above. What would be the dimensions? What do rows and columns represent?\nWhat would happen if we replaced new_perc_vac = ifelse(perc_complete_vac &gt; 95, NA, perc_complete_vac) with new_perc_vac = ifelse(perc_complete_vac &gt; 95, perc_complete_vac, NA)?\nWhat would happen if we replaced mean_white = mean(perc_white, na.rm = TRUE) with mean_white = mean(perc_white)?\nWhat would happen if we removed group_by(MD_group)?\n\n\nHypothetical R chunk #2:\n\n\n# Hypothetical R chunk 2\nggplot(data = vaccine_data) +\n  geom_point(mapping = aes(x = perc_over_65, y = perc_complete_vac, \n                           color = HR_party)) +\n  geom_smooth()\n\ntemp &lt;- vaccine_data |&gt;\n  group_by(HR_party) |&gt;\n  summarise(var1 = n()) |&gt;\n  arrange(desc(var1)) |&gt;\n  slice_head(n = 3)\n\nvaccine_data |&gt;\n  ggplot(mapping = aes(x = fct_reorder(HR_party, perc_over_65, .fun = median), \n                       y = perc_over_65)) +\n    geom_boxplot()\n\n\nWhy would the first plot produce an error?\nDescribe the tibble temp created above. What would be the dimensions? What do rows and columns represent?\nWhat would happen if we replaced fct_reorder(HR_party, perc_over_65, .fun = median) with HR_party?\n\n\nHypothetical R chunk #3:\n\n\n# Hypothetical R chunk 3\nvaccine_data |&gt;\n  filter(!is.na(people_per_MD)) |&gt;\n  mutate(state_lump = fct_lump(state, n = 4)) |&gt;\n  group_by(state_lump, rural_urban_code) |&gt;\n  summarise(mean_people_per_MD = mean(people_per_MD)) |&gt;\n  ggplot(mapping = aes(x = rural_urban_code, y = mean_people_per_MD, \n      colour = fct_reorder2(state_lump, rural_urban_code, mean_people_per_MD))) +\n    geom_line()\n\n\nDescribe the tibble piped into the ggplot above. What would be the dimensions? What do rows and columns represent?\nCarefully describe the plot created above.\nWhat would happen if we removed filter(!is.na(people_per_MD))?\nWhat would happen if we replaced fct_reorder2(state_lump, rural_urban_code, mean_people_per_MD) with state_lump?",
    "crumbs": [
      "Review of Data Science 1"
    ]
  },
  {
    "objectID": "02_maps.html",
    "href": "02_maps.html",
    "title": "Creating informative maps",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\n\n# Initial packages required (we'll be adding more)\nlibrary(tidyverse)\nlibrary(mdsr)      # package associated with our MDSR book\n\n\nOpening example\nHere is a simple choropleth map example from Section 3.2.3 of MDSR. Note how we use an underlying map with strategic shading to convey a story about a variable that’s been measured on each country.\n\n# CIACountries is a 236 x 8 data set with information on each country\n#   taken from the CIA factbook - gdp, education, internet use, etc.\nhead(CIACountries)\nCIACountries |&gt;\n  select(country, oil_prod) |&gt;\n  mutate(oil_prod_disc = cut(oil_prod, \n    breaks = c(0, 1e3, 1e5, 1e6, 1e7, 1e8), \n    labels = c(\"&gt;1000\", \"&gt;10,000\", \"&gt;100,000\", \"&gt;1 million\", \n               \"&gt;10 million\"))) |&gt;\n1  mosaic::mWorldMap(key = \"country\") +\n  geom_polygon(aes(fill = oil_prod_disc)) + \n  scale_fill_brewer(\"Oil Prod. (bbl/day)\", na.value = \"white\") +\n  theme(legend.position = \"top\")\n\n\n1\n\nWe won’t use mWorldMap often, but it’s a good quick illustration\n\n\n\n\n\n\n\n\n\n\n\n         country      pop    area oil_prod   gdp educ   roadways net_users\n1    Afghanistan 32564342  652230        0  1900   NA 0.06462444       &gt;5%\n2        Albania  3029278   28748    20510 11900  3.3 0.62613051      &gt;35%\n3        Algeria 39542166 2381741  1420000 14500  4.3 0.04771929      &gt;15%\n4 American Samoa    54343     199        0 13000   NA 1.21105528      &lt;NA&gt;\n5        Andorra    85580     468       NA 37200   NA 0.68376068      &gt;60%\n6         Angola 19625353 1246700  1742000  7300  3.5 0.04125211      &gt;15%\n\n\n\n\nChoropleth Maps\nWhen you have specific regions (e.g. countries, states, counties, census tracts,…) and a value associated with each region.\nA choropleth map will color the entire region according to the value. For example, let’s consider state vaccination data from March 2021.\n\nvaccines &lt;- read_csv(\"https://proback.github.io/264_fall_2024/Data/vacc_Mar21.csv\") \n\nvacc_mar13 &lt;- vaccines |&gt;\n  filter(Date ==\"2021-03-13\") |&gt;\n  select(State, Date, people_vaccinated_per100, share_doses_used, Governor)\n\nvacc_mar13\n\n# A tibble: 50 × 5\n   State       Date       people_vaccinated_per100 share_doses_used Governor\n   &lt;chr&gt;       &lt;date&gt;                        &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;   \n 1 Alabama     2021-03-13                     17.2            0.671 R       \n 2 Alaska      2021-03-13                     27.0            0.686 R       \n 3 Arizona     2021-03-13                     21.5            0.821 R       \n 4 Arkansas    2021-03-13                     19.2            0.705 R       \n 5 California  2021-03-13                     20.3            0.726 D       \n 6 Colorado    2021-03-13                     20.8            0.801 D       \n 7 Connecticut 2021-03-13                     26.2            0.851 D       \n 8 Delaware    2021-03-13                     20.2            0.753 D       \n 9 Florida     2021-03-13                     20.1            0.766 R       \n10 Georgia     2021-03-13                     15.2            0.674 R       \n# ℹ 40 more rows\n\n\nThe tricky part of choropleth maps is getting the shapes (polygons) that make up the regions. This is really a pretty complex set of lines for R to draw!\nLuckily, some maps are already created in R in the maps package.\n\nlibrary(maps)\nus_states &lt;- map_data(\"state\")\nhead(us_states)\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\nus_states |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(fill = \"white\", color = \"black\")\n\n\n\n\n\n\n\n\n[Pause to ponder:] What might the group and order columns represent?\nOther maps provided by the maps package include US counties, France, Italy, New Zealand, and two different views of the world. If you want maps of other countries or regions, you can often find them online.\nWhere the really cool stuff happens is when we join our data to the us_states dataframe. Notice that the state name appears in the “region” column of us_states, and that the state name is in all small letters. In vacc_mar13, the state name appears in the State column and is in title case. Thus, we have to be very careful when we join the state vaccine info to the state geography data.\nRun this line by line to see what it does:\n\nvacc_mar13 &lt;- vacc_mar13 |&gt;\n  mutate(State = str_to_lower(State))\n\nvacc_mar13 |&gt;\n  right_join(us_states, by = c(\"State\" = \"region\")) |&gt;\n  rename(region = State) |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(aes(fill = people_vaccinated_per100), color = \"black\")\n\n\n\n\n\n\n\n\noops, New York appears to be a problem.\n\nvacc_mar13 |&gt;\n  anti_join(us_states, by = c(\"State\" = \"region\"))\n\n# A tibble: 3 × 5\n  State          Date       people_vaccinated_per100 share_doses_used Governor\n  &lt;chr&gt;          &lt;date&gt;                        &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;   \n1 alaska         2021-03-13                     27.0            0.686 R       \n2 hawaii         2021-03-13                     22.8            0.759 D       \n3 new york state 2021-03-13                     21.7            0.764 D       \n\nus_states |&gt;\n  anti_join(vacc_mar13, by = c(\"region\" = \"State\")) |&gt;\n  count(region)\n\n                region   n\n1 district of columbia  10\n2             new york 495\n\n\n[Pause to ponder:] What did we learn by running anti_join() above?\nNotice that the us_states map also includes only the contiguous 48 states. This gives an example of creating really beautiful map insets for Alaska and Hawaii.\n\nvacc_mar13 &lt;- vacc_mar13 |&gt;\n  mutate(State = str_replace(State, \" state\", \"\"))\n\nvacc_mar13 |&gt;\n  anti_join(us_states, by = c(\"State\" = \"region\"))\n\n# A tibble: 2 × 5\n  State  Date       people_vaccinated_per100 share_doses_used Governor\n  &lt;chr&gt;  &lt;date&gt;                        &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;   \n1 alaska 2021-03-13                     27.0            0.686 R       \n2 hawaii 2021-03-13                     22.8            0.759 D       \n\nus_states |&gt;\n  anti_join(vacc_mar13, by = c(\"region\" = \"State\")) %&gt;%\n  count(region)\n\n                region  n\n1 district of columbia 10\n\n\nBetter.\n\nlibrary(viridis) # for color schemes\nvacc_mar13 |&gt;\n  right_join(us_states, by = c(\"State\" = \"region\")) |&gt;\n  rename(region = State) |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(aes(fill = people_vaccinated_per100), color = \"black\") + \n  labs(fill = \"People Vaccinated\\nper 100 pop.\") +\n1  coord_map() +\n2  theme_void() +\n3  scale_fill_viridis()\n\n\n1\n\nThis scales the longitude and latitude so that the shapes look correct. coord_quickmap() can also work here - it’s less exact but faster.\n\n2\n\nThis theme can give you a really clean look\n\n3\n\nYou can change the fill scale for different color schemes.\n\n\n\n\n\n\n\n\n\n\n\nYou can also use a categorical variable to color regions:\n\nvacc_mar13 |&gt;\n  right_join(us_states, by = c(\"State\" = \"region\")) |&gt;\n  rename(region = State) |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(aes(fill = Governor), color = \"darkgrey\", linewidth = 0.2) + \n  labs(fill = \"Governor\") +\n  coord_map() + \n  theme_void() +  \n1  scale_fill_manual(values = c(\"blue\", \"red\"))\n\n\n1\n\nYou can change the fill scale for different color schemes.\n\n\n\n\n\n\n\n\n\n\n\nNote: Map projections are actually pretty complicated, especially if you’re looking at large areas (e.g. world maps) or drilling down to very small regions where a few feet can make a difference (e.g. tracking a car on a map of roads). It’s impossible to preserve both shape and area when projecting an (imperfect) sphere onto a flat surface, so that’s why you sometimes see such different maps of the world. This is why packages like maps which connect latitude-longitude points are being phased out in favor of packages like sf with more GIS functionality. We won’t get too deep into GIS in this class, but to learn more, take Spatial Data Analysis!!\n\n\nMultiple maps!\nYou can still use data viz tools from Data Science 1 (like facetting) to create things like time trends in maps:\n\nlibrary(lubridate)\nweekly_vacc &lt;- vaccines |&gt;\n  mutate(State = str_to_lower(State)) |&gt;\n  mutate(State = str_replace(State, \" state\", \"\"),\n         week = week(Date)) |&gt;\n  group_by(week, State) |&gt;\n  summarize(date = first(Date),\n            mean_daily_vacc = mean(daily_vaccinated/est_population*1000)) |&gt;\n  right_join(us_states, by =c(\"State\" = \"region\")) |&gt;\n  rename(region = State)\n\nweekly_vacc |&gt;\n  filter(week &gt; 2, week &lt; 11) |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(aes(fill = mean_daily_vacc), color = \"darkgrey\", \n               linewidth = 0.1) + \n  labs(fill = \"Weekly Average Daily Vaccinations per 1000\") +\n  coord_map() + \n  theme_void() + \n  scale_fill_viridis() + \n  facet_wrap(~date) + \n  theme(legend.position = \"bottom\") \n\n\n\n\n\n\n\n\n[Pause to ponder:] are we bothered by the warning about many-to-many when you run the code above?\n\n\nOther cool state maps\n\nstatebin (square representation of states)\n\nlibrary(statebins) # may need to install\n\nvacc_mar13 |&gt;\n  mutate(State = str_to_title(State)) |&gt;\n  statebins(state_col = \"State\",\n            value_col = \"people_vaccinated_per100\") + \n1  theme_statebins() +\n  labs(fill = \"People Vaccinated per 100\")\n\n\n1\n\nOne nice layout. You can customize with usual ggplot themes.\n\n\n\n\n\n\n\n\n\n\n\n[Pause to ponder:] Why might one use a map like above instead of our previous choropleth maps?\nI used this example to create the code above. The original graph is located here.\n\n\n\nInteractive point maps with leaflet\nTo add even more power and value to your plots, we can add interactivity. For now, we will use the leaflet package, but later in the course we will learn even more powerful and flexible approaches for creating interactive plots and webpages.\nFor instance, here is a really simple plot with a pop-up window:\n\nlibrary(leaflet)\n\nleaflet() |&gt; \n1  addTiles() |&gt;\n2  setView(-93.1832, 44.4597, zoom = 17) |&gt;\n3  addPopups(-93.1832, 44.4597, 'Here is the &lt;b&gt;Regents Hall of Mathematical Sciences&lt;/b&gt;, home of the Statistics and Data Science program at St. Olaf College')\n\n\n1\n\naddTiles() uses OpenStreetMap, an awesome open-source mapping resource, as the default tile layer (background map)\n\n2\n\nsetView() centers the map at a specific latitude and longitude, then zoom controls how much of the surrounding area is shown\n\n3\n\nadd a popup message (with html formatting) that can be clicked on or off\n\n\n\n\n\n\n\n\nLeaflet is not part of the tidyverse, but the structure of its code is pretty similar and it also plays well with piping.\nLet’s try pop-up messages with a data set containing Airbnb listings in the Boston area:\n\nleaflet() |&gt;\n    addTiles() |&gt;\n    setView(lng = mean(airbnb.df$Long), lat = mean(airbnb.df$Lat), \n            zoom = 13) |&gt; \n    addCircleMarkers(data = airbnb.df,\n        lat = ~ Lat, \n        lng = ~ Long, \n        popup = ~ AboutListing, \n        radius = ~ S_Accomodates,  \n        # These last options describe how the circles look\n        weight = 2,\n        color = \"red\", \n        fillColor = \"yellow\")\n\n\n\n\n\n[Pause to ponder:] List similarities and differences between leaflet plots and ggplots.\n\n\nInteractive choropleth maps with leaflet\nOK. Now let’s see if we can put things together and duplicate the interactive choropleth map found here showing population density by state in the US.\n\nA preview to shapefiles and the sf package\n\n1library(sf)\n2states &lt;- read_sf(\"https://rstudio.github.io/leaflet/json/us-states.geojson\")\n3class(states)\nstates\n\n\n1\n\nsf stands for “simple features”\n\n2\n\nFrom https://leafletjs.com/examples/choropleth/us-states.js\n\n3\n\nNote that states has class sf in addition to the usual tbl and df\n\n\n\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\nSimple feature collection with 52 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -188.9049 ymin: 17.92956 xmax: -65.6268 ymax: 71.35163\nGeodetic CRS:  WGS 84\n# A tibble: 52 × 4\n   id    name                  density                                  geometry\n   &lt;chr&gt; &lt;chr&gt;                   &lt;dbl&gt;                        &lt;MULTIPOLYGON [°]&gt;\n 1 01    Alabama                 94.6  (((-87.3593 35.00118, -85.60667 34.98475…\n 2 02    Alaska                   1.26 (((-131.602 55.11798, -131.5692 55.28229…\n 3 04    Arizona                 57.0  (((-109.0425 37.00026, -109.048 31.33163…\n 4 05    Arkansas                56.4  (((-94.47384 36.50186, -90.15254 36.4963…\n 5 06    California             242.   (((-123.2333 42.00619, -122.3789 42.0116…\n 6 08    Colorado                49.3  (((-107.9197 41.00391, -105.729 40.99843…\n 7 09    Connecticut            739.   (((-73.05353 42.03905, -71.79931 42.0226…\n 8 10    Delaware               464.   (((-75.41409 39.80446, -75.5072 39.68396…\n 9 11    District of Columbia 10065    (((-77.03526 38.99387, -76.90929 38.8952…\n10 12    Florida                353.   (((-85.49714 30.99754, -85.00421 31.0030…\n# ℹ 42 more rows\n\n\nFor maps in leaflet that show boundaries and not just points, we need to input a shapefile rather than a series of latitude-longitude combinations like we did for the maps package. In the example we’re emulating, they use the read_sf() function from the sf package to read in data. While our us_states data frame from the maps package contained 15537 rows, our simple features object states contains only 52 rows - one per state. Importantly, states contains a column called geometry, which is a “multipolygon” with all the information necessary to draw a specific state. Also, while states can be treated as a tibble or data frame, it is also an sf class object with a specific “geodetic coordinate reference system”. Again, take Spatial Data Analysis for more on shapefiles and simple features!\nNote also that the authors of this example have already merged state population densities with state geometries, but if we wanted to merge in other state characteristics using the name column as a key, we could definitely do this!\nFirst we’ll start with a static plot using a simple features object and geom_sf():\n\n# Create density bins as on the webpage\nstate_plotting_sf &lt;- states |&gt;\n  mutate(density_intervals = cut(density, n = 8,\n          breaks = c(0, 10, 20, 50, 100, 200, 500, 1000, Inf))) |&gt;\n  filter(!(name %in% c(\"Alaska\", \"Hawaii\", \"Puerto Rico\")))\n\nggplot(data = state_plotting_sf) + \n  geom_sf(aes(fill = density_intervals), colour = \"white\", linetype = 2) + \n#  geom_sf_label(aes(label = density)) +   # labels too busy here\n  theme_void() +  \n  scale_fill_brewer(palette = \"YlOrRd\") \n\n\n\n\n\n\n\n\nNow let’s use leaflet to create an interactive plot!\n\n# Create our own category bins for population densities\n#   and assign the yellow-orange-red color palette\nbins &lt;- c(0, 10, 20, 50, 100, 200, 500, 1000, Inf)\npal &lt;- colorBin(\"YlOrRd\", domain = states$density, bins = bins)\n\n# Create labels that pop up when we hover over a state.  The labels must\n#   be part of a list where each entry is tagged as HTML code.\nlibrary(htmltools)\nlibrary(glue)\n\nstates &lt;- states |&gt;\n  mutate(labels = str_c(name, \": \", density, \" people / sq mile\"))\n\n# If want more HTML formatting, use these lines instead of those above:\n#states &lt;- states |&gt;\n#  mutate(labels = glue(\"&lt;strong&gt;{name}&lt;/strong&gt;&lt;br/&gt;{density} people / #mi&lt;sup&gt;2&lt;/sup&gt;\"))\n\nlabels &lt;- lapply(states$labels, HTML)\n\nleaflet(states) %&gt;%\n  setView(-96, 37.8, 4) %&gt;%\n  addTiles() %&gt;%\n  addPolygons(\n    fillColor = ~pal(density),\n    weight = 2,\n    opacity = 1,\n    color = \"white\",\n    dashArray = \"3\",\n    fillOpacity = 0.7,\n    highlightOptions = highlightOptions(\n      weight = 5,\n      color = \"#666\",\n      dashArray = \"\",\n      fillOpacity = 0.7,\n      bringToFront = TRUE),\n    label = labels,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n      textsize = \"15px\",\n      direction = \"auto\")) %&gt;%\n  addLegend(pal = pal, values = ~density, opacity = 0.7, title = NULL,\n    position = \"bottomright\")\n\n\n\n\n\n[Pause to ponder:] Pick several formatting options in the code above, determine what they do, and then change them to create a customized look.\n\n\n\nOn Your Own\nThe states dataset in the poliscidata package contains 135 variables on each of the 50 US states. See here for more detail.\nYour task is to create a two meaningful choropleth plots, one using a numeric variable and one using a categorical variable from poliscidata::states. You should make two versions of each plot: a static plot using the maps package and ggplot(), and an interactive plot using the sf package and leaflet(). Write a sentence or two describing what you can learn from each plot.\nHere’s some R code and hints to get you going:\n\n# Get info to draw US states for geom_polygon (connect the lat-long points)\nlibrary(maps)\nstates_polygon &lt;- as_tibble(map_data(\"state\")) |&gt;\n  select(region, group, order, lat, long)\n\n# See what the state (region) levels look like in states_polygon\nunique(states_polygon$region)\n\n [1] \"alabama\"              \"arizona\"              \"arkansas\"            \n [4] \"california\"           \"colorado\"             \"connecticut\"         \n [7] \"delaware\"             \"district of columbia\" \"florida\"             \n[10] \"georgia\"              \"idaho\"                \"illinois\"            \n[13] \"indiana\"              \"iowa\"                 \"kansas\"              \n[16] \"kentucky\"             \"louisiana\"            \"maine\"               \n[19] \"maryland\"             \"massachusetts\"        \"michigan\"            \n[22] \"minnesota\"            \"mississippi\"          \"missouri\"            \n[25] \"montana\"              \"nebraska\"             \"nevada\"              \n[28] \"new hampshire\"        \"new jersey\"           \"new mexico\"          \n[31] \"new york\"             \"north carolina\"       \"north dakota\"        \n[34] \"ohio\"                 \"oklahoma\"             \"oregon\"              \n[37] \"pennsylvania\"         \"rhode island\"         \"south carolina\"      \n[40] \"south dakota\"         \"tennessee\"            \"texas\"               \n[43] \"utah\"                 \"vermont\"              \"virginia\"            \n[46] \"washington\"           \"west virginia\"        \"wisconsin\"           \n[49] \"wyoming\"             \n\n# Get info to draw US states for geom_sf and leaflet (simple features object \n#   with multipolygon geometry column)\nlibrary(sf)\nstates_sf &lt;- read_sf(\"https://rstudio.github.io/leaflet/json/us-states.geojson\") |&gt;\n  select(name, geometry)\n\n# See what the state (name) levels look like in states_sf\nunique(states_sf$name)\n\n [1] \"Alabama\"              \"Alaska\"               \"Arizona\"             \n [4] \"Arkansas\"             \"California\"           \"Colorado\"            \n [7] \"Connecticut\"          \"Delaware\"             \"District of Columbia\"\n[10] \"Florida\"              \"Georgia\"              \"Hawaii\"              \n[13] \"Idaho\"                \"Illinois\"             \"Indiana\"             \n[16] \"Iowa\"                 \"Kansas\"               \"Kentucky\"            \n[19] \"Louisiana\"            \"Maine\"                \"Maryland\"            \n[22] \"Massachusetts\"        \"Michigan\"             \"Minnesota\"           \n[25] \"Mississippi\"          \"Missouri\"             \"Montana\"             \n[28] \"Nebraska\"             \"Nevada\"               \"New Hampshire\"       \n[31] \"New Jersey\"           \"New Mexico\"           \"New York\"            \n[34] \"North Carolina\"       \"North Dakota\"         \"Ohio\"                \n[37] \"Oklahoma\"             \"Oregon\"               \"Pennsylvania\"        \n[40] \"Rhode Island\"         \"South Carolina\"       \"South Dakota\"        \n[43] \"Tennessee\"            \"Texas\"                \"Utah\"                \n[46] \"Vermont\"              \"Virginia\"             \"Washington\"          \n[49] \"West Virginia\"        \"Wisconsin\"            \"Wyoming\"             \n[52] \"Puerto Rico\"         \n\n# Load in state-wise data for filling our choropleth maps\n#   (Note that I selected my two variables of interest to simplify)\nlibrary(poliscidata)   # may have to install first\npolisci_data &lt;- as_tibble(poliscidata::states) |&gt;\n  select(state, carfatal07, cook_index3)\n\n# See what the state (state) levels look like in polisci_data\nunique(polisci_data$state)   # can't see trailing spaces but can see\n\n [1] Alaska                                    \n [2] Alabama                                   \n [3] Arkansas                                  \n [4] Arizona                                   \n [5] California                                \n [6] Colorado                                  \n [7] Connecticut                               \n [8] Delaware                                  \n [9] Florida                                   \n[10] Georgia                                   \n[11] Hawaii                                    \n[12] Iowa                                      \n[13] Idaho                                     \n[14] Illinois                                  \n[15] Indiana                                   \n[16] Kansas                                    \n[17] Kentucky                                  \n[18] Louisiana                                 \n[19] Massachusetts                             \n[20] Maryland                                  \n[21] Maine                                     \n[22] Michigan                                  \n[23] Minnesota                                 \n[24] Missouri                                  \n[25] Mississippi                               \n[26] Montana                                   \n[27] NorthCarolina                             \n[28] NorthDakota                               \n[29] Nebraska                                  \n[30] NewHampshire                              \n[31] NewJersey                                 \n[32] NewMexico                                 \n[33] Nevada                                    \n[34] NewYork                                   \n[35] Ohio                                      \n[36] Oklahoma                                  \n[37] Oregon                                    \n[38] Pennsylvania                              \n[39] RhodeIsland                               \n[40] SouthCarolina                             \n[41] SouthDakota                               \n[42] Tennessee                                 \n[43] Texas                                     \n[44] Utah                                      \n[45] Virginia                                  \n[46] Vermont                                   \n[47] Washington                                \n[48] Wisconsin                                 \n[49] WestVirginia                              \n[50] Wyoming                                   \n50 Levels: Alabama                                    ...\n\n                             #   lack of internal spaces\nprint(polisci_data)   # can see trailing spaces\n\n# A tibble: 50 × 3\n   state                                        carfatal07 cook_index3\n   &lt;fct&gt;                                             &lt;dbl&gt; &lt;fct&gt;      \n 1 \"Alaska                                    \"       15.2 More Rep   \n 2 \"Alabama                                   \"       25.9 More Rep   \n 3 \"Arkansas                                  \"       23.7 More Rep   \n 4 \"Arizona                                   \"       17.6 Even       \n 5 \"California                                \"       11.7 More Dem   \n 6 \"Colorado                                  \"       12.3 Even       \n 7 \"Connecticut                               \"        8.7 More Dem   \n 8 \"Delaware                                  \"       13.6 More Dem   \n 9 \"Florida                                   \"       18.1 Even       \n10 \"Georgia                                   \"       18.5 Even       \n# ℹ 40 more rows\n\n\nR code hints:\n\nstringr functions like str_squish and str_to_lower and str_replace_all (be sure to carefully look at your keys!)\n*_join functions (make sure they preserve classes)\nfilter so that you only have 48 contiguous states (and maybe DC)\nfor help with colors: https://rstudio.github.io/leaflet/reference/colorNumeric.html\nbe sure labels pop up when scrolling with leaflet\n\n\n# Make sure all keys have the same format before joining:\n#   all lower case, no internal or external spaces\n\n\n# Now we can merge data sets together for the static and the interactive plots\n\n\n# Merge with states_polygon (static)\n\n# Check that merge worked for 48 contiguous states\n\n\n# Merge with states_sf (static or interactive)\n\n# Check that merge worked for 48 contiguous states\n\nNumeric variable (static plot):\nNumeric variable (interactive plot):\n\n# it's okay to skip a legend here\n\nCategorical variable (static plot):\n\n# be really careful with matching color order to factor level order\n\nCategorical variable (interactive plot):\n\n# may use colorFactor() here",
    "crumbs": [
      "Creating informative maps"
    ]
  },
  {
    "objectID": "04_code_quality.html",
    "href": "04_code_quality.html",
    "title": "Code quality",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.",
    "crumbs": [
      "Code quality"
    ]
  },
  {
    "objectID": "04_code_quality.html#code-style",
    "href": "04_code_quality.html#code-style",
    "title": "Code quality",
    "section": "Code style",
    "text": "Code style\nWe are going to take a timeout at this point to focus a little on code quality. Chapter 4 in R4DS provides a nice introduction to code style and why it’s important. As they do in that chapter, we will follow the tidyverse style guide in this class.\nBased on those resources, can you improve this poorly styled code chunk using the data from our DS1 Review activity?\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nVACCINE.DATA &lt;- read_csv(\"https://proback.github.io/264_fall_2024/Data/vaccinations_2021.csv\")\n\nRows: 3053 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): state, county, region, metro_status\ndbl (10): rural_urban_code, perc_complete_vac, tot_pop, votes_Trump, votes_B...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nVACCINE.DATA |&gt; filter(state %in% c(\"Minnesota\",\"Iowa\",\"Wisconsin\",\"North Dakota\",\"South Dakota\")) |&gt;\n  mutate(state_ordered=fct_reorder2(state,perc_Biden,perc_complete_vac),prop_Biden=perc_Biden/100,prop_complete_vac=perc_complete_vac/100) |&gt;\nggplot(mapping = aes(x = prop_Biden, y = prop_complete_vac, \n                       color = state_ordered)) +\ngeom_point() + geom_smooth(se = FALSE) +\nlabs(color = \"State\", x = \"Proportion of Biden votes\",\n     y = \"Proportion completely vaccinated\", title = \"The positive relationship between Biden votes and \\n vaccination rates by county differs by state\") +     \ntheme(axis.title = element_text(size=10), plot.title = element_text(size=12))  \n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'",
    "crumbs": [
      "Code quality"
    ]
  },
  {
    "objectID": "04_code_quality.html#code-comments",
    "href": "04_code_quality.html#code-comments",
    "title": "Code quality",
    "section": "Code comments",
    "text": "Code comments\nPlease read Fostering Better Coding Practices for Data Scientists, which lays out a nice case for the importance of teaching good coding practices. In particular, their Top 10 List can help achieve the four Cs (correctness, clarity, containment, and consistency) that typify high-quality code:\n\nChoose good names.\nFollow a style guide consistently.\nCreate documents using tools that support reproducible workflows.\nSelect a coherent, minimal, yet powerful tool kit.\nDon’t Repeat Yourself (DRY).\nTake advantage of a functional programming style.\nEmploy consistency checks.\nLearn how to debug and to ask for help.\nGet (version) control of the situation.\nBe multilingual.\n\nPlease also read the Stack Overflow blog on Best Practices for Writing Code Comments with their set of 9 rules:\n\nRule 1: Comments should not duplicate the code.\nRule 2: Good comments do not excuse unclear code.\nRule 3: If you can’t write a clear comment, there may be a problem with the code.\nRule 4: Comments should dispel confusion, not cause it.\nRule 5: Explain unidiomatic code in comments.\nRule 6: Provide links to the original source of copied code.\nRule 7: Include links to external references where they will be most helpful.\nRule 8: Add comments when fixing bugs.\nRule 9: Use comments to mark incomplete implementations.\n\nIn your projects and homework for this course, we will look for good style and good commenting to optimize your abilities as a collaborating data scientist!",
    "crumbs": [
      "Code quality"
    ]
  },
  {
    "objectID": "06_data_types.html",
    "href": "06_data_types.html",
    "title": "Data types",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\nThis leans on parts of R4DS Chapter 27: A field guide to base R, in addition to parts of the first edition of R4DS.\n# Initial packages required\nlibrary(tidyverse)",
    "crumbs": [
      "Data types"
    ]
  },
  {
    "objectID": "06_data_types.html#what-is-a-vector",
    "href": "06_data_types.html#what-is-a-vector",
    "title": "Data types",
    "section": "What is a vector?",
    "text": "What is a vector?\nWe’ve seen them:\n\n1:5 \n\n[1] 1 2 3 4 5\n\nc(3, 6, 1, 7)\n\n[1] 3 6 1 7\n\nc(\"a\", \"b\", \"c\")\n\n[1] \"a\" \"b\" \"c\"\n\nx &lt;- c(0:3, NA)\nis.na(x)\n\n[1] FALSE FALSE FALSE FALSE  TRUE\n\nsqrt(x)\n\n[1] 0.000000 1.000000 1.414214 1.732051       NA\n\n\nThis doesn’t really fit the mathematical definition of a vector (direction and magnitude)… its really just some numbers (or letters, or TRUE’s…) strung together.",
    "crumbs": [
      "Data types"
    ]
  },
  {
    "objectID": "06_data_types.html#types-of-vectors",
    "href": "06_data_types.html#types-of-vectors",
    "title": "Data types",
    "section": "Types of vectors",
    "text": "Types of vectors\nAtomic vectors are homogeneous… they can contain only one “type”. Types include logical, integer, double, and character (Also complex and raw, but we will ignore those).\nLists can be heterogeneous…. they can be made up of vectors of different types, or even of other lists!\nNULL denotes the absence of a vector (whereas NA denotes absence of a value in a vector).\nLet’s check out some vector types:\n\nx &lt;- c(0:3, NA)\ntypeof(x)\n\n[1] \"integer\"\n\nsqrt(x)\n\n[1] 0.000000 1.000000 1.414214 1.732051       NA\n\ntypeof(sqrt(x))\n\n[1] \"double\"\n\n\n[Pause to Ponder:] State the types of the following vectors, then use typeof() to check:\n\nis.na(x)\n\n[1] FALSE FALSE FALSE FALSE  TRUE\n\nx &gt; 2\n\n[1] FALSE FALSE FALSE  TRUE    NA\n\nc(\"apple\", \"banana\", \"pear\")\n\n[1] \"apple\"  \"banana\" \"pear\"  \n\n\nA logical vector can be implicitly coerced to numeric - T to 1 and F to 0\n\nx &lt;- sample(1:20, 100, replace = TRUE)\ny &lt;- x &gt; 10\nis_logical(y)\n\n[1] TRUE\n\nas.numeric(y)\n\n  [1] 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 1\n [38] 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1 1 1 1\n [75] 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1\n\nsum(y)  # how many are greater than 10?\n\n[1] 61\n\nmean(y) # what proportion are greater than 10?\n\n[1] 0.61\n\n\nIf there are multiple data types in a vector, then the most complex type wins, because you cannot mix types in a vector (although you can in a list)\n\ntypeof(c(TRUE, 1L))\n\n[1] \"integer\"\n\ntypeof(c(1L, 1.5))\n\n[1] \"double\"\n\ntypeof(c(1.5, \"a\"))\n\n[1] \"character\"\n\n\nIntegers are whole numbers. “double” refers to “Double-precision” representation of fractional values… don’t worry about the details here (Google it if you care), but just recognize that computers have to round at some point. “Double-precision” tries to store numbers precisely and efficiently.\nBut weird stuff can happen:\n\ny &lt;- sqrt(2) ^2\ny\n\n[1] 2\n\ny == 2\n\n[1] FALSE\n\n\nthe function near is better here:\n\nnear(y, 2)\n\n[1] TRUE\n\n\nAnd doubles have a couple extra possible values: Inf, -Inf, and NaN, in addition to NA:\n\n1/0\n\n[1] Inf\n\n-1/0\n\n[1] -Inf\n\n0/0\n\n[1] NaN\n\nInf*0\n\n[1] NaN\n\nInf/Inf\n\n[1] NaN\n\nInf/NA\n\n[1] NA\n\nInf*NA\n\n[1] NA\n\n\nIt’s not a good idea to check for special values (NA, NaN, Inf, -Inf) with ==. Use these instead:\n\nis.finite(Inf)\n\n[1] FALSE\n\nis.infinite(Inf)\n\n[1] TRUE\n\nis.finite(NA)\n\n[1] FALSE\n\nis.finite(NaN)\n\n[1] FALSE\n\nis.infinite(NA)\n\n[1] FALSE\n\nis.infinite(NaN)\n\n[1] FALSE\n\nis.na(NA)\n\n[1] TRUE\n\nis.na(NaN)\n\n[1] TRUE\n\nis.nan(NA)\n\n[1] FALSE\n\nis.nan(NaN)\n\n[1] TRUE\n\nis.na(Inf)\n\n[1] FALSE\n\nis.nan(Inf)\n\n[1] FALSE\n\n\nWhy not use == ?\n\n# Sometimes it works how you think it would:\n1/0\n\n[1] Inf\n\n1/0 == Inf\n\n[1] TRUE\n\n# Sometimes it doesn't (Because NA is contagious!)\n0/0\n\n[1] NaN\n\n0/0 == NaN    \n\n[1] NA\n\nNA == NA \n\n[1] NA\n\nx &lt;- c(0, 1, 1/0, 0/0)\n# Doesn't work well\nx == NA\n\n[1] NA NA NA NA\n\nx == Inf\n\n[1] FALSE FALSE  TRUE    NA\n\n# Works better\nis.na(x)\n\n[1] FALSE FALSE FALSE  TRUE\n\nis.infinite(x)\n\n[1] FALSE FALSE  TRUE FALSE\n\n\nAnother note: technically, each type of vector has its own type of NA… this usually doesn’t matter, but is good to know in case one day you get very very strange errors.",
    "crumbs": [
      "Data types"
    ]
  },
  {
    "objectID": "06_data_types.html#augmented-vectors",
    "href": "06_data_types.html#augmented-vectors",
    "title": "Data types",
    "section": "Augmented vectors",
    "text": "Augmented vectors\nVectors may carry additional metadata in the form of attributes which create augmented vectors.\n\nFactors are built on top of integer vectors\nDates and date-times are built on top of numeric (either integer or double) vectors\nData frames and tibbles are built on top of lists",
    "crumbs": [
      "Data types"
    ]
  },
  {
    "objectID": "06_data_types.html#naming-items-in-vectors",
    "href": "06_data_types.html#naming-items-in-vectors",
    "title": "Data types",
    "section": "Naming items in vectors",
    "text": "Naming items in vectors\nEach element of a vector can be named, either when it is created or with setnames from package purrr.\n\nx &lt;- c(a = 1, b = 2, c = 3)\nx\n\na b c \n1 2 3 \n\n\nThis is more commonly used when you’re dealing with lists or tibbles (which are just a special kind of list!)\n\ntibble(x = 1:4, y = 5:8)\n\n# A tibble: 4 × 2\n      x     y\n  &lt;int&gt; &lt;int&gt;\n1     1     5\n2     2     6\n3     3     7\n4     4     8",
    "crumbs": [
      "Data types"
    ]
  },
  {
    "objectID": "06_data_types.html#subsetting-vectors",
    "href": "06_data_types.html#subsetting-vectors",
    "title": "Data types",
    "section": "Subsetting vectors",
    "text": "Subsetting vectors\nSo many ways to do this.\nI. Subset with numbers.\nUse positive integers to keep elements at those positions:\n\nx &lt;- c(\"one\", \"two\", \"three\", \"four\", \"five\")\nx[1]\n\n[1] \"one\"\n\nx[4]\n\n[1] \"four\"\n\nx[1:2]\n\n[1] \"one\" \"two\"\n\n\n[Pause to Ponder:] How would you extract values 1 and 3?\nYou can also repeat values:\n\nx[c(1, 1, 3, 3, 5, 5, 2, 2, 4, 4, 4)]\n\n [1] \"one\"   \"one\"   \"three\" \"three\" \"five\"  \"five\"  \"two\"   \"two\"   \"four\" \n[10] \"four\"  \"four\" \n\n\nUse negative integers to drop elements:\n\nx[-3]\n\n[1] \"one\"  \"two\"  \"four\" \"five\"\n\n\n[Pause to Ponder:] How would you drop values 2 and 4?\nWhat happens if you mix positive and negative values?\n\nx[c(1, -1)]\n\nError in x[c(1, -1)]: only 0's may be mixed with negative subscripts\n\n\nYou can just subset with 0… this isn’t usually helpful, except perhaps for testing weird cases when you write functions:\n\nx[0]\n\ncharacter(0)\n\n\n\nSubset with a logical vector (“Logical subsetting”).\n\n\nx == \"one\"\n\n[1]  TRUE FALSE FALSE FALSE FALSE\n\nx[x == \"one\"]\n\n[1] \"one\"\n\ny &lt;- c(10, 3, NA, 5, 8, 1, NA)\nis.na(y)\n\n[1] FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE\n\ny[!is.na(y)]\n\n[1] 10  3  5  8  1\n\n\n[Pause to Ponder:] Extract values of y that are less than or equal to 5 (what happens to NAs?). Then extract all non-missing values of y that are less than or equal to 5\n\nIf named, subset with a character vector.\n\n\nz &lt;- c(abc = 1, def = 2, xyz = 3)\nz[\"abc\"]\n\nabc \n  1 \n\n# A slightly more useful example:\nsummary(y)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n    1.0     3.0     5.0     5.4     8.0    10.0       2 \n\nsummary(y)[\"Min.\"]\n\nMin. \n   1 \n\n\n[Pause to Ponder:] Extract abc and xyz from the vector z, and then extract the mean from summary(y)\nNote: Using $ is just for lists (and tibbles, since tibbles are lists)! Not atomic vectors!\n\nz$abc\n\nError in z$abc: $ operator is invalid for atomic vectors\n\n\n\nBlank space. (Don’t subset).\n\n\nx\n\n[1] \"one\"   \"two\"   \"three\" \"four\"  \"five\" \n\nx[]\n\n[1] \"one\"   \"two\"   \"three\" \"four\"  \"five\" \n\n\nThis seems kind of silly. But blank is useful for higher-dimensional objects… like a matrix, or data frame. But our book doesn’t use matrices, so this may be the last one you see this semester:\n\nz &lt;- matrix(1:8, nrow= 2)\nz\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    3    5    7\n[2,]    2    4    6    8\n\nz[1, ]\n\n[1] 1 3 5 7\n\nz[, 1]\n\n[1] 1 2\n\nz[, -3]\n\n     [,1] [,2] [,3]\n[1,]    1    3    7\n[2,]    2    4    8\n\n\nWe could use this with tibbles too, but it is generally better to use the column names (more readable, and less likely to get the wrong columns by accident), and you should probably use select, filter, or slice:\n\nmpg[, 1:2]\n\n# A tibble: 234 × 2\n   manufacturer model     \n   &lt;chr&gt;        &lt;chr&gt;     \n 1 audi         a4        \n 2 audi         a4        \n 3 audi         a4        \n 4 audi         a4        \n 5 audi         a4        \n 6 audi         a4        \n 7 audi         a4        \n 8 audi         a4 quattro\n 9 audi         a4 quattro\n10 audi         a4 quattro\n# ℹ 224 more rows\n\nmpg[1:3, ]\n\n# A tibble: 3 × 11\n  manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class \n  &lt;chr&gt;        &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; \n1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa…\n2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa…\n3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa…",
    "crumbs": [
      "Data types"
    ]
  },
  {
    "objectID": "06_data_types.html#recycling",
    "href": "06_data_types.html#recycling",
    "title": "Data types",
    "section": "Recycling",
    "text": "Recycling\nWhat does R do with vectors:\n\n1:5 + 1:5\n\n[1]  2  4  6  8 10\n\n1:5 * 1:5\n\n[1]  1  4  9 16 25\n\n1:5 + 2\n\n[1] 3 4 5 6 7\n\n1:5 * 2\n\n[1]  2  4  6  8 10\n\n\nThis last two lines makes sense… but R is doing something important here, called recycling. In other words, it is really doing this:\n\n1:5 * c(2, 2, 2, 2, 2)\n\n[1]  2  4  6  8 10\n\n\nYou never need to do this explicit iteration! (This is different from some other more general purpose computing languages…. R was built for analyzing data, so this type of behavior is really desirable!)\nR can recycle longer vectors too, and only warns you if lengths are not multiples of each other:\n\n1:10 + 1:2\n\n [1]  2  4  4  6  6  8  8 10 10 12\n\n1:10 + 1:3\n\nWarning in 1:10 + 1:3: longer object length is not a multiple of shorter object\nlength\n\n\n [1]  2  4  6  5  7  9  8 10 12 11\n\n\nHowever, functions within the tidyverse will not allow you to recycle anything other than scalars (math word for single number… in R, a vector of length 1).\n\n#OK:\ntibble(x = 1:4, y = 1)\n\n# A tibble: 4 × 2\n      x     y\n  &lt;int&gt; &lt;dbl&gt;\n1     1     1\n2     2     1\n3     3     1\n4     4     1\n\n#not OK:\ntibble(x = 1:4, y = 1:2)\n\nError in `tibble()`:\n! Tibble columns must have compatible sizes.\n• Size 4: Existing data.\n• Size 2: Column `y`.\nℹ Only values of size one are recycled.\n\n\nTo intentionally recycle, use rep:\n\nrep(1:3, times = 2)\n\n[1] 1 2 3 1 2 3\n\nrep(1:3, each = 2)\n\n[1] 1 1 2 2 3 3",
    "crumbs": [
      "Data types"
    ]
  },
  {
    "objectID": "06_data_types.html#lists",
    "href": "06_data_types.html#lists",
    "title": "Data types",
    "section": "Lists",
    "text": "Lists\nLists can contain a mix of objects, even other lists.\nAs noted previously, tibbles are an augmented list. Augmented lists have additional attributes. For example, the names of the columns in a tibble.\nAnother list you may have encountered in a stats class is output from lm, linear regression:\n\nmpg_model &lt;- lm(hwy ~ cty, data = mpg)\n\nmpg_model\n\n\nCall:\nlm(formula = hwy ~ cty, data = mpg)\n\nCoefficients:\n(Intercept)          cty  \n      0.892        1.337  \n\ntypeof(mpg_model)\n\n[1] \"list\"\n\nstr(mpg_model)\n\nList of 12\n $ coefficients : Named num [1:2] 0.892 1.337\n  ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"cty\"\n $ residuals    : Named num [1:234] 4.0338 0.0214 3.3588 1.0214 3.7087 ...\n  ..- attr(*, \"names\")= chr [1:234] \"1\" \"2\" \"3\" \"4\" ...\n $ effects      : Named num [1:234] -358.566 -86.887 3.121 0.787 3.458 ...\n  ..- attr(*, \"names\")= chr [1:234] \"(Intercept)\" \"cty\" \"\" \"\" ...\n $ rank         : int 2\n $ fitted.values: Named num [1:234] 25 29 27.6 29 22.3 ...\n  ..- attr(*, \"names\")= chr [1:234] \"1\" \"2\" \"3\" \"4\" ...\n $ assign       : int [1:2] 0 1\n $ qr           :List of 5\n  ..$ qr   : num [1:234, 1:2] -15.2971 0.0654 0.0654 0.0654 0.0654 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:234] \"1\" \"2\" \"3\" \"4\" ...\n  .. .. ..$ : chr [1:2] \"(Intercept)\" \"cty\"\n  .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  ..$ qraux: num [1:2] 1.07 1.06\n  ..$ pivot: int [1:2] 1 2\n  ..$ tol  : num 1e-07\n  ..$ rank : int 2\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 232\n $ xlevels      : Named list()\n $ call         : language lm(formula = hwy ~ cty, data = mpg)\n $ terms        :Classes 'terms', 'formula'  language hwy ~ cty\n  .. ..- attr(*, \"variables\")= language list(hwy, cty)\n  .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:2] \"hwy\" \"cty\"\n  .. .. .. ..$ : chr \"cty\"\n  .. ..- attr(*, \"term.labels\")= chr \"cty\"\n  .. ..- attr(*, \"order\")= int 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. ..- attr(*, \"predvars\")= language list(hwy, cty)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. ..- attr(*, \"names\")= chr [1:2] \"hwy\" \"cty\"\n $ model        :'data.frame':  234 obs. of  2 variables:\n  ..$ hwy: int [1:234] 29 29 31 30 26 26 27 26 25 28 ...\n  ..$ cty: int [1:234] 18 21 20 21 16 18 18 18 16 20 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language hwy ~ cty\n  .. .. ..- attr(*, \"variables\")= language list(hwy, cty)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"hwy\" \"cty\"\n  .. .. .. .. ..$ : chr \"cty\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"cty\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(hwy, cty)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"hwy\" \"cty\"\n - attr(*, \"class\")= chr \"lm\"\n\n\nThere are three ways to extract from a list. Check out the pepper shaker analogy in Section 27.3.3 (note: shaker = list)\n\n[] returns new, smaller list (fewer pepper packs in shaker)\n[[]] drills down one level (individual pepper packs not in shaker)\n\nI. [ to extract a sub-list. The result is a list.\n\nmpg_model[1]\n\n$coefficients\n(Intercept)         cty \n  0.8920411   1.3374556 \n\ntypeof(mpg_model[1])\n\n[1] \"list\"\n\n\nyou can also do this by name, rather than number:\n\nmpg_model[\"coefficients\"]\n\n$coefficients\n(Intercept)         cty \n  0.8920411   1.3374556 \n\n\n\n[[ extracts a single component from the list… It removes a level of hierarchy\n\n\nmpg_model[[1]]\n\n(Intercept)         cty \n  0.8920411   1.3374556 \n\ntypeof(mpg_model[[1]])\n\n[1] \"double\"\n\n\nAgain, it can be done by name instead:\n\nmpg_model[[\"coefficients\"]]\n\n(Intercept)         cty \n  0.8920411   1.3374556 \n\n\n\n$ is a shorthand way of extracting elements by name… it is similar to [[ in that it removes a level of hierarchy. You don’t need quotes. (We’ve seen this with tibbles before too!)\n\n\nmpg_model$coefficients\n\n(Intercept)         cty \n  0.8920411   1.3374556",
    "crumbs": [
      "Data types"
    ]
  },
  {
    "objectID": "06_data_types.html#str",
    "href": "06_data_types.html#str",
    "title": "Data types",
    "section": "str",
    "text": "str\nThe str function allows us to see the structure of a list, as well as any attributes.\n\nmpg\n\n# A tibble: 234 × 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n 2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n 3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n 4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n 5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n 6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n 7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n 8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n 9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n# ℹ 224 more rows\n\nstr(mpg)\n\ntibble [234 × 11] (S3: tbl_df/tbl/data.frame)\n $ manufacturer: chr [1:234] \"audi\" \"audi\" \"audi\" \"audi\" ...\n $ model       : chr [1:234] \"a4\" \"a4\" \"a4\" \"a4\" ...\n $ displ       : num [1:234] 1.8 1.8 2 2 2.8 2.8 3.1 1.8 1.8 2 ...\n $ year        : int [1:234] 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 ...\n $ cyl         : int [1:234] 4 4 4 4 6 6 6 4 4 4 ...\n $ trans       : chr [1:234] \"auto(l5)\" \"manual(m5)\" \"manual(m6)\" \"auto(av)\" ...\n $ drv         : chr [1:234] \"f\" \"f\" \"f\" \"f\" ...\n $ cty         : int [1:234] 18 21 20 21 16 18 18 18 16 20 ...\n $ hwy         : int [1:234] 29 29 31 30 26 26 27 26 25 28 ...\n $ fl          : chr [1:234] \"p\" \"p\" \"p\" \"p\" ...\n $ class       : chr [1:234] \"compact\" \"compact\" \"compact\" \"compact\" ...\n\nmpg_model\n\n\nCall:\nlm(formula = hwy ~ cty, data = mpg)\n\nCoefficients:\n(Intercept)          cty  \n      0.892        1.337  \n\nstr(mpg_model)\n\nList of 12\n $ coefficients : Named num [1:2] 0.892 1.337\n  ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"cty\"\n $ residuals    : Named num [1:234] 4.0338 0.0214 3.3588 1.0214 3.7087 ...\n  ..- attr(*, \"names\")= chr [1:234] \"1\" \"2\" \"3\" \"4\" ...\n $ effects      : Named num [1:234] -358.566 -86.887 3.121 0.787 3.458 ...\n  ..- attr(*, \"names\")= chr [1:234] \"(Intercept)\" \"cty\" \"\" \"\" ...\n $ rank         : int 2\n $ fitted.values: Named num [1:234] 25 29 27.6 29 22.3 ...\n  ..- attr(*, \"names\")= chr [1:234] \"1\" \"2\" \"3\" \"4\" ...\n $ assign       : int [1:2] 0 1\n $ qr           :List of 5\n  ..$ qr   : num [1:234, 1:2] -15.2971 0.0654 0.0654 0.0654 0.0654 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:234] \"1\" \"2\" \"3\" \"4\" ...\n  .. .. ..$ : chr [1:2] \"(Intercept)\" \"cty\"\n  .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  ..$ qraux: num [1:2] 1.07 1.06\n  ..$ pivot: int [1:2] 1 2\n  ..$ tol  : num 1e-07\n  ..$ rank : int 2\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 232\n $ xlevels      : Named list()\n $ call         : language lm(formula = hwy ~ cty, data = mpg)\n $ terms        :Classes 'terms', 'formula'  language hwy ~ cty\n  .. ..- attr(*, \"variables\")= language list(hwy, cty)\n  .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:2] \"hwy\" \"cty\"\n  .. .. .. ..$ : chr \"cty\"\n  .. ..- attr(*, \"term.labels\")= chr \"cty\"\n  .. ..- attr(*, \"order\")= int 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. ..- attr(*, \"predvars\")= language list(hwy, cty)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. ..- attr(*, \"names\")= chr [1:2] \"hwy\" \"cty\"\n $ model        :'data.frame':  234 obs. of  2 variables:\n  ..$ hwy: int [1:234] 29 29 31 30 26 26 27 26 25 28 ...\n  ..$ cty: int [1:234] 18 21 20 21 16 18 18 18 16 20 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language hwy ~ cty\n  .. .. ..- attr(*, \"variables\")= language list(hwy, cty)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"hwy\" \"cty\"\n  .. .. .. .. ..$ : chr \"cty\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"cty\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(hwy, cty)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"hwy\" \"cty\"\n - attr(*, \"class\")= chr \"lm\"\n\n\nAs you can see, the mpg_model is a very complicated list with lots of attributes. The elements of the list can be all different types.\nThe last attribute is the object class, which it lists as lm.\n\nclass(mpg_model)\n\n[1] \"lm\"\n\n\nNow let’s see how extracting from a list works with a tibble (since a tibble is built on top of a list).\n\nugly_data &lt;- tibble(\n  truefalse = c(\"TRUE\", \"FALSE\", \"NA\"),\n  numbers = c(\"1\", \"2\", \"3\"),\n  dates = c(\"2010-01-01\", \"1979-10-14\", \"2013-08-17\"),\n  more_numbers = c(\"1\", \"231\", \".\")\n)\nugly_data\n\n# A tibble: 3 × 4\n  truefalse numbers dates      more_numbers\n  &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;       \n1 TRUE      1       2010-01-01 1           \n2 FALSE     2       1979-10-14 231         \n3 NA        3       2013-08-17 .           \n\nstr(ugly_data)   # we've seen str before... stands for \"structure\"\n\ntibble [3 × 4] (S3: tbl_df/tbl/data.frame)\n $ truefalse   : chr [1:3] \"TRUE\" \"FALSE\" \"NA\"\n $ numbers     : chr [1:3] \"1\" \"2\" \"3\"\n $ dates       : chr [1:3] \"2010-01-01\" \"1979-10-14\" \"2013-08-17\"\n $ more_numbers: chr [1:3] \"1\" \"231\" \".\"\n\npretty_data &lt;- ugly_data %&gt;% \n  mutate(truefalse = parse_logical(truefalse),\n         numbers = parse_number(numbers),\n         dates = parse_date(dates),\n         more_numbers = parse_number(more_numbers))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `more_numbers = parse_number(more_numbers)`.\nCaused by warning:\n! 1 parsing failure.\nrow col expected actual\n  3  -- a number      .\n\npretty_data\n\n# A tibble: 3 × 4\n  truefalse numbers dates      more_numbers\n  &lt;lgl&gt;       &lt;dbl&gt; &lt;date&gt;            &lt;dbl&gt;\n1 TRUE            1 2010-01-01            1\n2 FALSE           2 1979-10-14          231\n3 NA              3 2013-08-17           NA\n\nstr(pretty_data)\n\ntibble [3 × 4] (S3: tbl_df/tbl/data.frame)\n $ truefalse   : logi [1:3] TRUE FALSE NA\n $ numbers     : num [1:3] 1 2 3\n $ dates       : Date[1:3], format: \"2010-01-01\" \"1979-10-14\" ...\n $ more_numbers: num [1:3] 1 231 NA\n  ..- attr(*, \"problems\")= tibble [1 × 4] (S3: tbl_df/tbl/data.frame)\n  .. ..$ row     : int 3\n  .. ..$ col     : int NA\n  .. ..$ expected: chr \"a number\"\n  .. ..$ actual  : chr \".\"\n\n# Get a smaller tibble\npretty_data[1]\n\n# A tibble: 3 × 1\n  truefalse\n  &lt;lgl&gt;    \n1 TRUE     \n2 FALSE    \n3 NA       \n\nclass(pretty_data[1])\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\ntypeof(pretty_data[1])\n\n[1] \"list\"\n\npretty_data[2:3]\n\n# A tibble: 3 × 2\n  numbers dates     \n    &lt;dbl&gt; &lt;date&gt;    \n1       1 2010-01-01\n2       2 1979-10-14\n3       3 2013-08-17\n\npretty_data[1, 3:4]\n\n# A tibble: 1 × 2\n  dates      more_numbers\n  &lt;date&gt;            &lt;dbl&gt;\n1 2010-01-01            1\n\npretty_data[\"dates\"]\n\n# A tibble: 3 × 1\n  dates     \n  &lt;date&gt;    \n1 2010-01-01\n2 1979-10-14\n3 2013-08-17\n\npretty_data[c(\"dates\", \"more_numbers\")]\n\n# A tibble: 3 × 2\n  dates      more_numbers\n  &lt;date&gt;            &lt;dbl&gt;\n1 2010-01-01            1\n2 1979-10-14          231\n3 2013-08-17           NA\n\npretty_data %&gt;% select(dates, more_numbers) \n\n# A tibble: 3 × 2\n  dates      more_numbers\n  &lt;date&gt;            &lt;dbl&gt;\n1 2010-01-01            1\n2 1979-10-14          231\n3 2013-08-17           NA\n\npretty_data %&gt;% select(dates, more_numbers) %&gt;% slice(1:2) \n\n# A tibble: 2 × 2\n  dates      more_numbers\n  &lt;date&gt;            &lt;dbl&gt;\n1 2010-01-01            1\n2 1979-10-14          231\n\n# Remove a level of hierarchy - drill down one level to get a new object\npretty_data$dates\n\n[1] \"2010-01-01\" \"1979-10-14\" \"2013-08-17\"\n\nclass(pretty_data$dates)\n\n[1] \"Date\"\n\ntypeof(pretty_data$dates)\n\n[1] \"double\"\n\npretty_data[[1]]\n\n[1]  TRUE FALSE    NA\n\nclass(pretty_data[[1]])\n\n[1] \"logical\"\n\ntypeof(pretty_data[[1]])\n\n[1] \"logical\"\n\n\n[Pause to Ponder:] Predict what these lines will produce BEFORE running them:\n\npretty_data[[c(\"dates\", \"more_numbers\")]]\n\nError in `pretty_data[[c(\"dates\", \"more_numbers\")]]`:\n! Can't extract column with `c(\"dates\", \"more_numbers\")`.\n✖ Subscript `c(\"dates\", \"more_numbers\")` must be size 1, not 2.\n\npretty_data[[2]][[3]]\n\n[1] 3\n\npretty_data[[2]][3]\n\n[1] 3\n\npretty_data[[2]][c(TRUE, FALSE, TRUE)]\n\n[1] 1 3\n\npretty_data[[1]][c(1, 2, 1, 2)]\n\n[1]  TRUE FALSE  TRUE FALSE",
    "crumbs": [
      "Data types"
    ]
  },
  {
    "objectID": "06_data_types.html#generic-functions",
    "href": "06_data_types.html#generic-functions",
    "title": "Data types",
    "section": "Generic functions",
    "text": "Generic functions\nAnother important feature of R is generic functions. Some functions, like plot and summary for example, behave very differently depending on the class of their input.\n\nclass(mpg)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nsummary(mpg)\n\n manufacturer          model               displ            year     \n Length:234         Length:234         Min.   :1.600   Min.   :1999  \n Class :character   Class :character   1st Qu.:2.400   1st Qu.:1999  \n Mode  :character   Mode  :character   Median :3.300   Median :2004  \n                                       Mean   :3.472   Mean   :2004  \n                                       3rd Qu.:4.600   3rd Qu.:2008  \n                                       Max.   :7.000   Max.   :2008  \n      cyl           trans               drv                 cty       \n Min.   :4.000   Length:234         Length:234         Min.   : 9.00  \n 1st Qu.:4.000   Class :character   Class :character   1st Qu.:14.00  \n Median :6.000   Mode  :character   Mode  :character   Median :17.00  \n Mean   :5.889                                         Mean   :16.86  \n 3rd Qu.:8.000                                         3rd Qu.:19.00  \n Max.   :8.000                                         Max.   :35.00  \n      hwy             fl               class          \n Min.   :12.00   Length:234         Length:234        \n 1st Qu.:18.00   Class :character   Class :character  \n Median :24.00   Mode  :character   Mode  :character  \n Mean   :23.44                                        \n 3rd Qu.:27.00                                        \n Max.   :44.00                                        \n\nclass(mpg_model)\n\n[1] \"lm\"\n\nsummary(mpg_model)\n\n\nCall:\nlm(formula = hwy ~ cty, data = mpg)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.3408 -1.2790  0.0214  1.0338  4.0461 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.89204    0.46895   1.902   0.0584 .  \ncty          1.33746    0.02697  49.585   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.752 on 232 degrees of freedom\nMultiple R-squared:  0.9138,    Adjusted R-squared:  0.9134 \nF-statistic:  2459 on 1 and 232 DF,  p-value: &lt; 2.2e-16\n\n\nAs a simpler case, consider the mean function.\n\nmean\n\nfunction (x, ...) \nUseMethod(\"mean\")\n&lt;bytecode: 0x000002d0f2d4b880&gt;\n&lt;environment: namespace:base&gt;\n\n\nAs a generic function, we can see what methods are available:\n\nmethods(mean)\n\n[1] mean.Date        mean.default     mean.difftime    mean.POSIXct    \n[5] mean.POSIXlt     mean.quosure*    mean.vctrs_vctr*\nsee '?methods' for accessing help and source code\n\n\n\nmean(c(20, 21, 23))\n\n[1] 21.33333\n\nlibrary(lubridate)\ndate_test &lt;- ymd(c(\"2020-03-20\", \"2020-03-21\", \"2020-03-23\"))\nmean(date_test)\n\n[1] \"2020-03-21\"",
    "crumbs": [
      "Data types"
    ]
  },
  {
    "objectID": "06_data_types.html#what-makes-tibbles-special",
    "href": "06_data_types.html#what-makes-tibbles-special",
    "title": "Data types",
    "section": "What makes Tibbles special?",
    "text": "What makes Tibbles special?\nTibbles are lists that: - have names attributes (column/variable names) as well as row.names attributes. - have elements that are all vectors of the same length\n\nattributes(mpg)\n\n$class\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n$row.names\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n[109] 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n[127] 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n[145] 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n[163] 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n[181] 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n[199] 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216\n[217] 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234\n\n$names\n [1] \"manufacturer\" \"model\"        \"displ\"        \"year\"         \"cyl\"         \n [6] \"trans\"        \"drv\"          \"cty\"          \"hwy\"          \"fl\"          \n[11] \"class\"",
    "crumbs": [
      "Data types"
    ]
  },
  {
    "objectID": "06_data_types.html#on-your-own",
    "href": "06_data_types.html#on-your-own",
    "title": "Data types",
    "section": "On Your Own",
    "text": "On Your Own\n\nThe dataset roster includes 24 names (the first 24 alphabetically on this list of names). Let’s suppose this is our class, and you want to divide students into 6 groups. Modify the code below using the rep function to create groups in two different ways.\n\n\nbabynames &lt;- read_csv(\"https://proback.github.io/264_fall_2024/Data/babynames_2000.csv\")\n\nroster &lt;- babynames %&gt;%\n  sample_n(size = 24) %&gt;%\n  select(name) \n\nroster %&gt;%\n  mutate(group_method1 = , \n         group_method2 = )\n\n\nHere’s is a really crazy list that tells you some stuff about data science.\n\n\ndata_sci &lt;- list(first = c(\"first it must work\", \"then it can be\" , \"pretty\"),\n                 DRY = c(\"Do not\", \"Repeat\", \"Yourself\"),\n                 dont_forget = c(\"garbage\", \"in\", \"out\"),\n                 our_first_tibble = mpg,\n                 integers = 1:25,\n                 doubles = sqrt(1:25),\n                 tidyverse = c(pack1 = \"ggplot2\", pack2 = \"dplyr\", \n                               pack3 = \"lubridate\", etc = \"and more!\"),\n                 opinion = list(\"MSCS 264 is\",  \"awesome!\", \"amazing!\", \"rainbows!\")\n                  )\n\nUse str to learn about data_sci.\nNow, figure out how to get exactly the following outputs. Bonus points if you can do it more than one way!\n[1] “first it must work” “then it can be” “pretty”\n$DRY [1] “Do not” “Repeat” “Yourself”\n[1] 3 1 4 1 5 9 3\n  pack1         etc \n“ggplot2” “and more!”\n[1] “rainbows!”\n[1] “garbage” “in” “garbage” “out”",
    "crumbs": [
      "Data types"
    ]
  },
  {
    "objectID": "github_links.html",
    "href": "github_links.html",
    "title": "GitHub Links",
    "section": "",
    "text": "Here are a few additional GitHub links that I’ve found helpful:\n\nUsing git and GitHub with RStudio (Lisa Lendway)\nGitHub with R projects (Lisa Lendway)\nGitHub starter course – basic terminology\nHappy Git and GitHub for the useR (Jenny Bryan)",
    "crumbs": [
      "GitHub Links"
    ]
  },
  {
    "objectID": "miniproject1.html",
    "href": "miniproject1.html",
    "title": "Mini-Project 1: Maps",
    "section": "",
    "text": "Overview\nYou will produce choropleth maps illustrating two different characteristics – one numeric and one categorical – that have been measured for each US state (you can choose to exclude Alaska and Hawaii), like we did in the “Creating Informative Maps” activity. Just as we found state-level data from both a vaccine data set and the poliscidata package, you should find your own state-level data that is interesting to you.\nA few additional details for this mini-project:\n\nYou should create two versions of each plot – one that is static and one that is interactive.\nBe sure to include a note on your plots with your data source.\nYou should be able to merge your state-level data with the state mapping data sets we used in class.\nBe sure you label your plot well and provide a description of what insights can be gained from each static plot. For one of your static plots, this should be in the form of alt-text, using the “Four Ingredient Model” in the article from class.\nCheck out this rubric for Mini-Project 1.\n\n\n\nSubmission and Timeline\nMini-Project 1 must be submitted on moodle by 11:00 PM on Fri Sep 20. You should submit your two static plots, along with descriptions and alt-text, in a pdf document. For your interactive plots, just submit a GitHub link where I can see your code that would produce a nice html document. Then, in Mini-Project 2, I will ask you to build a webpage where you will link to these interactive html pages.",
    "crumbs": [
      "Mini-Project 1: Maps"
    ]
  },
  {
    "objectID": "tech_setup.html",
    "href": "tech_setup.html",
    "title": "Tech Setup",
    "section": "",
    "text": "Ideally before class on Thurs Sep 5, and definitely before class on Tues Sep 10, you should follow these instructions to set up the software that we’ll be using throughout the semester. Even if you’ve already downloaded both R and RStudio, you’ll want to re-download to make sure that you have the most current versions.\n\nRequired: Download R and RStudio\n\nFIRST: Download R here.\n\nIn the top section, you will see three links “Download R for …”\nChoose the link that corresponds to your computer.\nAs of July 24, 2024, the latest version of R is 4.4.1 (“Race for Your Life”).\n\nSECOND: Download RStudio here.\n\nClick the button under step 2 to install the version of RStudio recommended for your computer.\nAs of July 24, 2024, the latest version of RStudio is 2024.04.2 (Build 764).\n\nTHIRD: Check that when you go to File &gt; New Project &gt; New Directory, you see “Quarto Website” as an option.\n\n\nSuggested: Watch this video from Lisa Lendway at Macalester describing key configuration options for RStudio.\n\nSuggested: Change the default file download location for your internet browser.\n\nGenerally by default, internet browsers automatically save all files to the Downloads folder on your computer. In that case, you have to grab files from Downloads and move them to a more appropriate storage spot. You can change this option so that your browser asks you where to save each file before downloading it.\nThis page has information on how to do this for the most common browsers.\n\n\nRequired: Install required packages.\n\nAn R package is an extra bit of functionality that will help us in our data analysis efforts in a variety of ways. Many contributors create open source packages that can be added to base R to perform certain tasks in new and better ways.\nFor now, we’ll just make sure the tidyverse package is installed. Open RStudio and click on the Packages tab in the bottom right pane. Click the Install button and type “tidyverse” (without quotes) in the pop-up box. Click the Install button at the bottom of the pop-up box.\nYou will see a lot of text from status messages appearing in the Console as the packages are being installed. Wait until you see the &gt; again.\nEnter the command library(tidyverse) in the Console and hit Enter.\n\nQuit RStudio. You’re done setting up!\n\n\nOptional: For a refresher on RStudio features, watch this video. It also shows you how to customize the layout and color scheme of RStudio.",
    "crumbs": [
      "Tech Setup"
    ]
  }
]