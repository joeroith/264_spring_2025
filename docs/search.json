[
  {
    "objectID": "why_quarto.html",
    "href": "why_quarto.html",
    "title": "Why Quarto?",
    "section": "",
    "text": "As described in the quarto documentation: Quarto is a multi-language, next generation version of R Markdown from RStudio, with many new features and capabilities. Like R Markdown, Quarto uses Knitr to execute R code, and is therefore able to render most existing Rmd files without modification.\nData scientists are pretty excited about the introduction of Quarto, and since it represents the future of R Markdown, we will conduct MSCS 264 using Quarto. Intriguing Quarto features that have been cited include:\nHere’s a cool example from the Quarto documentation, showing features like cross-referencing of figures, chunk options using the hash-pipe format, collapsed code, and easy figure legends:"
  },
  {
    "objectID": "why_quarto.html#air-quality",
    "href": "why_quarto.html#air-quality",
    "title": "Why Quarto?",
    "section": "Air Quality",
    "text": "Air Quality\nFigure 1 further explores the impact of temperature on ozone level.\n\n\nCode\nlibrary(ggplot2)\n\nggplot(airquality, aes(Temp, Ozone)) + \n  geom_point() + \n  geom_smooth(method = \"loess\")\n\n\n\n\n\nFigure 1: Temperature and ozone level."
  },
  {
    "objectID": "rtipoftheday.html",
    "href": "rtipoftheday.html",
    "title": "R Tip of the Day",
    "section": "",
    "text": "Signup Sheet"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MSCS 264: Data Science 2 (Spring 2024)",
    "section": "",
    "text": "Qurato\n\n\n\n\n\n\n\ntidyverse\n\n\n\n\n\n\nKey links for MSCS 264\n\nCourse syllabus\nRStudio server\nmoodle\nGitHub source code for this website"
  },
  {
    "objectID": "github_links.html",
    "href": "github_links.html",
    "title": "GitHub Links",
    "section": "",
    "text": "Here are a few additional GitHub links that I’ve found helpful:\n\nUsing git and GitHub with RStudio (Lisa Lendway)\nGitHub with R projects (Lisa Lendway)\nGitHub starter course – basic terminology\nHappy Git and GitHub for the useR (Jenny Bryan)"
  },
  {
    "objectID": "03_maps_part2.html",
    "href": "03_maps_part2.html",
    "title": "Working with geospatial data",
    "section": "",
    "text": "Based on Chapter 17 from Modern Data Science with R.\nYou can download this .qmd file from here. Just hit the Download Raw File button.\n\n# Initial packages required (we'll be adding more)\nlibrary(tidyverse)\nlibrary(mdsr)      # package associated with our MDSR book\nlibrary(sf)        \n# sf = support for simple features, a standardized way to encode spatial vector data\n\nOur goal in “Maps - Part 2” is to learn about how to work with shapefiles, which are an open data structure for encoding spatial information. We will learn about projections (from three-dimensional space into two-dimensional space) and how to create informative, spatially-aware visualizations. We will just skim the surface in 264; for a much more thorough coverage, take our Spatial Statistics course!\n\nSections 17.1: Intro to spatial data - the famous John Snow case study\nThe most famous early analysis of geospatial data was done by physician John Snow in 1854. In a certain London neighborhood, an outbreak of cholera killed 127 people in three days, resulting in a mass exodus of the local residents. At the time it was thought that cholera was an airborne disease caused by breathing foul air. Snow was critical of this theory, and set about discovering the true transmission mechanism.\n\n# the mdsr package contains data from the cholera outbreak in 1854\n\n# CholeraDeaths is in the sf class - a simple feature collection\n#   with 250 features (locations where people died) and 2 fields\n#   (number who died and location geometry)\nCholeraDeaths\n\nSimple feature collection with 250 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 529160.3 ymin: 180857.9 xmax: 529655.9 ymax: 181306.2\nProjected CRS: +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs\nFirst 10 features:\n   Id Count                  geometry\n1   0     3 POINT (529308.7 181031.4)\n2   0     2 POINT (529312.2 181025.2)\n3   0     1 POINT (529314.4 181020.3)\n4   0     1 POINT (529317.4 181014.3)\n5   0     4 POINT (529320.7 181007.9)\n6   0     2   POINT (529336.7 181006)\n7   0     2 POINT (529290.1 181024.4)\n8   0     2   POINT (529301 181021.2)\n9   0     3   POINT (529285 181020.2)\n10  0     2 POINT (529288.4 181031.8)\n\n# There is no context in this original plot - we want to include the\n#   underlying London street map and the location of water pumps\nplot(CholeraDeaths[\"Count\"])\n\n\n\n\n\n\nSection 17.2: Spatial data structures\nThe most commonly used format for spatial data is called a shapefile. There are many other formats, and while we won’t master all of the details in MSCS 264, there are some important basic notions that one must have in order to work with spatial data.\nShapefiles evolved as the native file format of the ArcView program developed by the Environmental Systems Research Institute (Esri) and have since become an open specification. They can be downloaded from many different government websites and other locations that publish spatial data. Spatial data consists not of rows and columns, but of geometric objects like points, lines, and polygons. Shapefiles contain vector-based instructions for drawing the boundaries of countries, counties, and towns, etc. As such, shapefiles are richer - and more complicated - data containers than simple data frames.\nFirst, the term “shapefile” is somewhat of a misnomer, as there are several files that you must have in order to read spatial data. These files have extensions like .shp, .shx, and .dbf, and they are typically stored in a common directory.\nThere are many packages for R that specialize in working with spatial data, but we will focus on the most recent: sf. This package provides a tidyverse-friendly set of class definitions and functions for spatial objects in R. These will have the class sf (we will learn more about classes later in 264!\n\n# First, load shapefiles for London in 1854, along with information\n#   about deaths and pumps\nsnow_url &lt;- \"https://raw.githubusercontent.com/proback/264_fall_2024/main/Data/SnowGIS_SHP.zip\"\nsnow_zip &lt;- fs::path(tempdir(), \"SnowGIS_SHP.zip\")\ndownload.file(snow_url, destfile = snow_zip)\nsnow_raw &lt;- fs::path(tempdir(), \"SnowGIS_SHP\")\nunzip(snow_zip, exdir = snow_raw)\ndsn &lt;- fs::path(snow_raw, \"SnowGIS_SHP\")\nlist.files(dsn)  # note 22 files\n\n [1] \"Cholera_Deaths.dbf\"          \"Cholera_Deaths.prj\"         \n [3] \"Cholera_Deaths.sbn\"          \"Cholera_Deaths.sbx\"         \n [5] \"Cholera_Deaths.shp\"          \"Cholera_Deaths.shx\"         \n [7] \"OSMap.tfw\"                   \"OSMap.tif\"                  \n [9] \"OSMap_Grayscale.tfw\"         \"OSMap_Grayscale.tif\"        \n[11] \"OSMap_Grayscale.tif.aux.xml\" \"OSMap_Grayscale.tif.ovr\"    \n[13] \"Pumps.dbf\"                   \"Pumps.prj\"                  \n[15] \"Pumps.sbx\"                   \"Pumps.shp\"                  \n[17] \"Pumps.shx\"                   \"README.txt\"                 \n[19] \"SnowMap.tfw\"                 \"SnowMap.tif\"                \n[21] \"SnowMap.tif.aux.xml\"         \"SnowMap.tif.ovr\"            \n\nst_layers(dsn)   # 1 layer for 8 pumps and 1 for 250 death locations\n\nDriver: ESRI Shapefile \nAvailable layers:\n      layer_name geometry_type features fields                       crs_name\n1 Cholera_Deaths         Point      250      2 OSGB36 / British National Grid\n2          Pumps         Point        8      1 OSGB36 / British National Grid\n\n# You can also downloaded zip file and uploaded it into R, but this uses a ton of space!\n# dsn &lt;- fs::path(\"Data/SnowGIS_SHP\")\n\n# How to obtain the CholeraDeaths data we examined earlier\nCholeraDeaths &lt;- st_read(dsn, layer = \"Cholera_Deaths\")\n\nReading layer `Cholera_Deaths' from data source \n  `C:\\Users\\roback\\AppData\\Local\\Temp\\Rtmp8SgLnL\\SnowGIS_SHP\\SnowGIS_SHP' \n  using driver `ESRI Shapefile'\nSimple feature collection with 250 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 529160.3 ymin: 180857.9 xmax: 529655.9 ymax: 181306.2\nProjected CRS: OSGB36 / British National Grid\n\nclass(CholeraDeaths)\n\n[1] \"sf\"         \"data.frame\"\n\nCholeraDeaths\n\nSimple feature collection with 250 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 529160.3 ymin: 180857.9 xmax: 529655.9 ymax: 181306.2\nProjected CRS: OSGB36 / British National Grid\nFirst 10 features:\n   Id Count                  geometry\n1   0     3 POINT (529308.7 181031.4)\n2   0     2 POINT (529312.2 181025.2)\n3   0     1 POINT (529314.4 181020.3)\n4   0     1 POINT (529317.4 181014.3)\n5   0     4 POINT (529320.7 181007.9)\n6   0     2   POINT (529336.7 181006)\n7   0     2 POINT (529290.1 181024.4)\n8   0     2   POINT (529301 181021.2)\n9   0     3   POINT (529285 181020.2)\n10  0     2 POINT (529288.4 181031.8)\n\n\n\n\nSection 17.3: Making maps\n\n# make basic map of deaths with correct lat/long information\nggplot(CholeraDeaths) +\n  geom_sf()    \n\n\n\n# assumes (x,y) info stored in a column called \"geometry\", so we don't\n#   explicitly have to specify the x and y aesthetics\n\n# place deaths on layout of London streets using ggspatial\n#   Note that aesthetics work like other geoms\nlibrary(ggspatial)\nlibrary(prettymapr)\nggplot(CholeraDeaths) + \n  annotation_map_tile(type = \"osm\", zoomin = 0, progress = \"none\") + \n  geom_sf(aes(size = Count), alpha = 0.7)   \n\n\n\n# Notice that points are off.  For example, there should be a cluster\n#   on Broadwick St, and deaths should be in homes and not streets\n\nst_bbox(CholeraDeaths)   # bounding box\n\n    xmin     ymin     xmax     ymax \n529160.3 180857.9 529655.9 181306.2 \n\n# Turns out the geospatial coordinates of CholeraDeaths and ggspatial\n#   are not the same - it comes down to projections\n\n[Pause to ponder:] What do the different options in annotation_map_tile() do? You might check out the help screen…\n\n\nSection 17.3.2: Projections\nThe process of converting locations in a three-dimensional geographic coordinate system to a two-dimensional representation is called projection. It is simply not possible to faithfully preserve all properties present in a three-dimensional space in a two-dimensional space. Thus there is no one best projection system - each has its own advantages and disadvantages.\n\nlibrary(mapproj)\nlibrary(maps)\n\nmap(\"world\", projection = \"mercator\", wrap = TRUE)\n\n\n\nmap(\"world\", projection = \"cylequalarea\", param = 45, wrap = TRUE)\n\n\n\n\n[Pause to ponder:] Describe differences between the first world map (Mercator projection) and the second (Gall-Peters projection).\nHere’s a clever map showing the Mercator projection with the true size and shape of each country overlaid.\nTwo common general-purpose map projections are the Lambert conformal conic projection and the Albers equal-area conic projection. In the former, angles are preserved, while in the latter neither scale nor shape are preserved, but gross distortions of both are minimized.\n\n# Scales specified to be true on the 20th and 50th parallels\n# Note that default resolution of 0 doesn't provide enough detail\nmap(\n  \"state\", projection = \"lambert\", \n  parameters = c(lat0 = 20, lat1 = 50), wrap = TRUE, resolution = -5,\n)\n\n\n\nmap(\n  \"state\", projection = \"albers\", \n  parameters = c(lat0 = 20, lat1 = 50), wrap = TRUE, resolution = -5,\n)\n\n\n\n\nA coordinate reference system (CRS) is needed to keep track of geographic locations. There are three main components to a CRS: ellipsoid, datum, and a projection. Every spatially-aware object in R can have a projection. Three formats that are common for storing information about the projection of a geospatial object are EPSG (an integer from the European Petroleum Survey Group), PROJ.4 (a cryptic string of text), and WKT (Well-Known Text, which can be retrieved or set using the st_crs() command).\nA few common CRSs are:\n\nEPSG:4326 - Also known as WGS84, this is the standard for GPS systems and Google Earth.\nEPSG:3857 - A Mercator projection used in maps tiles3 by Google Maps, Open Street Maps, etc.\nEPSG:27700 - Also known as OSGB 1936, or the British National Grid: United Kingdom Ordnance Survey. It is commonly used in Britain.\n\n\nst_crs(CholeraDeaths)\n\nCoordinate Reference System:\n  User input: OSGB36 / British National Grid \n  wkt:\nPROJCRS[\"OSGB36 / British National Grid\",\n    BASEGEOGCRS[\"OSGB36\",\n        DATUM[\"Ordnance Survey of Great Britain 1936\",\n            ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4277]],\n    CONVERSION[\"British National Grid\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",49,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-2,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996012717,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",400000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",-100000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"United Kingdom (UK) - offshore to boundary of UKCS within 49°45'N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\"],\n        BBOX[49.75,-9.01,61.01,2.01]],\n    ID[\"EPSG\",27700]]\n\n# Uses a transverse Mercator method and the datum (model of the Earth)\n#   is OSGB 1936 = British National Grid\n\n# The st_crs() function will translate from the shorthand EPSG code \n#   to the full-text PROJ.4 strings and WKT.\nst_crs(4326)$epsg\n\n[1] 4326\n\nst_crs(3857)$Wkt\n\n[1] \"PROJCS[\\\"WGS 84 / Pseudo-Mercator\\\",GEOGCS[\\\"WGS 84\\\",DATUM[\\\"WGS_1984\\\",SPHEROID[\\\"WGS 84\\\",6378137,298.257223563,AUTHORITY[\\\"EPSG\\\",\\\"7030\\\"]],AUTHORITY[\\\"EPSG\\\",\\\"6326\\\"]],PRIMEM[\\\"Greenwich\\\",0,AUTHORITY[\\\"EPSG\\\",\\\"8901\\\"]],UNIT[\\\"degree\\\",0.0174532925199433,AUTHORITY[\\\"EPSG\\\",\\\"9122\\\"]],AUTHORITY[\\\"EPSG\\\",\\\"4326\\\"]],PROJECTION[\\\"Mercator_1SP\\\"],PARAMETER[\\\"central_meridian\\\",0],PARAMETER[\\\"scale_factor\\\",1],PARAMETER[\\\"false_easting\\\",0],PARAMETER[\\\"false_northing\\\",0],UNIT[\\\"metre\\\",1],AXIS[\\\"Easting\\\",EAST],AXIS[\\\"Northing\\\",NORTH],EXTENSION[\\\"PROJ4\\\",\\\"+proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0 +x_0=0 +y_0=0 +k=1 +units=m +nadgrids=@null +wktext +no_defs\\\"],AUTHORITY[\\\"EPSG\\\",\\\"3857\\\"]]\"\n\nst_crs(27700)$proj4string\n\n[1] \"+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs\"\n\n# To get Cholera Deaths to line up with Google Maps (Open Street Map tiles) we need to convert to the EPSG 4326 system, since even though Google Maps tiles (and Open Street Map tiles) are projected in the espg:3857 system, they are confusingly returned with coordinates in the epsg:4326 system.  Iyiyi!\ncholera_4326 &lt;- CholeraDeaths |&gt;\n  st_transform(4326)\nst_bbox(cholera_4326)\n\n      xmin       ymin       xmax       ymax \n-0.1400738 51.5118557 -0.1329335 51.5158345 \n\n# Better but not perfect\nggplot(cholera_4326) + \n  annotation_map_tile(type = \"osm\", zoomin = 0) + \n  geom_sf(aes(size = Count), alpha = 0.7)\n\nZoom: 17\n\n\n\n\n# The +datum and +towgs84 arguments were missing from our PROJ.4 string.\nst_crs(CholeraDeaths)$proj4string\n\n[1] \"+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs\"\n\n# If we first assert that the CholeraDeaths data is in epsg:27700. \n#   Then, projecting to epsg:4326 works as intended.\ncholera_latlong &lt;- CholeraDeaths |&gt;\n  st_set_crs(27700) |&gt;\n  st_transform(4326)\nsnow &lt;- ggplot(cholera_latlong) + \n  annotation_map_tile(type = \"osm\", zoomin = 0) + \n  geom_sf(aes(size = Count))\n\n# Add pumps in the same way, and we're done!\npumps &lt;- st_read(dsn, layer = \"Pumps\")\n\nReading layer `Pumps' from data source \n  `C:\\Users\\roback\\AppData\\Local\\Temp\\Rtmp8SgLnL\\SnowGIS_SHP\\SnowGIS_SHP' \n  using driver `ESRI Shapefile'\nSimple feature collection with 8 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 529183.7 ymin: 180660.5 xmax: 529748.9 ymax: 181193.7\nProjected CRS: OSGB36 / British National Grid\n\npumps_latlong &lt;- pumps |&gt;\n  st_set_crs(27700) |&gt;\n  st_transform(4326)\n\n# The final plot is really only 3 layers - background tiles, points representing deaths, and points representing pumps - but they must be very carefully lined up!\nsnow +\n  geom_sf(data = pumps_latlong, size = 3, color = \"red\")\n\nZoom: 17\n\n\n\n\n\n\n\nSection 17.4: Extended example: NC Congressional Districts\nIn North Carolina, there are about the same number of Democratic and Republican voters in the state. In the fall of 2020, 10 of North Carolina’s 13 congressional representatives were Republican (with one seat currently vacant). How can this be? In this case, geospatial data can help us understand.\nNote: the seats are currently 7 and 7 (NC earned an additional seat for 2022 after the 2020 Census), but 3 are expected to flip back to Republicans again after yet another round of questionable redistricting\n\n# To install fec 12 the first time, uncomment the code below (you might have to install devtools as well):\n# devtools::install_github(\"baumer-lab/fec12\")\nlibrary(fec12)\nprint(results_house, width = Inf)\n\n# A tibble: 2,343 × 13\n   state district_id cand_id   incumbent party primary_votes primary_percent\n   &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;     &lt;lgl&gt;     &lt;chr&gt;         &lt;dbl&gt;           &lt;dbl&gt;\n 1 AL    01          H2AL01077 TRUE      R             48702          0.555 \n 2 AL    01          H2AL01176 FALSE     R             21308          0.243 \n 3 AL    01          H2AL01184 FALSE     R             13809          0.158 \n 4 AL    01          H0AL01030 FALSE     R              3854          0.0440\n 5 AL    02          H0AL02087 TRUE      R                NA         NA     \n 6 AL    02          H2AL02141 FALSE     D                NA         NA     \n 7 AL    03          H2AL03032 TRUE      R                NA         NA     \n 8 AL    03          H2AL03099 FALSE     D                NA         NA     \n 9 AL    04          H6AL04098 TRUE      R                NA         NA     \n10 AL    04          H2AL04055 FALSE     D             10971          0.514 \n   runoff_votes runoff_percent general_votes general_percent won   footnotes\n          &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;           &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt;    \n 1           NA             NA        196374           0.979 TRUE  &lt;NA&gt;     \n 2           NA             NA            NA          NA     FALSE &lt;NA&gt;     \n 3           NA             NA            NA          NA     FALSE &lt;NA&gt;     \n 4           NA             NA            NA          NA     FALSE &lt;NA&gt;     \n 5           NA             NA        180591           0.636 TRUE  &lt;NA&gt;     \n 6           NA             NA        103092           0.363 FALSE &lt;NA&gt;     \n 7           NA             NA        175306           0.640 TRUE  &lt;NA&gt;     \n 8           NA             NA         98141           0.358 FALSE &lt;NA&gt;     \n 9           NA             NA        199071           0.740 TRUE  &lt;NA&gt;     \n10           NA             NA         69706           0.259 FALSE &lt;NA&gt;     \n# ℹ 2,333 more rows\n\nresults_house |&gt;\n  group_by(state, district_id) |&gt;\n  summarize(N = n())\n\n# A tibble: 445 × 3\n# Groups:   state [56]\n   state district_id     N\n   &lt;chr&gt; &lt;chr&gt;       &lt;int&gt;\n 1 AK    00             10\n 2 AL    01              4\n 3 AL    02              2\n 4 AL    03              2\n 5 AL    04              3\n 6 AL    05              3\n 7 AL    06              6\n 8 AL    07              3\n 9 AR    01              6\n10 AR    02              4\n# ℹ 435 more rows\n\n\n[Pause to ponder:] Why are there 435 Representatives in the US House but 445 state/district combinations in our data? And how should we handle cases in which there’s just not 1 Democrat vs 1 Republican?\n\n# summary of the 13 congressional NC districts and the 2012 voting\ndistrict_elections &lt;- results_house |&gt;\n  mutate(district = parse_number(district_id)) |&gt;\n  group_by(state, district) |&gt;\n  summarize(\n    N = n(), \n    total_votes = sum(general_votes, na.rm = TRUE),\n    d_votes = sum(ifelse(party == \"D\", general_votes, 0), na.rm = TRUE),\n    r_votes = sum(ifelse(party == \"R\", general_votes, 0), na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  mutate(\n    other_votes = total_votes - d_votes - r_votes,\n    r_prop = r_votes / total_votes,  \n    winner = ifelse(r_votes &gt; d_votes, \"Republican\", \"Democrat\")\n  )\nnc_results &lt;- district_elections |&gt;\n  filter(state == \"NC\")\nnc_results |&gt;                  \n  select(-state)\n\n# A tibble: 13 × 8\n   district     N total_votes d_votes r_votes other_votes r_prop winner    \n      &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n 1        1     4      338066  254644   77288        6134  0.229 Democrat  \n 2        2     8      311397  128973  174066        8358  0.559 Republican\n 3        3     3      309885  114314  195571           0  0.631 Republican\n 4        4     4      348485  259534   88951           0  0.255 Democrat  \n 5        5     3      349197  148252  200945           0  0.575 Republican\n 6        6     4      364583  142467  222116           0  0.609 Republican\n 7        7     4      336736  168695  168041           0  0.499 Democrat  \n 8        8     8      301824  137139  160695        3990  0.532 Republican\n 9        9    13      375690  171503  194537        9650  0.518 Republican\n10       10     6      334849  144023  190826           0  0.570 Republican\n11       11    11      331426  141107  190319           0  0.574 Republican\n12       12     3      310908  247591   63317           0  0.204 Democrat  \n13       13     5      370610  160115  210495           0  0.568 Republican\n\n\n[Pause to ponder:]\n\nExplain how sum(ifelse(party == \"D\", general_votes, 0), na.rm = TRUE) works\nExplain why we use .groups = \"drop\". Hint: try excluding that line and running again.\nDo you see any potential problems with ifelse(r_votes &gt; d_votes, \"Republican\", \"Democrat\")?\nWhat observations can you make about the final nc_results table?\n\n\n# distribution of total number of votes is narrow by design\nnc_results |&gt;\n  skim(total_votes) |&gt;\n  select(-na)\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvar\nn\nmean\nsd\np0\np25\np50\np75\np100\n\n\n\n\ntotal_votes\n13\n337204.3\n24175.2\n301824\n311397\n336736\n349197\n375690\n\n\n\n\n# compare total Dem and Rep votes across NC in 2012\nnc_results |&gt;\n  summarize(\n    N = n(), \n    state_votes = sum(total_votes), \n    state_d = sum(d_votes), \n    state_r = sum(r_votes)\n  ) |&gt;\n  mutate(\n    d_prop = state_d / state_votes, \n    r_prop = state_r / state_votes\n  )\n\n# A tibble: 1 × 6\n      N state_votes state_d state_r d_prop r_prop\n  &lt;int&gt;       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1    13     4383656 2218357 2137167  0.506  0.488\n\n# Proportion of Rep votes by district\nnc_results |&gt;\n  select(district, r_prop, winner) |&gt;\n  arrange(desc(r_prop))\n\n# A tibble: 13 × 3\n   district r_prop winner    \n      &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n 1        3  0.631 Republican\n 2        6  0.609 Republican\n 3        5  0.575 Republican\n 4       11  0.574 Republican\n 5       10  0.570 Republican\n 6       13  0.568 Republican\n 7        2  0.559 Republican\n 8        8  0.532 Republican\n 9        9  0.518 Republican\n10        7  0.499 Democrat  \n11        4  0.255 Democrat  \n12        1  0.229 Democrat  \n13       12  0.204 Democrat  \n\n\nNow let’s layer the results above on a map of North Carolina to create an effective visualization of the situation. How does the shape of districts where Republicans won compare with the shape where Democrats won?\n\n# Download congressional district shapefiles for the 113th Congress from a UCLA website (don't sweat the details too much)\nsrc &lt;- \"http://cdmaps.polisci.ucla.edu/shp/districts113.zip\"\nlcl_zip &lt;- fs::path(tempdir(), \"districts113.zip\")\ndownload.file(src, destfile = lcl_zip)\nlcl_districts &lt;- fs::path(tempdir(), \"districts113\")\nunzip(lcl_zip, exdir = lcl_districts)\ndsn_districts &lt;- fs::path(lcl_districts, \"districtShapes\")\n\n# You can also downloaded zip file and uploaded it into R, but this uses a ton of space!\n# dsn_districts &lt;- fs::path(\"Data/districtShapes\")\n\n# read shapefiles into R as an sf object\nst_layers(dsn_districts)\n\nDriver: ESRI Shapefile \nAvailable layers:\n    layer_name geometry_type features fields crs_name\n1 districts113       Polygon      436     15    NAD83\n\n# be able to read as a data frame as well\ndistricts &lt;- st_read(dsn_districts, layer = \"districts113\") |&gt;\n  mutate(DISTRICT = parse_number(as.character(DISTRICT)))\n\nReading layer `districts113' from data source \n  `C:\\Users\\roback\\AppData\\Local\\Temp\\Rtmp8SgLnL\\districts113\\districtShapes' \n  using driver `ESRI Shapefile'\nSimple feature collection with 436 features and 15 fields (with 1 geometry empty)\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.1473 ymin: 18.91383 xmax: 179.7785 ymax: 71.35256\nGeodetic CRS:  NAD83\n\nhead(districts, width = Inf)\n\nSimple feature collection with 6 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -91.82307 ymin: 29.41135 xmax: -66.94983 ymax: 47.45969\nGeodetic CRS:  NAD83\n  STATENAME           ID DISTRICT STARTCONG ENDCONG DISTRICTSI COUNTY PAGE  LAW\n1 Louisiana 022113114006        6       113     114       &lt;NA&gt;   &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;\n2     Maine 023113114001        1       113     114       &lt;NA&gt;   &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;\n3     Maine 023113114002        2       113     114       &lt;NA&gt;   &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;\n4  Maryland 024113114001        1       113     114       &lt;NA&gt;   &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;\n5  Maryland 024113114002        2       113     114       &lt;NA&gt;   &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;\n6  Maryland 024113114003        3       113     114       &lt;NA&gt;   &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;\n  NOTE BESTDEC                  FINALNOTE RNOTE                 LASTCHANGE\n1 &lt;NA&gt;    &lt;NA&gt; {\"From US Census website\"}  &lt;NA&gt; 2016-05-29 16:44:10.857626\n2 &lt;NA&gt;    &lt;NA&gt; {\"From US Census website\"}  &lt;NA&gt; 2016-05-29 16:44:10.857626\n3 &lt;NA&gt;    &lt;NA&gt; {\"From US Census website\"}  &lt;NA&gt; 2016-05-29 16:44:10.857626\n4 &lt;NA&gt;    &lt;NA&gt; {\"From US Census website\"}  &lt;NA&gt; 2016-05-29 16:44:10.857626\n5 &lt;NA&gt;    &lt;NA&gt; {\"From US Census website\"}  &lt;NA&gt; 2016-05-29 16:44:10.857626\n6 &lt;NA&gt;    &lt;NA&gt; {\"From US Census website\"}  &lt;NA&gt; 2016-05-29 16:44:10.857626\n  FROMCOUNTY                       geometry\n1          F MULTIPOLYGON (((-91.82288 3...\n2          F MULTIPOLYGON (((-70.98905 4...\n3          F MULTIPOLYGON (((-71.08216 4...\n4          F MULTIPOLYGON (((-77.31156 3...\n5          F MULTIPOLYGON (((-76.8763 39...\n6          F MULTIPOLYGON (((-77.15622 3...\n\nclass(districts)\n\n[1] \"sf\"         \"data.frame\"\n\n# create basic plot with NC congressional districts\nnc_shp &lt;- districts |&gt;\n  filter(STATENAME == \"North Carolina\")\nnc_shp |&gt;\n  st_geometry() |&gt;\n  plot(col = gray.colors(nrow(nc_shp)))\n\n\n\n# Append election results to geospatial data\nnc_merged &lt;- nc_shp |&gt;\n  st_transform(4326) |&gt;\n  inner_join(nc_results, by = c(\"DISTRICT\" = \"district\"))\nhead(nc_merged, width = Inf)\n\nSimple feature collection with 6 features and 23 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -81.91811 ymin: 34.19118 xmax: -75.45998 ymax: 36.58812\nGeodetic CRS:  WGS 84\n       STATENAME           ID DISTRICT STARTCONG ENDCONG DISTRICTSI COUNTY PAGE\n1 North Carolina 037113114002        2       113     114       &lt;NA&gt;   &lt;NA&gt; &lt;NA&gt;\n2 North Carolina 037113114003        3       113     114       &lt;NA&gt;   &lt;NA&gt; &lt;NA&gt;\n3 North Carolina 037113114004        4       113     114       &lt;NA&gt;   &lt;NA&gt; &lt;NA&gt;\n4 North Carolina 037113114001        1       113     114       &lt;NA&gt;   &lt;NA&gt; &lt;NA&gt;\n5 North Carolina 037113114005        5       113     114       &lt;NA&gt;   &lt;NA&gt; &lt;NA&gt;\n6 North Carolina 037113114006        6       113     114       &lt;NA&gt;   &lt;NA&gt; &lt;NA&gt;\n   LAW NOTE BESTDEC                  FINALNOTE RNOTE                 LASTCHANGE\n1 &lt;NA&gt; &lt;NA&gt;    &lt;NA&gt; {\"From US Census website\"}  &lt;NA&gt; 2016-05-29 16:44:10.857626\n2 &lt;NA&gt; &lt;NA&gt;    &lt;NA&gt; {\"From US Census website\"}  &lt;NA&gt; 2016-05-29 16:44:10.857626\n3 &lt;NA&gt; &lt;NA&gt;    &lt;NA&gt; {\"From US Census website\"}  &lt;NA&gt; 2016-05-29 16:44:10.857626\n4 &lt;NA&gt; &lt;NA&gt;    &lt;NA&gt; {\"From US Census website\"}  &lt;NA&gt; 2016-05-29 16:44:10.857626\n5 &lt;NA&gt; &lt;NA&gt;    &lt;NA&gt; {\"From US Census website\"}  &lt;NA&gt; 2016-05-29 16:44:10.857626\n6 &lt;NA&gt; &lt;NA&gt;    &lt;NA&gt; {\"From US Census website\"}  &lt;NA&gt; 2016-05-29 16:44:10.857626\n  FROMCOUNTY state N total_votes d_votes r_votes other_votes    r_prop\n1          F    NC 8      311397  128973  174066        8358 0.5589842\n2          F    NC 3      309885  114314  195571           0 0.6311083\n3          F    NC 4      348485  259534   88951           0 0.2552506\n4          F    NC 4      338066  254644   77288        6134 0.2286181\n5          F    NC 3      349197  148252  200945           0 0.5754488\n6          F    NC 4      364583  142467  222116           0 0.6092330\n      winner                       geometry\n1 Republican MULTIPOLYGON (((-80.05325 3...\n2 Republican MULTIPOLYGON (((-78.27217 3...\n3   Democrat MULTIPOLYGON (((-79.47249 3...\n4   Democrat MULTIPOLYGON (((-78.95837 3...\n5 Republican MULTIPOLYGON (((-81.91805 3...\n6 Republican MULTIPOLYGON (((-80.97462 3...\n\n# Color based on winning party\n#   Note that geom_sf is part of ggplot2 package, while st_geometry is\n#   part of sf package\nnc &lt;- ggplot(data = nc_merged, aes(fill = winner)) +\n  annotation_map_tile(zoom = 6, type = \"osm\", progress = \"none\") + \n  geom_sf(alpha = 0.5) +\n  scale_fill_manual(\"Winner\", values = c(\"blue\", \"red\")) + \n  geom_sf_label(aes(label = DISTRICT), fill = \"white\") + \n  theme_void()\nnc\n\n\n\n# Color based on proportion Rep.  Be sure to let limits so centered at 0.5.\n# This is a choropleth map, where meaningful shading relates to some attribute\nnc +\n  aes(fill = r_prop) + \n  scale_fill_distiller(\n    \"Proportion\\nRepublican\", \n    palette = \"RdBu\", \n    limits = c(0.2, 0.8)\n  )\n\n\n\n# A leaflet map can allow us to zoom in and see where major cities fit, etc.\nlibrary(leaflet)\npal &lt;- colorNumeric(palette = \"RdBu\", domain = c(0, 1))\n\nleaflet_nc &lt;- leaflet(nc_merged) |&gt;\n  addTiles() |&gt;\n  addPolygons(\n    weight = 1, fillOpacity = 0.7, \n    color = ~pal(1 - r_prop),   # so red association with Reps\n    popup = ~paste(\"District\", DISTRICT, \"&lt;/br&gt;\", round(r_prop, 4))\n  ) |&gt;                          # popups show prop Republican\n  setView(lng = -80, lat = 35, zoom = 7)\nleaflet_nc\n\n\n\n\n\n[Pause to ponder:] What have you learned by layering the voting data on the voting districts of North Carolina?"
  },
  {
    "objectID": "01_review164.html",
    "href": "01_review164.html",
    "title": "Review of Data Science 1",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\n\nDeterminants of COVID vaccination rates\nFirst, a little detour to describe several alternatives for reading in data:\nIf you navigate to my Github account, and find the 264_fall_2024 repo, there is a Data folder inside. You can then click on vacc_Mar21.csv to see the data we want to download. This link should also get you there, but it’s good to be able to navigate there yourself.\n\n# Approach 1: create a Data folder in the same location where this .qmd file resides, and then store vaccinations_2021.csv in that Data folder\nvaccine_data &lt;- read_csv(\"Data/vaccinations_2021.csv\")\n\n# Approach 2: give R the complete path to the location of vaccinations_2021.csv, starting with Home (~)\nvaccine_data &lt;- read_csv(\"~/264_fall_2024/Data/vaccinations_2021.csv\")\n\n# Approach 3: link to our course webpage, and then know we have a Data folder containing all our csvs\nvaccine_data &lt;- read_csv(\"https://proback.github.io/264_fall_2024/Data/vaccinations_2021.csv\")\n\n# Approach 4: navigate to the data in GitHub, hit the Raw button, and copy that link\nvaccine_data &lt;- read_csv(\"https://raw.githubusercontent.com/proback/264_fall_2024/main/Data/vaccinations_2021.csv\")\n\nA recent Stat 272 project examined determinants of covid vaccination rates at the county level. Our data set contains 3053 rows (1 for each county in the US) and 14 columns; here is a quick description of the variables we’ll be using:\n\nstate = state the county is located in\ncounty = name of the county\nregion = region the state is located in\nmetro_status = Is the county considered “Metro” or “Non-metro”?\nrural_urban_code = from 1 (most urban) to 9 (most rural)\nperc_complete_vac = percent of county completely vaccinated as of 11/9/21\ntot_pop = total population in the county\nvotes_Trump = number of votes for Trump in the county in 2020\nvotes_Biden = number of votes for Biden in the county in 2020\nperc_Biden = percent of votes for Biden in the county in 2020\ned_somecol_perc = percent with some education beyond high school (but not a Bachelor’s degree)\ned_bachormore_perc = percent with a Bachelor’s degree or more\nunemployment_rate_2020 = county unemployment rate in 2020\nmedian_HHincome_2019 = county’s median household income in 2019\n\n\nConsider only Minnesota and its surrounding states (Iowa, Wisconsin, North Dakota, and South Dakota). We want to examine the relationship between the percentage who voted for Biden and the percentage of complete vaccinations by state. Generate two plots to examine this relationship:\n\n\nA scatterplot with points and smoothers colored by state. Make sure the legend is ordered in a meaningful way, and include good labels on your axes and your legend. Also leave off the error bars from your smoothers.\nOne plot per state containing a scatterplot and a smoother.\n\nDescribe which plot you prefer and why. What can you learn from your preferred plot?\n\nWe wish to compare the proportions of counties in each region with median household income above the national median ($69,560).\n\n\nFill in the blanks below to produce a segmented bar plot with regions ordered from highest proportion above the median to lowest.\nCreate a table of proportions by region to illustrate that your bar plot in (a) is in the correct order (you should find two regions that are really close when you just try to eyeball differences).\nExplain why we can replace fct_relevel(region, FILL IN CODE) with\n\nmutate(region_sort = fct_reorder(region, median_HHincome_2019 &lt; 69560, .fun = mean))\nbut not\nmutate(region_sort = fct_reorder(region, median_HHincome_2019 &lt; 69560))\n\nvaccine_data |&gt;\n  mutate(HHincome_vs_national = ifelse(median_HHincome_2019 &lt; 69560, FILL IN CODE)) |&gt;\n  mutate(region_sort = fct_relevel(region, FILL IN CODE)) |&gt;\n  ggplot(mapping = aes(x = region_sort, fill = HHincome_vs_national)) +\n    geom_bar(position = \"fill\")\n\n\nWe want to examine the distribution of total county populations and then see how it’s related to vaccination rates.\n\n\nCarefully and thoroughly explain why the two histograms below provide different plots.\n\n\nvaccine_data |&gt;\n  mutate(tot_pop_millions = tot_pop / 1000000) |&gt;\n  ggplot(mapping = aes(x = tot_pop_millions)) +\n    geom_histogram(bins = 40) +\n    labs(x = \"Total population in millions\")\n\n\n\nvaccine_data |&gt;\n  mutate(tot_pop_millions = tot_pop %/% 1000000) |&gt;\n  ggplot(mapping = aes(x = tot_pop_millions)) +\n    geom_histogram(bins = 40) +\n    labs(x = \"Total population in millions\")\n\n\n\n\n\nFind the top 5 counties in terms of total population.\nPlot a histogram of logged population and describe this distribution.\nPlot the relationship between log population and percent vaccinated using separate colors for Metro and Non-metro counties (be sure there’s no 3rd color used for NAs). Reduce the size and transparency of each point to make the plot more readable. Describe what you can learn from this plot.\n\n\nProduce 3 different plots for illustrating the relationship between the rural_urban_code and percent vaccinated. Hint: you can sometimes turn numeric variables into categorical variables for plotting purposes (e.g. as.factor(), ifelse()).\n\nState your favorite plot, why you like it better than the other two, and what you can learn from your favorite plot.\n\nBEFORE running the code below, sketch the plot that will be produced by R. AFTER running the code, describe what conclusion(s) can we draw from this plot?\n\n\nvaccine_data |&gt;\n  filter(!is.na(perc_Biden)) |&gt;\n  mutate(big_states = fct_lump(state, n = 10)) |&gt;\n  group_by(big_states) |&gt;\n  summarize(IQR_Biden = IQR(perc_Biden)) |&gt;\n  mutate(big_states = fct_reorder(big_states, IQR_Biden)) |&gt;\n  ggplot() + \n    geom_point(aes(x = IQR_Biden, y = big_states))\n\n\n\n\n\nIn this question we will focus only on the 12 states in the Midwest (i.e. where region == “Midwest”).\n\n\nCreate a tibble with the following information for each state. Order states from least to greatest state population.\n\n\nnumber of different rural_urban_codes represented among the state’s counties (there are 9 possible)\ntotal state population\nproportion of Metro counties\nmedian unemployment rate\n\n\nUse your tibble in (a) to produce a plot of the relationship between proportion of Metro counties and median unemployment rate. Points should be colored by the number of different rural_urban_codes in a state, but a single linear trend should be fit to all points. What can you conclude from the plot?\n\n\nGenerate an appropriate plot to compare vaccination rates between two subregions of the US: New England (which contains the states Maine, Vermont, New Hampshire, Massachusetts, Connecticut, Rhode Island) and the Upper Midwest (which, according to the USGS, contains the states Minnesota, Wisconsin, Michigan, Illinois, Indiana, and Iowa). What can you conclude from your plot?\n\nIn this next section, we consider a few variables that could have been included in our data set, but were NOT. Thus, you won’t be able to write and test code, but you nevertheless should be able to use your knowledge of the tidyverse to answer these questions.\nHere are the hypothetical variables:\n\nHR_party = party of that county’s US Representative (Republican, Democrat, Independent, Green, or Libertarian)\npeople_per_MD = number of residents per doctor (higher values = fewer doctors)\nperc_over_65 = percent of residents over 65 years old\nperc_white = percent of residents who identify as white\n\n\nHypothetical R chunk #1:\n\n\n# Hypothetical R chunk 1\ntemp &lt;- vaccine_data |&gt;\n  mutate(new_perc_vac = ifelse(perc_complete_vac &gt; 95, NA, perc_complete_vac),\n         MD_group = cut_number(people_per_MD, 3)) |&gt;\n  group_by(MD_group) |&gt;\n  summarise(n = n(),\n            mean_perc_vac = mean(new_perc_vac, na.rm = TRUE),\n            mean_white = mean(perc_white, na.rm = TRUE))\n\n\nDescribe the tibble temp created above. What would be the dimensions? What do rows and columns represent?\nWhat would happen if we replaced new_perc_vac = ifelse(perc_complete_vac &gt; 95, NA, perc_complete_vac) with new_perc_vac = ifelse(perc_complete_vac &gt; 95, perc_complete_vac, NA)?\nWhat would happen if we replaced mean_white = mean(perc_white, na.rm = TRUE) with mean_white = mean(perc_white)?\nWhat would happen if we removed group_by(MD_group)?\n\n\nHypothetical R chunk #2:\n\n\n# Hypothetical R chunk 2\nggplot(data = vaccine_data) +\n  geom_point(mapping = aes(x = perc_over_65, y = perc_complete_vac, \n                           color = HR_party)) +\n  geom_smooth()\n\ntemp &lt;- vaccine_data |&gt;\n  group_by(HR_party) |&gt;\n  summarise(var1 = n()) |&gt;\n  arrange(desc(var1)) |&gt;\n  slice_head(n = 3)\n\nvaccine_data |&gt;\n  ggplot(mapping = aes(x = fct_reorder(HR_party, perc_over_65, .fun = median), \n                       y = perc_over_65)) +\n    geom_boxplot()\n\n\nWhy would the first plot produce an error?\nDescribe the tibble temp created above. What would be the dimensions? What do rows and columns represent?\nWhat would happen if we replaced fct_reorder(HR_party, perc_over_65, .fun = median) with HR_party?\n\n\nHypothetical R chunk #3:\n\n\n# Hypothetical R chunk 3\nvaccine_data |&gt;\n  filter(!is.na(people_per_MD)) |&gt;\n  mutate(state_lump = fct_lump(state, n = 4)) |&gt;\n  group_by(state_lump, rural_urban_code) |&gt;\n  summarise(mean_people_per_MD = mean(people_per_MD)) |&gt;\n  ggplot(mapping = aes(x = rural_urban_code, y = mean_people_per_MD, \n      colour = fct_reorder2(state_lump, rural_urban_code, mean_people_per_MD))) +\n    geom_line()\n\n\nDescribe the tibble piped into the ggplot above. What would be the dimensions? What do rows and columns represent?\nCarefully describe the plot created above.\nWhat would happen if we removed filter(!is.na(people_per_MD))?\nWhat would happen if we replaced fct_reorder2(state_lump, rural_urban_code, mean_people_per_MD) with state_lump?"
  },
  {
    "objectID": "02_maps_part1.html",
    "href": "02_maps_part1.html",
    "title": "Creating informative maps",
    "section": "",
    "text": "Based on Section 3.2.3 from Modern Data Science with R.\nYou can download this .qmd file from here. Just hit the Download Raw File button.\n\n# Initial packages required (we'll be adding more)\nlibrary(tidyverse)\nlibrary(mdsr)      # package associated with our MDSR book\n\n\nOpening example\nHere is a simple choropleth map example from MDSR\n\n# CIACountries is a 236 x 8 data set with information on each country\n#   taken from the CIA factbook - gdp, education, internet use, etc.\nhead(CIACountries)\n\n         country      pop    area oil_prod   gdp educ   roadways net_users\n1    Afghanistan 32564342  652230        0  1900   NA 0.06462444       &gt;5%\n2        Albania  3029278   28748    20510 11900  3.3 0.62613051      &gt;35%\n3        Algeria 39542166 2381741  1420000 14500  4.3 0.04771929      &gt;15%\n4 American Samoa    54343     199        0 13000   NA 1.21105528      &lt;NA&gt;\n5        Andorra    85580     468       NA 37200   NA 0.68376068      &gt;60%\n6         Angola 19625353 1246700  1742000  7300  3.5 0.04125211      &gt;15%\n\nCIACountries |&gt;\n  select(country, oil_prod) |&gt;\n  mutate(oil_prod_disc = cut(oil_prod, \n                             breaks = c(0, 1e3, 1e5, 1e6, 1e7, 1e8), \n                             labels = c(\"&gt;1000\", \"&gt;10,000\", \"&gt;100,000\", \"&gt;1 million\", \"&gt;10 million\"))) |&gt;\n  # we won't use mWorldMap often, but it's a good quick illustration\n  mosaic::mWorldMap(key = \"country\") +\n  geom_polygon(aes(fill = oil_prod_disc)) + \n  scale_fill_brewer(\"Oil Prod. (bbl/day)\", na.value = \"white\") +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\nChoropleth Maps\nWhen you have specific regions (e.g. countries, states, counties, census tracts,…) and a value associated with each region.\nA choropleth map will color the entire region according to the value. For example, let’s consider state vaccination data from March 2021.\n\nvaccines &lt;- read_csv(\"https://proback.github.io/264_fall_2024/Data/vacc_Mar21.csv\") \n\nvacc_mar13 &lt;- vaccines |&gt;\n  filter(Date ==\"2021-03-13\") |&gt;\n  select(State, Date, people_vaccinated_per100, share_doses_used, Governor)\n\nvacc_mar13\n\n# A tibble: 50 × 5\n   State       Date       people_vaccinated_per100 share_doses_used Governor\n   &lt;chr&gt;       &lt;date&gt;                        &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;   \n 1 Alabama     2021-03-13                     17.2            0.671 R       \n 2 Alaska      2021-03-13                     27.0            0.686 R       \n 3 Arizona     2021-03-13                     21.5            0.821 R       \n 4 Arkansas    2021-03-13                     19.2            0.705 R       \n 5 California  2021-03-13                     20.3            0.726 D       \n 6 Colorado    2021-03-13                     20.8            0.801 D       \n 7 Connecticut 2021-03-13                     26.2            0.851 D       \n 8 Delaware    2021-03-13                     20.2            0.753 D       \n 9 Florida     2021-03-13                     20.1            0.766 R       \n10 Georgia     2021-03-13                     15.2            0.674 R       \n# ℹ 40 more rows\n\n\nThe tricky part of choropleth maps is getting the shapes (polygons) that make up the regions. This is really a pretty complex set of lines for R to draw!\nLuckily, some maps are already created in R in the maps package.\n\nlibrary(maps)\nus_states &lt;- map_data(\"state\")\nhead(us_states)\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\n# Note that points in the same \"group\" are connected with a line\n\nus_states |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(fill = \"white\", color = \"black\")\n\n\n\n\nOther maps provided by the maps package include US counties, France, Italy, New Zealand, and two different views of the world. If you want maps of other countries or regions, you can often find them online.\nSometimes maps may be provided as shapefiles. To use these, you’ll first need to read them into R and then turn them into tidy dataframes in order to use them with ggplot. See here:. More on shapefiles in Part 2.\nWhere the really cool stuff happens is when we join our data to the us_states dataframe. Notice that the state name appears in the “region” column of us_states, and that the state name is in all small letters. In vacc_mar13, the state name appears in the State column and is in title case. Thus, we have to be very careful when we join the state vaccine info to the state geography data.\nRun this line by line to see what it does:\n\nvacc_mar13 &lt;- vacc_mar13 |&gt;\n  mutate(State = str_to_lower(State))\n\nvacc_mar13 |&gt;\n  right_join(us_states, by = c(\"State\" = \"region\")) |&gt;\n  rename(region = State) |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(aes(fill = people_vaccinated_per100), color = \"black\")\n\n\n\n\noops, New York appears to be a problem.\n\nvacc_mar13 |&gt;\n  anti_join(us_states, by = c(\"State\" = \"region\"))\n\n# A tibble: 3 × 5\n  State          Date       people_vaccinated_per100 share_doses_used Governor\n  &lt;chr&gt;          &lt;date&gt;                        &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;   \n1 alaska         2021-03-13                     27.0            0.686 R       \n2 hawaii         2021-03-13                     22.8            0.759 D       \n3 new york state 2021-03-13                     21.7            0.764 D       \n\nus_states |&gt;\n  anti_join(vacc_mar13, by = c(\"region\" = \"State\")) |&gt;\n  count(region)\n\n                region   n\n1 district of columbia  10\n2             new york 495\n\n\n[Pause to ponder:] What did we learn by running anti_join() above?\nNotice that the us_states map also includes only the contiguous 48 states. This gives an example of creating really beautiful map insets for Alaska and Hawaii.\n\nvacc_mar13 &lt;- vacc_mar13 |&gt;\n  mutate(State = str_replace(State, \" state\", \"\"))\n\nvacc_mar13 |&gt;\n  anti_join(us_states, by = c(\"State\" = \"region\"))\n\n# A tibble: 2 × 5\n  State  Date       people_vaccinated_per100 share_doses_used Governor\n  &lt;chr&gt;  &lt;date&gt;                        &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;   \n1 alaska 2021-03-13                     27.0            0.686 R       \n2 hawaii 2021-03-13                     22.8            0.759 D       \n\nus_states |&gt;\n  anti_join(vacc_mar13, by = c(\"region\" = \"State\")) %&gt;%\n  count(region)\n\n                region  n\n1 district of columbia 10\n\n\nBetter.\n\nlibrary(viridis) # for color schemes\nvacc_mar13 |&gt;\n  right_join(us_states, by = c(\"State\" = \"region\")) |&gt;\n  rename(region = State) |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(aes(fill = people_vaccinated_per100), color = \"black\") + \n  labs(fill = \"People Vaccinated\\nper 100 pop.\") +\n  # This scales the longitude and latitude so that the shapes look correct.\n  coord_map() + \n  # This theme can give you a really clean look!\n  theme_void() +  \n  # you can change the fill scale for different color schemes.\n  scale_fill_viridis() \n\n\n\n\n[Pause to ponder:] Use autofill to play with different themes and scale_fills.\nNote: Map projections are actually pretty complicated, especially if you’re looking at large areas (e.g. world maps). It’s impossible to preserve both shape and area when projecting a sphere onto a flat surface, so that’s why you sometimes see such different maps of the world\nThere are a few different options in coord_map(). See the help menu, although this function is being phased out.\nYou can also use a categorical variable to color regions:\n\nvacc_mar13 |&gt;\n  right_join(us_states, by = c(\"State\" = \"region\")) |&gt;\n  rename(region = State) |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(aes(fill = Governor), color = \"darkgrey\", linewidth = 0.2) + \n  labs(fill = \"Governor\") +\n  # This scales the longitude and latitude so that the shapes look correct.\n  coord_map() + \n  # This theme can give you a really clean look!\n  theme_void() +  \n  # you can change the fill scale for different color schemes.\n  scale_fill_manual(values = c(\"blue\", \"red\")) \n\n\n\n\n\n\nMultiple maps!\n[Pause to ponder:] are we bothered by the warning about many-to-many when you run the code below?\n\nlibrary(lubridate)\nweekly_vacc &lt;- vaccines |&gt;\n  mutate(State = str_to_lower(State)) |&gt;\n  mutate(State = str_replace(State, \" state\", \"\"),\n         week = week(Date)) |&gt;\n  group_by(week, State) |&gt;\n  summarize(date = first(Date),\n            mean_daily_vacc = mean(daily_vaccinated/est_population*1000)) |&gt;\n  right_join(us_states, by =c(\"State\" = \"region\")) |&gt;\n  rename(region = State)\n\nweekly_vacc |&gt;\n  filter(week &gt; 2, week &lt; 11) |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(aes(fill = mean_daily_vacc), color = \"darkgrey\", size = 0.1) + \n  labs(fill = \"Weekly Average Daily Vaccinations per 1000\") +\n  coord_map() + \n  theme_void() + \n  scale_fill_viridis() + \n  facet_wrap(~date) + \n  theme(legend.position = \"bottom\") \n\n\n\n\n\n\nOther cool state maps\n\nstatebin (square representation of states)\n\nlibrary(statebins) # may need to install\n\nvacc_mar13 |&gt;\n  mutate(State = str_to_title(State)) |&gt;\n  statebins(state_col = \"State\",\n            value_col = \"people_vaccinated_per100\") + \n  # one nice layout. You can customize with usual ggplot themes.\n  theme_statebins() + \n  labs(fill = \"People Vaccinated per 100\")\n\n\n\n\n[Pause to ponder:] Why might one use a map like above instead of our previous choropleth maps?\nI used this example to create the code above. The original graph is located here.\n\n\n\nInteractive map with leaflet\nLeaflet is a powerful open-source JavaScript library for building interactive maps in HTML. Although the commands are different, the architecture is very similar to ggplot2. However, instead of putting data-based layers on top of a static map, leaflet allows you to put data-based layers on top of an interactive map. Because leaflet renders as HTML to allow interactivity, they are less effective as static pdfs.\nWith leaflet, you can have “pop-up” messages when you hover over points, and have a zoom-in and zoom-out option.\nTwo main features: addTiles() = Add background map setView() = Set where the map should originally zoom to\n\n# This part is for the pop-up messages.... some are weird or just \"\\n\" for example, so this turns the weird stuff to blanks. We could also probably do this with str_ functions.\nEncoding( x = airbnb.df$AboutListing ) &lt;- \"UTF-8\"\nairbnb.df$AboutListing &lt;-\n  iconv( x = airbnb.df$AboutListing\n         , from = \"UTF-8\"\n         , to = \"UTF-8\"\n         , sub = \"\" )\nhead(airbnb.df)\n\n# A tibble: 6 × 65\n  ListingID Title    UserID baseurl Price AboutListing HostName MemberDate   Lat\n      &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt;\n1    281552 Harvard… 1.47e6 https:…   175 \"\\n\"         Mary Ca… December …  42.4\n2    182613 Luxury … 8.76e5 https:…   249 \"Entire lar… Max      July 2011   42.4\n3   1587540 Cozy Ho… 2.00e6 https:…   225 \"\\n\"         Finola   March 2012  42.4\n4    469506 Luxury … 1.77e6 https:…   140 \"\\n\"         Rupal    February …  42.3\n5   3937268 Boston … 2.53e6 https:…    99 \"I offer a … Natasha  June 2012   42.3\n6   3036349 Top flo… 1.37e6 https:…    89 \"Two bedroo… Carol    November …  42.3\n# ℹ 56 more variables: Long &lt;dbl&gt;, BookInstantly &lt;chr&gt;, Cancellation &lt;chr&gt;,\n#   PageCounter &lt;dbl&gt;, PageNumber &lt;dbl&gt;, A_AC &lt;dbl&gt;, A_Breakfast &lt;dbl&gt;,\n#   A_CableTV &lt;dbl&gt;, A_CarbonMonoxDetector &lt;dbl&gt;, A_Doorman &lt;dbl&gt;,\n#   A_Dryer &lt;dbl&gt;, A_TV &lt;dbl&gt;, A_Elevator &lt;dbl&gt;, A_Essentials &lt;dbl&gt;,\n#   A_Events &lt;dbl&gt;, A_FamilyFriendly &lt;dbl&gt;, A_FireExt &lt;dbl&gt;, A_Fireplace &lt;dbl&gt;,\n#   A_FirstAidKit &lt;dbl&gt;, A_Gym &lt;dbl&gt;, A_Heat &lt;dbl&gt;, A_HotTub &lt;dbl&gt;,\n#   A_Intercom &lt;dbl&gt;, A_Internet &lt;dbl&gt;, A_Kitchen &lt;dbl&gt;, A_Parking &lt;dbl&gt;, …\n\n# This part makes the map!\nleaflet() |&gt;\n    addTiles() |&gt;\n    setView(lng = mean(airbnb.df$Long), lat = mean(airbnb.df$Lat), \n            zoom = 13) |&gt; \n    addCircleMarkers(data = airbnb.df,\n        lat = ~ Lat, \n        lng = ~ Long, \n        popup = ~ AboutListing, \n        radius = ~ S_Accomodates,  \n        # These last options describe how the circles look\n        weight = 2,\n        color = \"red\", \n        fillColor = \"yellow\")\n\n\n\n\n\n\n\n\nOn Your Own\nThe states dataset in the poliscidata package contains 135 variables on each of the 50 US states. See here for more detail.\nYour task is to create a two meaningful choropleth plots, one using a numeric variable and one using a categorical variable from states. Write a sentence or two describing what you can learn from each plot.\nHere’s some R code to get you going:\n\nlibrary(poliscidata)   # may have to install first\n\n# Be sure you know what the mutate statement below is doing!\nstate_data &lt;- as_tibble(poliscidata::states) |&gt;\n  mutate(state_name = str_squish(str_to_lower(as.character(state)))) |&gt;\n  select(-state)\nprint(state_data, n = 5, width = Inf)\n\n# A tibble: 50 × 135\n  abort_rank3 abortion_rank12 adv_or_more ba_or_more cig_tax12 cig_tax12_3\n  &lt;fct&gt;                 &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;      \n1 Less restr               35         9         26.6     2     HiTax      \n2 Mid                      20         7.7       22       0.425 LoTax      \n3 More restr                4         6.1       18.9     1.15  MidTax     \n4 More restr                5         9.3       25.6     2     HiTax      \n5 Less restr               49        10.7       29.9     0.87  MidTax     \n  conserv_advantage conserv_public dem_advantage govt_worker gun_rank3 \n              &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;     \n1              21.3           43.1         -12.2        28   Less restr\n2              36             44.7         -14.6        17.5 Mid       \n3              26.7           45.2          -1.4        17.6 Less restr\n4              19.5           36            -3.5        15.5 Less restr\n5               6.3           30.8          14.9        14.9 More restr\n  gun_rank11 gun_scale11 hr_cons_rank11 hr_conserv11 hr_lib_rank11 hr_liberal11\n       &lt;dbl&gt;       &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1         50           0           200          55.7          228          44.3\n2         17          14           152.         65.6          278.         34.4\n3         39           4           132.         69.3          295.         30.7\n4         50           0           156.         62.6          270.         37.4\n5          1          81           274.         54.8          152.         81.0\n  hs_or_more obama2012 obama_win12  pop2000  pop2010 pop2010_hun_thou\n       &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt;    &lt;dbl&gt;            &lt;dbl&gt;\n1       91.4      40.8 No            626932   710231             7.10\n2       82.1      38.4 No           4447100  4779736            47.8 \n3       82.4      36.9 No           2673400  2915918            29.2 \n4       84.2      44.4 No           5130632  6392017            63.9 \n5       80.6      60.2 Yes         33871648 37253956           373.  \n  popchng0010 popchngpct pot_policy          prochoice prolife relig_cath\n        &lt;dbl&gt;      &lt;dbl&gt; &lt;fct&gt;                   &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n1       83299       13.3 Medicinal                  58      37       14.6\n2      332636        7.5 Decrim                     36      54        6.6\n3      242518        9.1 Not legal                  40      55        5.9\n4     1261385       24.6 Medicinal                  56      39       27.3\n5     3382308       10   Medicinal / Decrim.        65      28       31.9\n  relig_prot relig_high relig_low religiosity3 romney2012 smokers12 stateid \n       &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;             &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;   \n1       50         31.3      39.5 Low                54.8        24 \"AK    \"\n2       79.3       55.7      14.3 High               60.6        25 \"AL    \"\n3       78.6       52.3      18.7 High               60.6        26 \"AR    \"\n4       43.3       36.6      33.9 Mid                53.5        21 \"AZ    \"\n5       37.8       34.5      36.6 Low                37.1        15 \"CA    \"\n  to_0812 uninsured_pct abort_rate05 abort_rate08 abortlaw3  abortlaw10 alcohol\n    &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt; &lt;fct&gt;           &lt;dbl&gt;   &lt;dbl&gt;\n1   -9.40          21.8         13.6         12   0-5 restr           5    3.02\n2   -2.90          18.8         11.9         12   6-8 restr           8    2.01\n3   -2.90          21.9          8.3          8.7 9-10 restr          9    1.83\n4   -3.10          20.5         16           15.2 6-8 restr           6    2.31\n5   -6.5           23.2         27.1         27.6 0-5 restr           4    2.33\n  attend_pct battle04 blkleg blkpct04 blkpct08 blkpct10 bush00 bush04 carfatal\n       &lt;dbl&gt; &lt;fct&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n1         22 No            2      3.6      4.3      4.7   58.6   61.1     17.4\n2         52 No           25     26.4     26.4     26.8   56.5   62.5     24.9\n3         50 No           11     15.8     15.8     16.1   51.3   54.3     25.6\n4         29 No            1      3.5      4.2      5     51.0   54.8     20.3\n5         33 No            5      6.8      6.7      7.2   41.7   44.4     12.1\n  carfatal07 cig_tax cig_tax_3   cigarettes college conpct_m cons_hr06 cons_hr09\n       &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;            &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1       15.2   2     $1.41-$2.58       6.22    26.7     36.3      72        75  \n2       25.9   0.425 $.07-$.64         9.41    21.1     40.7      77.7      72  \n3       23.7   0.59  $.07-$.64         8.51    19.1     38.9      56.2      28.5\n4       17.6   2     $1.41-$2.58       2.4     24.3     33.3      69        49.5\n5       11.7   0.87  $.695-$1.36       3.69    29.1     28.5      37.3      35.1\n  cook_index cook_index3 defexpen demhr11 dem_hr09 demnat06 dempct_m demstate06\n       &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n1      -13.4 More Rep        3556     0        0        0       26.1       43.3\n2      -13.2 More Rep        1757    14.3     42.9     22.2     38.9       60.7\n3       -8.8 More Rep         530    25       75       83.3     43.1       75.6\n4       -6.1 Even            1771    28.6     62.5     20       31.9       44.4\n5        7.4 More Dem        1106    64.2     64.2     63.6     41.3       60.8\n  demstate09 demstate13 density division      earmarks_pcap   evm   evo evo2012\n       &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;                 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1       46.7       36.7     1.2 Pacific               426.      3     0       0\n2       57.9       35.7    94.4 E. South Cent          38.9     9     0       0\n3       72.6       46.7    56   W. South Cent          26.7     6     0       0\n4       41.1       41.1    56.3 Mountain               20.9    10     0       0\n5       64.2       66.7   239.  Pacific                12.5     0    55      55\n  evr2012 gay_policy        gay_policy2  gay_policy_con gay_support gay_support3\n    &lt;dbl&gt; &lt;fct&gt;             &lt;fct&gt;        &lt;fct&gt;                &lt;dbl&gt; &lt;fct&gt;       \n1       3 Conservative      Conservative No                      56 Med         \n2       9 Most conservative Conservative Yes                     44 Low         \n3       6 Most conservative Conservative Yes                     44 Low         \n4      11 Conservative      Conservative No                      58 Med         \n5       0 Liberal           Liberal      No                      64 High        \n  gb_win00 gb_win04  gore00 gun_check gun_dealer gun_murder10 gun_rank_rev\n  &lt;fct&gt;    &lt;fct&gt;      &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 Bush win Bush win    27.7    12016.      139.           2.7            6\n2 Bush win Bush win    41.6     9025.       47.4          2.8           30\n3 Bush win Bush win    45.9     8443.       67.4          3.2           13\n4 Bush win Bush win    44.7     5314.       45.4          3.6           13\n5 Gore win Kerry win   53.4     3040.       21.6          3.4           48\n  gunlaw_rank gunlaw_rank3_rev gunlaw_scale hispanic04 hispanic08 hispanic10\n        &lt;dbl&gt; &lt;fct&gt;                   &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1          43 Fewer restr                 4        4.9        6.1        5.5\n2          19 Mid                        15        2.2        2.9        3.9\n3          36 Fewer restr                 6        4.4        5.6        6.4\n4          36 Fewer restr                 6       28         30.1       29.6\n5           1 More restr                 79       34.7       36.6       37.6\n  indpct_m kerry04 libpct_m mccain08 modpct_m nader00 obama08 obama_win08 over64\n     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;        &lt;dbl&gt;\n1     43.6    35.5     17.9     59.4     45.7   10.1     37.9 McCain win     6.4\n2     30.0    36.8     16.8     60.3     42.5    1.10    38.7 McCain win    13.2\n3     35.9    44.6     16.8     58.7     44.3    1.46    38.9 McCain win    13.8\n4     29.7    44.4     19.2     53.4     47.4    2.98    44.9 McCain win    12.7\n5     25.8    54.3     24.2     36.9     47.3    3.82    60.9 Obama win     10.7\n  permit pop_18_24 pop_18_24_10 prcapinc region relig_import religiosity\n   &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1   NA       11.1         10.6     34454 West           NA          -177\n2   27.6     10.0         10.0     27795 South          58.5         -13\n3   21.1     10.1          9.74    25725 South          53.1         -23\n4   46.2      9.61         9.90    28442 West           33.2        -140\n5   52.8      9.95        10.5     35019 West           28.8        -147\n  reppct_m rtw   secularism secularism3 seniority_sen2 south    to_0004 to_0408\n     &lt;dbl&gt; &lt;fct&gt;      &lt;dbl&gt; &lt;fct&gt;       &lt;fct&gt;          &lt;fct&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n1     30.3 No           177 Secular     Yes            Nonsouth  -12.4   -0.800\n2     31.2 Yes           13 Religious   No             South       4.2    4.60 \n3     21.0 Yes           23 Religious   No             South       3.31  -0.200\n4     38.3 Yes          140 Secular     No             Nonsouth   -2.32   1.90 \n5     32.9 No           147 Secular     No             Nonsouth    6.63   2.90 \n  trnout00 trnout04 unemploy union04 union07 union10 urban vep00_turnout\n     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;\n1     67.2     54.8      7.5    20.1    23.8    22.3  65.6          68.1\n2     50.6     54.8      5.8     9.7     9.5    10.9  55.4          51.6\n3     46.9     50.2      5.9     4.8     5.4     4.2  52.5          47.9\n4     44.6     42.3      5.1     6.3     8.8     6.5  88.2          45.6\n5     54.6     61.2      6.2    16.5    16.7    17.2  94.4          55.7\n  vep04_turnout vep08_turnout vep12_turnout womleg_2007 womleg_2010 womleg_2011\n          &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1          69.1          68.3          58.9        21.7        21.7        23.3\n2          57.2          61.8          58.9        12.9        12.9        13.6\n3          53.6          53.4          50.5        20.7        23          22.2\n4          54.1          56            52.9        34.4        31.1        34.4\n5          58.8          61.7          55.2        28.3        27.5        28.3\n  womleg_2015 state_name\n        &lt;dbl&gt; &lt;chr&gt;     \n1        28.3 alaska    \n2        14.3 alabama   \n3        20   arkansas  \n4        35.6 arizona   \n5        25.8 california\n# ℹ 45 more rows"
  },
  {
    "objectID": "04_functions.html",
    "href": "04_functions.html",
    "title": "Functions and tidy evaluation",
    "section": "",
    "text": "Based on Chapter 25 from R for Data Science\nYou can download this .qmd file from here. Just hit the Download Raw File button."
  },
  {
    "objectID": "04_functions.html#vector-functions",
    "href": "04_functions.html#vector-functions",
    "title": "Functions and tidy evaluation",
    "section": "Vector functions",
    "text": "Vector functions\n\nExample 1: Rescale variables from 0 to 1.\nThis code creates a 10 x 4 tibble filled with random values taken from a normal distribution with mean 0 and SD 1\n\ndf &lt;- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\ndf\n\n# A tibble: 10 × 4\n         a      b       c      d\n     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 -1.03    0.464 -0.558   0.719\n 2 -0.797   0.140 -0.516  -1.22 \n 3 -0.0993  1.91  -1.16    0.583\n 4 -0.0312 -0.547  0.0132 -0.735\n 5  1.42   -0.162  0.0441  2.17 \n 6  0.207  -1.07  -0.239  -0.989\n 7 -0.348   0.128  0.891   0.362\n 8  0.692  -0.370 -1.32    0.870\n 9  1.21    2.58  -0.604   1.29 \n10  1.04    1.21   0.115   0.267\n\n\nThis code below for rescaling variables from 0 to 1 is ripe for functions… we did it four times!\nIt’s easiest to start with working code and turn it into a function.\n\ndf$a &lt;- (df$a - min(df$a)) / (max(df$a) - min(df$a))\ndf$b &lt;- (df$b - min(df$b)) / (max(df$b) - min(df$b))\ndf$c &lt;- (df$c - min(df$c)) / (max(df$c) - min(df$c))\ndf$d &lt;- (df$d - min(df$d)) / (max(df$d) - min(df$d))\ndf\n\n# A tibble: 10 × 4\n        a     b      c      d\n    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 0      0.420 0.344  0.573 \n 2 0.0946 0.332 0.363  0     \n 3 0.380  0.817 0.0709 0.532 \n 4 0.408  0.144 0.603  0.143 \n 5 1      0.249 0.617  1     \n 6 0.505  0     0.488  0.0681\n 7 0.278  0.328 1      0.467 \n 8 0.703  0.192 0      0.617 \n 9 0.915  1     0.323  0.741 \n10 0.846  0.623 0.649  0.439 \n\n\nNotice first what changes and what stays the same in each line. Then, if we look at the first line above, we see we have one value we’re using over and over: df$a. So our function will have one input. We’ll start with our code from that line, then replace the input (df$a) with x. We should give our function a name that explains what it does. The name should be a verb.\n\n# I'm going to show you how to write the function in class! \n# I have it in the code already below, but don't look yet!\n# Let's try to write it together first!\n\n. . . . . . . . .\n\n# Our function (first draft!)\nrescale01 &lt;- function(x) {\n  (x - min(x)) / (max(x) - min(x))\n}\n\nNote the general form of a function:\n\nname &lt;- function(arguments) {\n  body\n}\n\nEvery function contains 3 essential components:\n\nA name. The name should clearly evoke what the function does; hence, it is often a verb (action). Here we’ll use rescale01 because this function rescales a vector to lie between 0 and 1. snake_case is good; CamelCase is just okay.\nThe arguments. The arguments are things that vary across calls and they are usually nouns - first the data, then other details. Our analysis above tells us that we have just one; we’ll call it x because this is the conventional name for a numeric vector, but you can use any word.\nThe body. The body is the code that’s repeated across all the calls. By default a function will return the last statement; use return() to specify a return value\n\nSummary: Functions should be written for both humans and computers!\nOnce we have written a function we like, then we need to test it with different inputs!\n\ntemp &lt;- c(4, 6, 8, 9)\nrescale01(temp)\n\n[1] 0.0 0.4 0.8 1.0\n\ntemp0 &lt;- c(4, 6, 8, 9, NA)\nrescale01(temp0)\n\n[1] NA NA NA NA NA\n\n\nOK, so NA’s don’t work the way we want them to.\n\nrescale01 &lt;- function(x) {\n  (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))\n}\nrescale01(temp)\n\n[1] 0.0 0.4 0.8 1.0\n\nrescale01(temp0)\n\n[1] 0.0 0.4 0.8 1.0  NA\n\n\nWe can continue to improve our function. Here is another method, which uses the existing range function within R to avoid 3 max/min executions:\n\nrescale01 &lt;- function(x) {\n  rng &lt;- range(x, na.rm = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\nrescale01(temp)\n\n[1] 0.0 0.4 0.8 1.0\n\nrescale01(c(0, 5, 10))\n\n[1] 0.0 0.5 1.0\n\nrescale01(c(-10, 0, 10))\n\n[1] 0.0 0.5 1.0\n\nrescale01(c(1, 2, 3, NA, 5))\n\n[1] 0.00 0.25 0.50   NA 1.00\n\n\nWe should continue testing unusual inputs. Think carefully about how you want this function to behave… the current behavior is to include the Inf (infinity) value when calculating the range. You get strange output everywhere, but it’s pretty clear that there is a problem right away when you use the function. In the example below (rescale1), you ignore the infinity value when calculating the range. The function returns Inf for one value, and sensible stuff for the rest. In many cases this may be useful, but it could also hide a problem until you get deeper into an analysis.\n\nx &lt;- c(1:10, Inf)\nrescale01(x)\n\n [1]   0   0   0   0   0   0   0   0   0   0 NaN\n\nrescale1 &lt;- function(x) {\n  rng &lt;- range(x, na.rm = TRUE, finite = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\nrescale1(x)\n\n [1] 0.0000000 0.1111111 0.2222222 0.3333333 0.4444444 0.5555556 0.6666667\n [8] 0.7777778 0.8888889 1.0000000       Inf\n\n\nNow we’ve used functions to simplify original example. We will learn to simplify further in iterations (Ch 26)\n\ndf &lt;- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n# add a little noise\ndf$a[5] = NA\ndf$b[6] = Inf\ndf\n\n# A tibble: 10 × 4\n          a       b       c       d\n      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1  0.530    -0.430 -0.0200  0.391 \n 2  1.20     -0.478 -1.45   -0.149 \n 3 -0.667    -0.576  0.178   0.771 \n 4  0.409    -0.950  0.626  -0.0997\n 5 NA        -1.36   0.438  -0.952 \n 6  0.214   Inf      0.727   0.208 \n 7  0.549    -2.12  -0.841   0.833 \n 8 -0.00366  -0.430  0.111  -1.02  \n 9 -0.141    -0.495 -0.123   0.950 \n10  0.234    -0.242 -0.0251  0.800 \n\ndf$a_new &lt;- rescale1(df$a)\ndf$b_new &lt;- rescale1(df$b)\ndf$c_new &lt;- rescale1(df$c)\ndf$d_new &lt;- rescale1(df$d)\ndf\n\n# A tibble: 10 × 8\n          a       b       c       d  a_new   b_new c_new  d_new\n      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1  0.530    -0.430 -0.0200  0.391   0.641   0.899 0.657 0.716 \n 2  1.20     -0.478 -1.45   -0.149   1       0.874 0     0.441 \n 3 -0.667    -0.576  0.178   0.771   0       0.822 0.748 0.909 \n 4  0.409    -0.950  0.626  -0.0997  0.576   0.622 0.954 0.467 \n 5 NA        -1.36   0.438  -0.952  NA       0.403 0.867 0.0333\n 6  0.214   Inf      0.727   0.208   0.472 Inf     1     0.623 \n 7  0.549    -2.12  -0.841   0.833   0.651   0     0.281 0.941 \n 8 -0.00366  -0.430  0.111  -1.02    0.355   0.899 0.717 0     \n 9 -0.141    -0.495 -0.123   0.950   0.281   0.865 0.610 1     \n10  0.234    -0.242 -0.0251  0.800   0.483   1     0.655 0.924 \n\ndf %&gt;% \n  mutate(a_new = rescale1(a),\n         b_new = rescale1(b),\n         c_new = rescale1(c),\n         d_new = rescale1(d))\n\n# A tibble: 10 × 8\n          a       b       c       d  a_new   b_new c_new  d_new\n      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1  0.530    -0.430 -0.0200  0.391   0.641   0.899 0.657 0.716 \n 2  1.20     -0.478 -1.45   -0.149   1       0.874 0     0.441 \n 3 -0.667    -0.576  0.178   0.771   0       0.822 0.748 0.909 \n 4  0.409    -0.950  0.626  -0.0997  0.576   0.622 0.954 0.467 \n 5 NA        -1.36   0.438  -0.952  NA       0.403 0.867 0.0333\n 6  0.214   Inf      0.727   0.208   0.472 Inf     1     0.623 \n 7  0.549    -2.12  -0.841   0.833   0.651   0     0.281 0.941 \n 8 -0.00366  -0.430  0.111  -1.02    0.355   0.899 0.717 0     \n 9 -0.141    -0.495 -0.123   0.950   0.281   0.865 0.610 1     \n10  0.234    -0.242 -0.0251  0.800   0.483   1     0.655 0.924 \n\n# Even better - from Chapter 26\ndf |&gt; mutate(across(a:d, rescale1))\n\n# A tibble: 10 × 8\n        a       b     c      d  a_new   b_new c_new  d_new\n    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1  0.641   0.899 0.657 0.716   0.641   0.899 0.657 0.716 \n 2  1       0.874 0     0.441   1       0.874 0     0.441 \n 3  0       0.822 0.748 0.909   0       0.822 0.748 0.909 \n 4  0.576   0.622 0.954 0.467   0.576   0.622 0.954 0.467 \n 5 NA       0.403 0.867 0.0333 NA       0.403 0.867 0.0333\n 6  0.472 Inf     1     0.623   0.472 Inf     1     0.623 \n 7  0.651   0     0.281 0.941   0.651   0     0.281 0.941 \n 8  0.355   0.899 0.717 0       0.355   0.899 0.717 0     \n 9  0.281   0.865 0.610 1       0.281   0.865 0.610 1     \n10  0.483   1     0.655 0.924   0.483   1     0.655 0.924 \n\n\n\n\nOptions for handling NAs in functions\nBefore we try some practice problems, let’s consider various options for handling NAs in functions. We used the na.rm option within functions like min, max, and range in order to take care of missing values. But there are alternative approaches:\n\nfilter/remove the NA values before rescaling\ncreate an if statement to check if there are NAs; return an error if NAs exist\ncreate a removeNAs option in the function we are creating\n\nLet’s take a look at each alternative approach in turn:\n\nFilter/remove the NA values before rescaling\n\ndf &lt;- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\ndf$a[5] = NA\ndf\n\n# A tibble: 10 × 4\n        a      b      c      d\n    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 -0.504  0.603  1.47   0.453\n 2  0.817  0.640 -0.496  1.75 \n 3 -0.805 -0.252 -3.17   0.291\n 4 -0.665  1.32   1.61  -1.95 \n 5 NA      0.489  0.673  0.187\n 6 -1.73   0.940 -1.60  -0.370\n 7 -0.102  0.881  0.389 -0.448\n 8 -1.16   4.84  -0.708  1.39 \n 9  0.886  1.59  -0.748 -0.860\n10 -0.365 -1.80   1.23  -0.482\n\nrescale_basic &lt;- function(x) {\n  (x - min(x)) / (max(x) - min(x))\n}\n\ndf %&gt;%\n  filter(!is.na(a)) %&gt;%\n  mutate(new_a = rescale_basic(a))\n\n# A tibble: 9 × 5\n       a      b      c      d new_a\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 -0.504  0.603  1.47   0.453 0.468\n2  0.817  0.640 -0.496  1.75  0.974\n3 -0.805 -0.252 -3.17   0.291 0.352\n4 -0.665  1.32   1.61  -1.95  0.406\n5 -1.73   0.940 -1.60  -0.370 0    \n6 -0.102  0.881  0.389 -0.448 0.622\n7 -1.16   4.84  -0.708  1.39  0.215\n8  0.886  1.59  -0.748 -0.860 1    \n9 -0.365 -1.80   1.23  -0.482 0.521\n\n\n[Pause to Ponder:] Do you notice anything in the output above that gives you pause?\n\n\nCreate an if statement to check if there are NAs; return an error if NAs exist\nFirst, here’s an example involving weighted means:\n\n# Create function to calculate weighted mean\nwt_mean &lt;- function(x, w) {\n  sum(x * w) / sum(w)\n}\nwt_mean(c(1, 10), c(1/3, 2/3))\n\n[1] 7\n\nwt_mean(1:6, 1:3)\n\n[1] 7.666667\n\n\n[Pause to Ponder:] Why is the answer to the last call above 7.67??\n\n# update function to handle cases where data and weights of unequal length\nwt_mean &lt;- function(x, w) {\n  if (length(x) != length(w)) {\n    stop(\"`x` and `w` must be the same length\", call. = FALSE)\n  } else {\n  sum(w * x) / sum(w)\n  }  \n}\nwt_mean(1:6, 1:3) \n\nError: `x` and `w` must be the same length\n\n# should produce an error now if weights and data different lengths\n#  - nice example of if and else\n\n[Pause to Ponder:] What does the call. option do?\nNow let’s apply this to our rescaling function\n\nrescale_w_error &lt;- function(x) {\n  if (is.na(sum(x))) {\n    stop(\"`x` cannot have NAs\", call. = FALSE)\n  } else {\n  (x - min(x)) / (max(x) - min(x))\n  }  \n}\n\ntemp &lt;- c(4, 6, 8, 9)\nrescale_w_error(temp)\n\n[1] 0.0 0.4 0.8 1.0\n\ntemp &lt;- c(4, 6, 8, 9, NA)\nrescale_w_error(temp)\n\nError: `x` cannot have NAs\n\n\n[Pause to Ponder:] Why can’t we just use if (is.na(x)) instead of is.na(sum(x))?\n\n\nCreate a removeNAs option in the function we are creating\n\nrescale_NAoption &lt;- function(x, removeNAs = FALSE) {\n  (x - min(x, na.rm = removeNAs)) / \n    (max(x, na.rm = removeNAs) - min(x, na.rm = removeNAs))\n} \n\ntemp &lt;- c(4, 6, 8, 9)\nrescale_NAoption(temp)\n\n[1] 0.0 0.4 0.8 1.0\n\ntemp &lt;- c(4, 6, 8, 9, NA)\nrescale_NAoption(temp, removeNAs = TRUE)\n\n[1] 0.0 0.4 0.8 1.0  NA\n\n\nOK, but all the other summary stats functions use na.rm as the input, so to be consistent, it’s probably better to do something slightly awkward like this:\n\nrescale_NAoption &lt;- function(x, na.rm = FALSE) {\n  (x - min(x, na.rm = na.rm)) / \n    (max(x, na.rm = na.rm) - min(x, na.rm = na.rm))\n} \n\ntemp &lt;- c(4, 6, 8, 9, NA)\nrescale_NAoption(temp, na.rm = TRUE)\n\n[1] 0.0 0.4 0.8 1.0  NA\n\n\nwt_mean() is an example of a “summary function (single value output) instead of a”mutate function” (vector output) like rescale01(). Here’s another summary function to produce the mean absolute percentage error:\n\nmape &lt;- function(actual, predicted) {\n  sum(abs((actual - predicted) / actual)) / length(actual)\n}\n\ny &lt;- c(2,6,3,8,5)\nyhat &lt;- c(2.5, 5.1, 4.4, 7.8, 6.1)\nmape(actual = y, predicted = yhat)\n\n[1] 0.2223333"
  },
  {
    "objectID": "04_functions.html#data-frame-functions",
    "href": "04_functions.html#data-frame-functions",
    "title": "Functions and tidy evaluation",
    "section": "Data frame functions",
    "text": "Data frame functions\nThese work like dplyr verbs, taking a data frame as the first argument, and then returning a data frame or a vector.\n\nDemonstration of tidy evaluation in functions\n\n# Start with working code then functionize\nggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +\n  geom_point(size = 0.75) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\nmake_plot &lt;- function(dataset, xvar, yvar, pt_size = 0.75)  {\n  ggplot(data = dataset, mapping = aes(x = xvar, y = yvar)) +\n    geom_point(size = pt_size) +\n    geom_smooth()\n}\n\nmake_plot(dataset = mpg, xvar = cty, yvar = hwy)  # Error!\n\nError in `geom_point()`:\n! Problem while computing aesthetics.\nℹ Error occurred in the 1st layer.\nCaused by error:\n! object 'cty' not found\n\n\nThe problem is tidy evaluation, which makes most common coding easier, but makes some less common things harder. Key terms to understand tidy evaluation:\n\nenv-variables = live in the environment (mpg)\ndata-variables = live in data frame or tibble (cty)\ndata masking = tidyverse use data-variables as if env-variables. That is, you don’t always need mpg$cty to access cty in tidyverse\n\nThe key idea behind data masking is that it blurs the line between the two different meanings of the word “variable”:\n\nenv-variables are “programming” variables that live in an environment. They are usually created with &lt;-.\ndata-variables are “statistical” variables that live in a data frame. They usually come from data files (e.g. .csv, .xls), or are created manipulating existing variables.\n\nThe solution is to embrace {{ }} data-variables which are user inputs into functions. One way to remember what’s happening, as suggested by our book authors, is to think of {{ }} as looking down a tunnel — {{ var }} will make a dplyr function look inside of var rather than looking for a variable called var.\nSee Section 25.3 of R4DS for more details (and there are plenty!).\n\n# This will work to make our plot!\nmake_plot &lt;- function(dataset, xvar, yvar, pt_size = 0.75)  {\n  ggplot(data = dataset, mapping = aes(x = {{ xvar }}, y = {{ yvar }})) +\n    geom_point(size = pt_size) +\n    geom_smooth()\n}\n\nmake_plot(dataset = mpg, xvar = cty, yvar = hwy)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nI often wish it were easier to get my own custom summary statistics for numeric variables in EDA rather than using mosaic:favstats(). Using group_by() and summarise() from the tidyverse reads clearly but takes so many lines, but if I only had to write the code once…\n\nsummary6 &lt;- function(data, var) {\n  data |&gt; summarize(\n    min = min({{ var }}, na.rm = TRUE),\n    mean = mean({{ var }}, na.rm = TRUE),\n    median = median({{ var }}, na.rm = TRUE),\n    max = max({{ var }}, na.rm = TRUE),\n    n = n(),\n    n_miss = sum(is.na({{ var }})),\n    .groups = \"drop\"    # to leave the data in an ungrouped state\n  )\n}\n\nmpg |&gt; summary6(hwy)\n\n# A tibble: 1 × 6\n    min  mean median   max     n n_miss\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n1    12  23.4     24    44   234      0\n\n\nEven cooler, I can use my new function with group_by()!\n\nmpg |&gt; \n  group_by(drv) |&gt;\n  summary6(hwy)\n\n# A tibble: 3 × 7\n  drv     min  mean median   max     n n_miss\n  &lt;chr&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n1 4        12  19.2     18    28   103      0\n2 f        17  28.2     28    44   106      0\n3 r        15  21       21    26    25      0\n\n\nYou can even pass conditions into a function using the embrace:\n[Pause to Ponder:] Predict what the code below will do, then uncomment it and run it to check. Think about: why use sort = sort? why not embrace df? why didn’t we need n in the arguments?\n\n#new_function &lt;- function(df, var, condition, sort = TRUE) {\n#  df |&gt;\n#    filter({{ condition }}) |&gt;\n#    count({{ var }}, sort = sort) |&gt;\n#    mutate(prop = n / sum(n))\n#}\n\n#mpg |&gt; new_function(var = manufacturer, \n#                    condition = manufacturer %in% c(\"audi\", \"honda\", \"hyundai\", \"nissan\", \"subaru\", \"toyota\", \"volkswagen\"))\n\n\n\nData-masking vs. tidy-selection (Section 25.3.4)\nWhy doesn’t the following code work?\n\ncount_missing &lt;- function(df, group_vars, x_var) {\n  df |&gt; \n    group_by({{ group_vars }}) |&gt; \n    summarize(\n      n_miss = sum(is.na({{ x_var }})),\n      .groups = \"drop\"\n    )\n}\n\nflights |&gt; \n  count_missing(c(year, month, day), dep_time)\n\nError in `group_by()`:\nℹ In argument: `c(year, month, day)`.\nCaused by error:\n! `c(year, month, day)` must be size 336776 or 1, not 1010328.\n\n\nThe problem is that group_by() uses data-masking rather than tidy-selection. These are the two most common subtypes of tidy evaluation:\n\nData-masking is used in functions like arrange(), filter(), and summarize() that compute with variables. Data masking is an R feature that blends programming variables that live inside environments (env-variables) with statistical variables stored in data frames (data-variables).\n\nTidy-selection is used for functions like select(), relocate(), and rename() that select variables. Tidy selection provides a concise dialect of R for selecting variables based on their names or properties.\n\nMore detail can be found here.\nThe error above can be solved by using the pick() function:\n\ncount_missing &lt;- function(df, group_vars, x_var) {\n  df |&gt; \n    group_by(pick({{ group_vars }})) |&gt; \n    summarize(\n      n_miss = sum(is.na({{ x_var }})),\n      .groups = \"drop\"\n  )\n}\n\nflights |&gt; \n  count_missing(c(year, month, day), dep_time)\n\n# A tibble: 365 × 4\n    year month   day n_miss\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1  2013     1     1      4\n 2  2013     1     2      8\n 3  2013     1     3     10\n 4  2013     1     4      6\n 5  2013     1     5      3\n 6  2013     1     6      1\n 7  2013     1     7      3\n 8  2013     1     8      4\n 9  2013     1     9      5\n10  2013     1    10      3\n# ℹ 355 more rows\n\n\n[Pause to Ponder:] Here’s another nice use of pick(). Predict what the function will do, then uncomment the code to see if you are correct.\n\n# Source: https://twitter.com/pollicipes/status/1571606508944719876\n#new_function &lt;- function(data, rows, cols) {\n#  data |&gt; \n#    count(pick(c({{ rows }}, {{ cols }}))) |&gt; \n#    pivot_wider(\n#      names_from = {{ cols }}, \n#      values_from = n,\n#      names_sort = TRUE,\n#      values_fill = 0\n#    )\n#}\n\n#mpg |&gt; new_function(c(manufacturer, model), cyl)"
  },
  {
    "objectID": "04_functions.html#plot-functions",
    "href": "04_functions.html#plot-functions",
    "title": "Functions and tidy evaluation",
    "section": "Plot functions",
    "text": "Plot functions\nLet’s say you find yourself making a lot of histograms:\n\nflights |&gt; \n  ggplot(aes(x = dep_time)) +\n  geom_histogram(bins = 25)\n\n\n\nflights |&gt; \n  ggplot(aes(x = air_time)) +\n  geom_histogram(bins = 35)\n\n\n\n\nJust use embrace to create a histogram-making function\n\nhistogram &lt;- function(df, var, bins = NULL) {\n  df |&gt; \n    ggplot(aes(x = {{ var }})) + \n    geom_histogram(bins = bins)\n}\n\nflights |&gt; histogram(air_time, 35)\n\n\n\n\nSince histogram() returns a ggplot, you can add any layers you want\n\nflights |&gt; \n  histogram(air_time, 35) +\n  labs(x = \"Flight time (minutes)\", y = \"Number of flights\")\n\n\n\n\nYou can also combine data wrangling with plotting. Note that we need the “walrus operator” (:=) since the variable name on the left is being generated with user-supplied data.\n\n# sort counts with highest values at top and counts on x-axis\nsorted_bars &lt;- function(df, var) {\n  df |&gt; \n    mutate({{ var }} := fct_rev(fct_infreq({{ var }})))  |&gt;\n    ggplot(aes(y = {{ var }})) +\n    geom_bar()\n}\n\nflights |&gt; sorted_bars(carrier)\n\n\n\n\nFinally, it would be really helpful to label plots based on user inputs. This is a bit more complicated, but still do-able!\nFor this, we’ll need the rlang package. rlang is a low-level package that’s used by just about every other package in the tidyverse because it implements tidy evaluation (as well as many other useful tools).\nCheck out the following update of our histogram() function which uses the englue() function from the rlang package:\n\nhistogram &lt;- function(df, var, bins) {\n  label &lt;- rlang::englue(\"A histogram of {{var}} with binwidth {bins}\")\n  \n  df |&gt; \n    ggplot(aes(x = {{ var }})) + \n    geom_histogram(bins = {bins}) + \n    labs(title = label)\n}\n\nflights |&gt; histogram(air_time, 35)\n\n\n\n\n\nOn Your Own\n\nRewrite this code snippet as a function: x / sum(x, na.rm = TRUE). This code creates weights which sum to 1, where NA values are ignored. Test it for at least two different vectors. (Make sure at least one has NAs!)\nCreate a function to calculate the standard error of a variable, where SE = square root of the variance divided by the sample size. Hint: start with a vector like x &lt;- 0:5 or x &lt;- gss_cat$age and write code to find the SE of x, then turn it into a function to handle any vector x. Note: var is the function to find variance in R and sqrt does square root. length will also be handy. Test your function on two vectors that do not include NAs (i.e. do not worry about removing NAs at this point).\nUse your se function within summarize to get a table of the mean and s.e. of hwy and cty by class in the mpg dataset.\nUse your se function within summarize to get a table of the mean and s.e. of arr_delay and dep_delay by carrier in the flights dataset. Why does the output look like this?\nMake your se function handle NAs with an na.rm option. Test your new function (you can call it se again) on a vector that doesn’t include NA and on the same vector with an added NA. Be sure to check that it gives the expected output with na.rm = TRUE and na.rm = FALSE. Make na.rm = FALSE the default value. Repeat #4. (Hint: be sure when you divide by sample size you don’t count any NAs)\nCreate both_na(), a function that takes two vectors of the same length and returns how many positions have an NA in both vectors. Hint: create two vectors like test_x &lt;- c(1, 2, 3, NA, NA) and test_y &lt;- c(NA, 1, 2, 3, NA) and write code that works for test_x and test_y, then turn it into a function that can handle any x and y. (In this case, the answer would be 1, since both vectors have NA in the 5th position.) Test it for at least one more combination of x and y.\nRun your code from (6) with the following two vectors: test_x &lt;- c(1, 2, 3, NA, NA, NA) and test_y &lt;- c(NA, 1, 2, 3, NA). Did you get the output you wanted or expected? Modify your function using if, else, and stop to print an error if x and y are not the same length. Then test again with test_x, test_y and the sets of vectors you used in (6).\nHere is a way to get not_cancelled flights in the flights dataset:\n\n\nnot_cancelled &lt;- flights %&gt;% \n  filter(!is.na(dep_delay), !is.na(arr_delay))\n\nIs it necessary to check is.na for both departure and arrival? Using summarize, find the number of flights missing departure delay, arrival delay, and both. (Use your new function!)\n\nRead the code for each of the following three functions, puzzle out what they do, and then brainstorm better names.\n\n\nf1 &lt;- function(time1, time2) {\n  hour1 &lt;- time1 %/% 100\n  min1 &lt;- time1 %% 100\n  hour2 &lt;- time2 %/% 100\n  min2 &lt;- time2 %% 100\n  \n  (hour2 - hour1)*60 + (min2 - min1)\n}\n\n\nf2 &lt;- function(lengthcm, widthcm) {\n  (lengthcm / 2.54) * (widthcm / 2.54)\n}\n\n\nf3 &lt;- function(x) {\n  fct_collapse(x, \"non answer\" = c(\"No answer\", \"Refused\", \n                                   \"Don't know\", \"Not applicable\"))\n}\n\n\nExplain what the following function does and demonstrate by running foo1(x) with a few appropriately chosen vectors x. (Hint: set x and run the “guts” of the function piece by piece.)\n\n\nfoo1 &lt;- function(x) {\n  diff &lt;- x[-1] - x[1:(length(x) - 1)]\n  sum(diff &lt; 0)\n}\n\n\nThe foo1() function doesn’t perform well if a vector has missing values. Amend foo1() so that it produces a helpful error message and stops if there are any missing values in the input vector. Show that it works with appropriately chosen vectors x. Be sure you add error = TRUE to your R chunk, or else knitting will fail!\nWrite a function called greet using if, else if, and else to print out “good morning” if it’s before 12 PM, “good afternoon” if it’s between 12 PM and 5 PM, and “good evening” if it’s after 5 PM. Your function should work if you input a time like: greet(time = \"2018-05-03 17:38:01 CDT\") or if you input the current time with greet(time = Sys.time()). [Hint: check out the hour function in the lubridate package]\nWrite a function called summary_stats() to produce a custom set of summary statistics (n, mean, median, sd, IQR, min, and max) for any variable that you input from a tibble. Add an option to remove missing values, if any exist (and be sure the sample size n reflects the number of non-missing values). Thus, your function should have 3 inputs: data (the tibble of interest), x (the variable of interest), and na_option (with a default value of FALSE). Show that your function works for (a) the hwy variable in mpg_tbl &lt;- as_tibble(mpg), and (b) the age variable in gss_cat.\nAdd an option to (4) to produce summary statistics by group for a second variable. Show that your function works for (a) the hwy variable in mpg_tbl &lt;- as_tibble(mpg) grouped by drv, and (b) the age variable in gss_cat grouped by partyid.\nCreate a function that has a vector as the input and returns the last value. (Note: Be sure to use a name that does not write over an existing function!)"
  },
  {
    "objectID": "github_website.html",
    "href": "github_website.html",
    "title": "Using GitHub",
    "section": "",
    "text": "Version Control (GitHub)\nIn order to collaborate on an R project (or any coding project in general), data scientists typically use a version control system like GitHub. With GitHub, you never have to save files as Final.docx, Final2.docx, Newfinal.docx, RealFinal.docx, nothisisreallyit.docx, etc. You update code like .qmd and .Rmd and data files, recording descriptions of any changes. Then if you ever want to go back to an earlier version, GitHub can facilitate that. Or if you want to make your work public, others can see it and even suggest changes, but you are ultimately in control of any changes that get made.\nAnd, you can have multiple collaborators with access to the same set of files. While it can be dicey if multiple people have the file open and make changes at the same time, if you do this with GitHub, it is at least POSSIBLE to get it straightened out, and the second person who tries to save will get warned. If you are both just using a common folder on RStudio, you can easily write over and erase each other’s work. (If you use a common folder, be sure that one person is editing at a time to prevent this).\nIn order to begin to get familiar with GitHub, we will use it to create a personal website.\n\n\n\n\n\n\nAn alternative approach\n\n\n\n\n\nYou can also follow instructions at the excellent website Creating Your Personal Website using Quarto by Sam Csik (the underlying quarto code for creating this website can be found here).\nProcess Overview\n\nThe general plan is to create a local version (using RStudio on your own computer) of a quarto webpage, and then connect the files you build to an upstream cloud-based hosting service (GitHub).\n\nAfter initializing a basic webpage, you will toggle between RStudio to improve the webpage and GitHub to save the results in a public repository.\nFinally, you will use GitHub Page to publish and host your webpage.\n\nClarifications\nWhile Sam Csik’s instructions and insights are awesome, there are a few places where you should tread carefully or take a slightly different path:\n\nBe sure to heed the “Before getting started” box in this section! In particular, follow the MEDS Installation Guide and make sure you have the latest versions of both R and RStudio, a GitHub account, and Git installed on your machine.\nIn that same section, be sure to choose the “Using the RStudio IDE” tab to access that set of instructions.\nBe careful with the first step in this section. I would recommend the second option, although you’ll actually need to select File &gt; New File &gt; Text file to open a blank text file, and then use Save As to save that blank file as “.nojekyll” (this file will be invisible in RStudio but not in GitHub). If you use the first option, her command is only for Macs; Windows users should use “copy NUL .nojekyll” in the Terminal.\nIn Step 3, you can also type “quarto_render()” in the Console. You might have to run this twice if you needed to delete your _site folder as in the green box.\nIn Step 5, you also have to return to the main page of your GitHub repository and hit the Settings button to the right of About. In the box that appears, you’ll enter a description of your project, and check “Use your GitHub Pages website”.\n\n\n\n\n\n\nGetting started on GitHub and connecting to RStudio\n\nCreate a GitHub account at github.com. It’s usually okay to hit “Skip Personalization” at the bottom of the screen after entering an email, username, and password. There are a few bonuses you can get as a student that you might consider.\nObtain a personal access token (PAT) in GitHub using the following steps:\n\nClick your profile picture/symbol in the upper right of your GitHub page. Then select Settings &gt; Developer settings &gt; Personal access tokens &gt; Tokens (classic).\nClick “Generate new token” and give a descriptive name like “My PAT for RStudio”. Note that the default expiration is 30 days; I did a Custom setting through the end of the semester.\nSelect scopes and permissions; I often select: repo, workflow, gist, and user.\nClick “Generate token”. Copy your personal access token and store it somewhere.\n\nStore your credentials in RStudio using the following steps:\n\nIn the console, type library(credentials). You might have to install the credentials package first.\nThen type set_github_pat(), hit Return, choose the Token option, and copy in your personal access token\n\n\nAlert! If the steps in (3) don’t work, you may have to install Git on your computer first. This chapter in “Happy Git with R” provides nice guidance for installing Git. Installing Git for Windows seems to work well on most Windows machines, using this site seems to work well for macOS, and a command like sudo apt-get install git can often work nicely in Linux. Once Git is installed, restart RStudio, and it will usually magically find Git. If not, there’s some good advice in this chapter of “Happy Git with R”. If you ever get frustrated with Git, remember that No one is giving out Git Nerd merit badges! Just muddle through until you figure out something that works for you!\n\n\nCreating an R project (local) that’s connected to GitHub (cloud)\n\nIn your GitHub account, click the \\(+ \\nabla\\) (+down arrow) button near the top right and select New Repository (repo). For a personal website, you might put something like “proback.github.io” for your repository (repo) name, where you should replace proback with your GitHub user name. For other repos, choose simple but descriptive names that should avoid spaces. Be sure Public is checked; sometimes I create Private repositories and turn them Public if I want later, but we know a personal webpage should be Public. Check Add a ReadMe File. Finally hit Create Repository and copy the URL once your repo has been created; the URL should be something like github.com/username/user_name.github.io.\nGo into your RStudio and select File &gt; New Project &gt; Version Control &gt; Git. For the repository URL paste in the URL for the repository you just created. A project directory named “username.github.io” will be created in your specified folder (which you can navigate to).\n\nNotice that you are now starting with a blank slate! Nothing in the environment or history. Also note where it says your project name in the top right corner.\nAt this point your should have a GitHub repo called “user_name.github.io” connected to an R project named “username.github.io”. The basic framework is set!\n\n\nCreating a personal website in RStudio (local)\n\nFollow the instructions here for constructing a personal website on your local machine. These instructions use materials from Brianna Heggeseth at Macalester, Joyce Robbins at Columbia, and quarto documentation.\n\n\n\nPushing your work to GitHub (cloud)\n\nWe now want to “push” the changes made in “user_name.github.io” to your GitHub repo (the changes have only been made in your local RStudio for now).\n\nIn the console, type library(credentials)\nThen type set_github_pat(), hit Return, and copy in your personal access token. We do this again because we are in a new project. (These first two steps might not be necessary, but it’s good to make sure.)\nunder the Git tab in the Environment panel, check the boxes in front of all modified files to “stage” your changes. To select a large number of files, check the top box, scroll down to the bottom of the list, and then shift-click the final box\nclick the Commit tab to “commit” your changes (like saving a file locally) along with a message describing the changes you made. GitHub guides you by by showing your old code (red) and new code (green), to make sure you approve of the changes. This first time, all code should be green since we’ve only added new things rather than modifying previously saved files.\n“push” your changes to GitHub (an external website) by clicking the green Up arrow. Refresh your GitHub account to see new files in the user_name.github.io repo!\n\n\n\n\nModifying files that have already been pushed to GitHub\n\nMake a change (anything) to one of your files in RStudio. Now go back under the Git tab and “push” these new changes to GitHub. You’ll have to go through the same process of Stage, Commit, and Push, although this time you’ll see only your newest changes in green when you Commit. Confirm that your changes appear in GitHub.\n\n\n\nPulling work from GitHub\nBefore you start a new session of working on a project in RStudio, you should always Pull changes from GitHub first. Most of the time there will be nothing new, but if a collaborator made changes since the last time you worked on a file, you want to make sure you’re working with the latest and greatest version. If not, you’ll end up trying to Push changes made to an old version, and GitHub will balk and produce Merge Conflict messages. We’ll see how to handle Merge Conflicts later, but it’s a bit of a pain and best avoided!\n\nGo into one of your files on GitHub and hit the Edit icon. Add a line anywhere, and then scroll down to hit Commit Changes. (This is not recommended and for illustrative purposes only! You typically won’t edit directly in GitHub, but we’re emulating what might happen if a collaborator makes changes since the last time you worked on a document.) Now go back to RStudio and, under the Git tab, “Pull” the changes from GitHub into your R project folder. (Use the blue Down arrow). Confirm that your changes now appear in RStudio. Before you start working on the R server, you should always Pull any changes that might have been made on GitHub (especially if you’re working on a team!), since things can get dicey if you try to merge new changes from RStudio with new changes on GitHub.\n\n\n\nA bit more about R projects\n\nTo see the power of projects, select File &gt; Close Project and Don’t Save the workspace image. Then, select File &gt; Recent Projects &gt; user_name.github.io; you will get a clean Environment and Console once again, but History shows the commands you ran by hand, active Rmd and qmd files appear in the Source panel, and Files contains the Rmd, qmd, html, and csv files produced by your last session. And you can stage, commit and push the changes and the new file to GitHub!"
  },
  {
    "objectID": "miniproject1.html",
    "href": "miniproject1.html",
    "title": "Mini-Project 1: Maps",
    "section": "",
    "text": "Overview\nYou will produce a personal website using Quarto that contains, at a minimum, a Home/About page with a picture or image, some basic information about you, and a link to the GitHub repo that contains the R code for your website.\nThen, in the Menu bar, you should have a selection called “Maps” with two sub-selections:\n\n“US States”. On this page, you should produce a choropleth map of the U.S. states (you can choose to exclude Alaska and Hawaii) like we did in the “Creating Informative Maps” activity. Just as we found state-level data from both a vaccine data set and the poliscidata package, you should find your own state-level data. Merge your data with the us_states dataframe, and then produce a choropleth map. Your variable of interest can be either numeric or categorical. Be sure you label your plot well and provide a description of what insights can be gained from your plot.\n“Wisconsin Districts”. On this page, you will emulate our analysis of potential gerrymandering in North Carolina by generating a similar plot of the state of Wisconsin. For background about controversial congressional districts and gerrymandering in Wisconsin, see this article.\n\nA few detailed instructions and hints for Part 2:\n\nuse voting data from 2016 from the fec16 package. Make sure that data from fec16 is being used rather than similarly named data from fec12, for instance by first running detach(\"package:fec12\").\nyour final plot should have shaded reds and blues representing relative levels of support for Republican and Democrat candidates in a district. You may use ggplot2 or leaflet.\nif you’ve been doing previous geospatial modeling in RStudio, you may have to clear your environment and start a new R session to have adequate space available.\n\nthe UCLA site with district geographies only goes to 2012, so you can’t grab 115th Congress in 2016 from there. You could try to find that updated geospatial information, potentially at a site like this:, but it’s fine to just use the 2012 geospatial data like we did with North Carolina.\n\nAgain, be sure you label your plot well and provide a description of what insights can be gained from your plot.\n\n\nTimeline\nMini-Project 1 must be submitted on moodle by 11:00 PM on Wed Feb 28. All you need to submit is a URL for your website!"
  },
  {
    "objectID": "tech_setup.html",
    "href": "tech_setup.html",
    "title": "Tech Setup",
    "section": "",
    "text": "Ideally before class on Thurs Feb 8, and definitely before class on Tues Feb 13, you should follow these instructions to set up the software that we’ll be using throughout the semester. Even if you’ve already downloaded both R and RStudio, you’ll want to re-download to make sure that you have the most current versions.\n\nRequired: Download R and RStudio\n\nFIRST: Download R here.\n\nIn the top section, you will see three links “Download R for …”\nChoose the link that corresponds to your computer.\nAs of Feb 1, 2024, the latest version of R is 4.3.2 (“Eye Holes”).\n\nSECOND: Download RStudio here.\n\nClick the button under step 2 to install the version of RStudio recommended for your computer.\nAs of Feb 1, 2024, the latest version of RStudio is 2023.12.1 (Build 402).\n\nTHIRD: Check that when you go to File &gt; New Project &gt; New Directory, you see “Quarto Website” as an option.\n\n\nSuggested: Watch this video from Lisa Lendway at Macalester describing key configuration options for RStudio.\n\nSuggested: Change the default file download location for your internet browser.\n\nGenerally by default, internet browsers automatically save all files to the Downloads folder on your computer. In that case, you have to grab files from Downloads and move them to a more appropriate storage spot. You can change this option so that your browser asks you where to save each file before downloading it.\nThis page has information on how to do this for the most common browsers.\n\n\nRequired: Install required packages.\n\nAn R package is an extra bit of functionality that will help us in our data analysis efforts in a variety of ways. Many contributors create open source packages that can be added to base R to perform certain tasks in new and better ways.\nFor now, we’ll just make sure the tidyverse package is installed. Open RStudio and click on the Packages tab in the bottom right pane. inside the Console pane (by default, the bottom left pane). Click the Install button and type “tidyverse” (without quotes) in the pop-up box. Click the Install button at the bottom of the pop-up box.\nYou will see a lot of text from status messages appearing in the Console as the packages are being installed. Wait until you see the &gt; again.\nEnter the command library(tidyverse) in the Console and hit Enter.\n\nQuit RStudio. You’re done setting up!\n\n\nOptional: For a refresher on RStudio features, watch this video. It also shows you how to customize the layout and color scheme of RStudio."
  }
]