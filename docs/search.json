[
  {
    "objectID": "01_review164.html",
    "href": "01_review164.html",
    "title": "Review of Data Science 1",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\n\nDeterminants of COVID vaccination rates\nFirst, a little detour to describe several alternatives for reading in data:\nIf you navigate to my Github account, and find the 264_spring_2025 repo, there is a Data folder inside. You can then click on vacc_Mar21.csv to see the data we want to download. This link should also get you there, but it’s good to be able to navigate there yourself.\n\n# Approach 1\n1vaccine_data &lt;- read_csv(\"Data/vaccinations_2021.csv\")\n\n# Approach 2\n2vaccine_data &lt;- read_csv(\"~/264_spring_2025/Data/vaccinations_2021.csv\")\n\n# Approach 3\n3vaccine_data &lt;- read_csv(\"https://joeroith.github.io/264_spring_2025/Data/vaccinations_2021.csv\")\n\n# Approach 4\n4vaccine_data &lt;- read_csv(\"https://raw.githubusercontent.com/joeroith/264_spring_2025/refs/heads/main/Data/vaccinations_2021.csv\")\n\n\n1\n\nApproach 1: create a Data folder in the same location where this .qmd file resides, and then store vaccinations_2021.csv in that Data folder\n\n2\n\nApproach 2: give R the complete path to the location of vaccinations_2021.csv, starting with Home (~)\n\n3\n\nApproach 3: link to our course webpage, and then know we have a Data folder containing all our csvs\n\n4\n\nApproach 4: navigate to the data in GitHub, hit the Raw button, and copy that link\n\n\n\n\nA recent Stat 272 project examined determinants of covid vaccination rates at the county level. Our data set contains 3053 rows (1 for each county in the US) and 14 columns; here is a quick description of the variables we’ll be using:\n\nstate = state the county is located in\ncounty = name of the county\nregion = region the state is located in\nmetro_status = Is the county considered “Metro” or “Non-metro”?\nrural_urban_code = from 1 (most urban) to 9 (most rural)\nperc_complete_vac = percent of county completely vaccinated as of 11/9/21\ntot_pop = total population in the county\nvotes_Trump = number of votes for Trump in the county in 2020\nvotes_Biden = number of votes for Biden in the county in 2020\nperc_Biden = percent of votes for Biden in the county in 2020\ned_somecol_perc = percent with some education beyond high school (but not a Bachelor’s degree)\ned_bachormore_perc = percent with a Bachelor’s degree or more\nunemployment_rate_2020 = county unemployment rate in 2020\nmedian_HHincome_2019 = county’s median household income in 2019\n\n\nConsider only Minnesota and its surrounding states (Iowa, Wisconsin, North Dakota, and South Dakota). We want to examine the relationship between the percentage who voted for Biden and the percentage of complete vaccinations by state. Generate two plots to examine this relationship:\n\n\nA scatterplot with points and smoothers colored by state. Make sure the legend is ordered in a meaningful way, and include good labels on your axes and your legend. Also leave off the error bars from your smoothers.\nOne plot per state containing a scatterplot and a smoother.\n\nDescribe which plot you prefer and why. What can you learn from your preferred plot?\n\nWe wish to compare the proportions of counties in each region with median household income above the national median ($69,560).\n\n\nFill in the blanks below to produce a segmented bar plot with regions ordered from highest proportion above the median to lowest.\nCreate a table of proportions by region to illustrate that your bar plot in (a) is in the correct order (you should find two regions that are really close when you just try to eyeball differences).\nExplain why we can replace fct_relevel(region, FILL IN CODE) with\n\nmutate(region_sort = fct_reorder(region, median_HHincome_2019 &lt; 69560, .fun = mean))\nbut not\nmutate(region_sort = fct_reorder(region, median_HHincome_2019 &lt; 69560))\n\nvaccine_data |&gt;\n  mutate(HHincome_vs_national = ifelse(median_HHincome_2019 &lt; 69560, FILL IN CODE)) |&gt;\n  mutate(region_sort = fct_relevel(region, FILL IN CODE)) |&gt;\n  ggplot(mapping = aes(x = region_sort, fill = HHincome_vs_national)) +\n    geom_bar(position = \"fill\")\n\n\nWe want to examine the distribution of total county populations and then see how it’s related to vaccination rates.\n\n\nCarefully and thoroughly explain why the two histograms below provide different plots.\n\n\nvaccine_data |&gt;\n  mutate(tot_pop_millions = tot_pop / 1000000) |&gt;\n  ggplot(mapping = aes(x = tot_pop_millions)) +\n    geom_histogram(bins = 40) +\n    labs(x = \"Total population in millions\")\n\n\n\n\n\n\n\nvaccine_data |&gt;\n  mutate(tot_pop_millions = tot_pop %/% 1000000) |&gt;\n  ggplot(mapping = aes(x = tot_pop_millions)) +\n    geom_histogram(bins = 40) +\n    labs(x = \"Total population in millions\")\n\n\n\n\n\n\n\n\n\nFind the top 5 counties in terms of total population.\nPlot a histogram of logged population and describe this distribution.\nPlot the relationship between log population and percent vaccinated using separate colors for Metro and Non-metro counties (be sure there’s no 3rd color used for NAs). Reduce the size and transparency of each point to make the plot more readable. Describe what you can learn from this plot.\n\n\nProduce 3 different plots for illustrating the relationship between the rural_urban_code and percent vaccinated. Hint: you can sometimes turn numeric variables into categorical variables for plotting purposes (e.g. as.factor(), ifelse()).\n\nState your favorite plot, why you like it better than the other two, and what you can learn from your favorite plot. Create an alt text description of your favorite plot, using the Four Ingredient Model. See this link for reminders and references about alt text.\n\nBEFORE running the code below, sketch the plot that will be produced by R. AFTER running the code, describe what conclusion(s) can we draw from this plot?\n\n\nvaccine_data |&gt;\n  filter(!is.na(perc_Biden)) |&gt;\n  mutate(big_states = fct_lump(state, n = 10)) |&gt;\n  group_by(big_states) |&gt;\n  summarize(IQR_Biden = IQR(perc_Biden)) |&gt;\n  mutate(big_states = fct_reorder(big_states, IQR_Biden)) |&gt;\n  ggplot() + \n    geom_point(aes(x = IQR_Biden, y = big_states))\n\n\nIn this question we will focus only on the 12 states in the Midwest (i.e. where region == “Midwest”).\n\n\nCreate a tibble with the following information for each state. Order states from least to greatest state population.\n\n\nnumber of different rural_urban_codes represented among the state’s counties (there are 9 possible)\ntotal state population\nproportion of Metro counties\nmedian unemployment rate\n\n\nUse your tibble in (a) to produce a plot of the relationship between proportion of Metro counties and median unemployment rate. Points should be colored by the number of different rural_urban_codes in a state, but a single linear trend should be fit to all points. What can you conclude from the plot?\n\n\nGenerate an appropriate plot to compare vaccination rates between two subregions of the US: New England (which contains the states Maine, Vermont, New Hampshire, Massachusetts, Connecticut, Rhode Island) and the Upper Midwest (which, according to the USGS, contains the states Minnesota, Wisconsin, Michigan, Illinois, Indiana, and Iowa). What can you conclude from your plot?\n\nIn this next section, we consider a few variables that could have been included in our data set, but were NOT. Thus, you won’t be able to write and test code, but you nevertheless should be able to use your knowledge of the tidyverse to answer these questions.\nHere are the hypothetical variables:\n\nHR_party = party of that county’s US Representative (Republican, Democrat, Independent, Green, or Libertarian)\npeople_per_MD = number of residents per doctor (higher values = fewer doctors)\nperc_over_65 = percent of residents over 65 years old\nperc_white = percent of residents who identify as white\n\n\nHypothetical R chunk #1:\n\n\n# Hypothetical R chunk 1\ntemp &lt;- vaccine_data |&gt;\n  mutate(new_perc_vac = ifelse(perc_complete_vac &gt; 95, NA, perc_complete_vac),\n         MD_group = cut_number(people_per_MD, 3)) |&gt;\n  group_by(MD_group) |&gt;\n  summarise(n = n(),\n            mean_perc_vac = mean(new_perc_vac, na.rm = TRUE),\n            mean_white = mean(perc_white, na.rm = TRUE))\n\n\nDescribe the tibble temp created above. What would be the dimensions? What do rows and columns represent?\nWhat would happen if we replaced new_perc_vac = ifelse(perc_complete_vac &gt; 95, NA, perc_complete_vac) with new_perc_vac = ifelse(perc_complete_vac &gt; 95, perc_complete_vac, NA)?\nWhat would happen if we replaced mean_white = mean(perc_white, na.rm = TRUE) with mean_white = mean(perc_white)?\nWhat would happen if we removed group_by(MD_group)?\n\n\nHypothetical R chunk #2:\n\n\n# Hypothetical R chunk 2\nggplot(data = vaccine_data) +\n  geom_point(mapping = aes(x = perc_over_65, y = perc_complete_vac, \n                           color = HR_party)) +\n  geom_smooth()\n\ntemp &lt;- vaccine_data |&gt;\n  group_by(HR_party) |&gt;\n  summarise(var1 = n()) |&gt;\n  arrange(desc(var1)) |&gt;\n  slice_head(n = 3)\n\nvaccine_data |&gt;\n  ggplot(mapping = aes(x = fct_reorder(HR_party, perc_over_65, .fun = median), \n                       y = perc_over_65)) +\n    geom_boxplot()\n\n\nWhy would the first plot produce an error?\nDescribe the tibble temp created above. What would be the dimensions? What do rows and columns represent?\nWhat would happen if we replaced fct_reorder(HR_party, perc_over_65, .fun = median) with HR_party?\n\n\nHypothetical R chunk #3:\n\n\n# Hypothetical R chunk 3\nvaccine_data |&gt;\n  filter(!is.na(people_per_MD)) |&gt;\n  mutate(state_lump = fct_lump(state, n = 4)) |&gt;\n  group_by(state_lump, rural_urban_code) |&gt;\n  summarise(mean_people_per_MD = mean(people_per_MD)) |&gt;\n  ggplot(mapping = aes(x = rural_urban_code, y = mean_people_per_MD, \n      colour = fct_reorder2(state_lump, rural_urban_code, mean_people_per_MD))) +\n    geom_line()\n\n\nDescribe the tibble piped into the ggplot above. What would be the dimensions? What do rows and columns represent?\nCarefully describe the plot created above.\nWhat would happen if we removed filter(!is.na(people_per_MD))?\nWhat would happen if we replaced fct_reorder2(state_lump, rural_urban_code, mean_people_per_MD) with state_lump?",
    "crumbs": [
      "Review of Data Science 1"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SDS 264: Data Science 2 (Spring 2025)",
    "section": "",
    "text": "Quarto\n\n\n\n\n\n\n\ntidyverse\n\n\n\n\n\n\nKey links for SDS 264\n\nCourse syllabus\nRStudio server\nmoodle\nGitHub source code for this website"
  },
  {
    "objectID": "miniproject1.html",
    "href": "miniproject1.html",
    "title": "Mini-Project 1: Maps",
    "section": "",
    "text": "Overview\nYou will produce choropleth maps illustrating two different characteristics – one numeric and one categorical – that have been measured for each US state (you can choose to exclude Alaska and Hawaii), like we did in the “Creating Informative Maps” activity. Just as we found state-level data from both a vaccine data set and the poliscidata package, you should find your own state-level data that is interesting to you.\nA few additional details for this mini-project:\n\nYou should create two versions of each plot – one that is static and one that is interactive.\nBe sure to include a note on your plots with your data source.\nYou should be able to merge your state-level data with the state mapping data sets we used in class.\nBe sure you label your plot well and provide a description of what insights can be gained from each static plot. For one of your static plots, this should be in the form of alt-text, using the “Four Ingredient Model” in the article from class.\nCheck out this rubric for Mini-Project 1.\n\n\n\nSubmission and Timeline\nMini-Project 1 must be submitted on Moodle by 11:00 PM on Fri Feb 21. You should submit your two static plots, along with descriptions and alt-text, in a pdf document. For your interactive plots, just submit a GitHub link where I can see your code that would produce a nice html document. Then, in Mini-Project 2, I will ask you to build a webpage where you will link to these interactive html pages.",
    "crumbs": [
      "Mini-Project 1: Maps"
    ]
  },
  {
    "objectID": "rtipoftheday.html",
    "href": "rtipoftheday.html",
    "title": "R Tip of the Day",
    "section": "",
    "text": "Signup Sheet\nHow to create cool presentations using Quarto\nCode for creating the presentation above\nMy example of an RTD presentation with revealjs in quarto\nCode for my example\nRTD rubric\n\nTIP: Create your presentation in a new project (separate from the one you created for class). You can run the presentation from your own machine OR you can publish it to the RStudio/Posit Connect server. To do this:\n\n\nRender your presentation.\nClick the ‘Publish’ icon in the upper right corner of the screen.\nSelect Posit Connect as the server to publish to.\nChoose to publish the finished product only.\nAt this point if you are doing it for the first time, you will be asked to connect to an account. Choose Posit Connect.\nEnter “https://rconnect.stolaf.edu” as the URL then click next.\nAt this point you will be redirected to a webpage to sign in. Use your stolaf account to log in and follow the instructions to connect your RStudio to the server. Go back to your desktop RStudio and click ‘Connect Account’.\nNow you are ready to publish the document. Give it a good name and click ‘Publish’.\nAfter a minute or two, you will again be redirected to a browser where your document is now available. Edit the Share settings appropriately (Anyone - no login required is fine, but be aware that others will be able to see your work). Copy the Content URL to share.\n\n\n\n\n\n\n\n\n\n\nPublish\n\n\n\n\n\n\n\n\n\nStep 2\n\n\n\n\n\n\n \n\n\n\n\n\n\n\nLast Step"
  },
  {
    "objectID": "02_maps.html",
    "href": "02_maps.html",
    "title": "Creating informative maps",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\n\n# Initial packages required (we'll be adding more)\nlibrary(tidyverse)\nlibrary(mdsr)      # package associated with our MDSR book\n\n\nOpening example\nHere is a simple choropleth map example from Section 3.2.3 of MDSR. Note how we use an underlying map with strategic shading to convey a story about a variable that’s been measured on each country.\n\n# CIACountries is a 236 x 8 data set with information on each country\n#   taken from the CIA factbook - gdp, education, internet use, etc.\nhead(CIACountries)\nCIACountries |&gt;\n  select(country, oil_prod) |&gt;\n  mutate(oil_prod_disc = cut(oil_prod, \n    breaks = c(0, 1e3, 1e5, 1e6, 1e7, 1e8), \n    labels = c(\"&gt;1000\", \"&gt;10,000\", \"&gt;100,000\", \"&gt;1 million\", \n               \"&gt;10 million\"))) |&gt;\n1  mosaic::mWorldMap(key = \"country\") +\n  geom_polygon(aes(fill = oil_prod_disc)) + \n  scale_fill_brewer(\"Oil Prod. (bbl/day)\", na.value = \"white\") +\n  theme(legend.position = \"top\")\n\n\n1\n\nWe won’t use mWorldMap often, but it’s a good quick illustration\n\n\n\n\n\n\n\n\n\n\n\n         country      pop    area oil_prod   gdp educ   roadways net_users\n1    Afghanistan 32564342  652230        0  1900   NA 0.06462444       &gt;5%\n2        Albania  3029278   28748    20510 11900  3.3 0.62613051      &gt;35%\n3        Algeria 39542166 2381741  1420000 14500  4.3 0.04771929      &gt;15%\n4 American Samoa    54343     199        0 13000   NA 1.21105528      &lt;NA&gt;\n5        Andorra    85580     468       NA 37200   NA 0.68376068      &gt;60%\n6         Angola 19625353 1246700  1742000  7300  3.5 0.04125211      &gt;15%\n\n\n\n\nChoropleth Maps\nWhen you have specific regions (e.g. countries, states, counties, census tracts,…) and a value associated with each region.\nA choropleth map will color the entire region according to the value. For example, let’s consider state vaccination data from March 2021.\n\nvaccines &lt;- read_csv(\"https://joeroith.github.io/264_spring_2025/Data/vacc_Mar21.csv\") \n\nvacc_mar13 &lt;- vaccines |&gt;\n  filter(Date ==\"2021-03-13\") |&gt;\n  select(State, Date, people_vaccinated_per100, share_doses_used, Governor)\n\nvacc_mar13\n\n# A tibble: 50 × 5\n   State       Date       people_vaccinated_per100 share_doses_used Governor\n   &lt;chr&gt;       &lt;date&gt;                        &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;   \n 1 Alabama     2021-03-13                     17.2            0.671 R       \n 2 Alaska      2021-03-13                     27.0            0.686 R       \n 3 Arizona     2021-03-13                     21.5            0.821 R       \n 4 Arkansas    2021-03-13                     19.2            0.705 R       \n 5 California  2021-03-13                     20.3            0.726 D       \n 6 Colorado    2021-03-13                     20.8            0.801 D       \n 7 Connecticut 2021-03-13                     26.2            0.851 D       \n 8 Delaware    2021-03-13                     20.2            0.753 D       \n 9 Florida     2021-03-13                     20.1            0.766 R       \n10 Georgia     2021-03-13                     15.2            0.674 R       \n# ℹ 40 more rows\n\n\nThe tricky part of choropleth maps is getting the shapes (polygons) that make up the regions. This is really a pretty complex set of lines for R to draw!\nLuckily, some maps are already created in R in the maps package.\n\nlibrary(maps)\nus_states &lt;- map_data(\"state\")\nhead(us_states)\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\nus_states |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(fill = \"white\", color = \"black\")\n\n\n\n\n\n\n\n\n[Pause to ponder:] What might the group and order columns represent?\nOther maps provided by the maps package include US counties, France, Italy, New Zealand, and two different views of the world. If you want maps of other countries or regions, you can often find them online.\nWhere the really cool stuff happens is when we join our data to the us_states dataframe. Notice that the state name appears in the “region” column of us_states, and that the state name is in all small letters. In vacc_mar13, the state name appears in the State column and is in lower case. Thus, we have to be very careful when we join the state vaccine info to the state geography data.\nRun this line by line to see what it does:\n\nvacc_mar13 &lt;- vacc_mar13 |&gt;\n  mutate(State = str_to_lower(State))\n\nvacc_mar13 |&gt;\n  right_join(us_states, by = c(\"State\" = \"region\")) |&gt;\n  rename(region = State) |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(aes(fill = people_vaccinated_per100), color = \"black\")\n\n\n\n\n\n\n\n\noops, New York appears to be a problem.\n\nvacc_mar13 |&gt;\n  anti_join(us_states, by = c(\"State\" = \"region\"))\n\n# A tibble: 3 × 5\n  State          Date       people_vaccinated_per100 share_doses_used Governor\n  &lt;chr&gt;          &lt;date&gt;                        &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;   \n1 alaska         2021-03-13                     27.0            0.686 R       \n2 hawaii         2021-03-13                     22.8            0.759 D       \n3 new york state 2021-03-13                     21.7            0.764 D       \n\nus_states |&gt;\n  anti_join(vacc_mar13, by = c(\"region\" = \"State\")) |&gt;\n  count(region)\n\n                region   n\n1 district of columbia  10\n2             new york 495\n\n\n[Pause to ponder:] What did we learn by running anti_join() above?\nNotice that the us_states map also includes only the contiguous 48 states. This gives an example of creating really beautiful map insets for Alaska and Hawaii.\n\nvacc_mar13 &lt;- vacc_mar13 |&gt;\n  mutate(State = str_replace(State, \" state\", \"\"))\n\nvacc_mar13 |&gt;\n  anti_join(us_states, by = c(\"State\" = \"region\"))\n\n# A tibble: 2 × 5\n  State  Date       people_vaccinated_per100 share_doses_used Governor\n  &lt;chr&gt;  &lt;date&gt;                        &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;   \n1 alaska 2021-03-13                     27.0            0.686 R       \n2 hawaii 2021-03-13                     22.8            0.759 D       \n\nus_states |&gt;\n  anti_join(vacc_mar13, by = c(\"region\" = \"State\")) %&gt;%\n  count(region)\n\n                region  n\n1 district of columbia 10\n\n\nBetter.\n\nlibrary(viridis) # for color schemes\nvacc_mar13 |&gt;\n  right_join(us_states, by = c(\"State\" = \"region\")) |&gt;\n  rename(region = State) |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(aes(fill = people_vaccinated_per100), color = \"black\") + \n  labs(fill = \"People Vaccinated\\nper 100 pop.\") +\n1  coord_map() +\n2  theme_void() +\n3  scale_fill_viridis()\n\n\n1\n\nThis scales the longitude and latitude so that the shapes look correct. coord_quickmap() can also work here - it’s less exact but faster.\n\n2\n\nThis theme can give you a really clean look\n\n3\n\nYou can change the fill scale for different color schemes.\n\n\n\n\n\n\n\n\n\n\n\nYou can also use a categorical variable to color regions:\n\nvacc_mar13 |&gt;\n  right_join(us_states, by = c(\"State\" = \"region\")) |&gt;\n  rename(region = State) |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(aes(fill = Governor), color = \"darkgrey\", linewidth = 0.2) + \n  labs(fill = \"Governor\") +\n  coord_map() + \n  theme_void() +  \n1  scale_fill_manual(values = c(\"blue\", \"red\"))\n\n\n1\n\nYou can change the fill scale for different color schemes.\n\n\n\n\n\n\n\n\n\n\n\nNote: Map projections are actually pretty complicated, especially if you’re looking at large areas (e.g. world maps) or drilling down to very small regions where a few feet can make a difference (e.g. tracking a car on a map of roads). It’s impossible to preserve both shape and area when projecting an (imperfect) sphere onto a flat surface, so that’s why you sometimes see such different maps of the world. This is why packages like maps which connect latitude-longitude points are being phased out in favor of packages like sf with more GIS functionality. We won’t get too deep into GIS in this class, but to learn more, take Spatial Data Analysis!!\n\n\nMultiple maps!\nYou can still use data viz tools from Data Science 1 (like faceting) to create things like time trends in maps:\n\nlibrary(lubridate)\nweekly_vacc &lt;- vaccines |&gt;\n  mutate(State = str_to_lower(State)) |&gt;\n  mutate(State = str_replace(State, \" state\", \"\"),\n         week = week(Date)) |&gt;\n  group_by(week, State) |&gt;\n  summarize(date = first(Date),\n            mean_daily_vacc = mean(daily_vaccinated/est_population*1000)) |&gt;\n  right_join(us_states, by =c(\"State\" = \"region\")) |&gt;\n  rename(region = State)\n\nweekly_vacc |&gt;\n  filter(week &gt; 2, week &lt; 11) |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(aes(fill = mean_daily_vacc), color = \"darkgrey\", \n               linewidth = 0.1) + \n  labs(fill = \"Weekly Average Daily Vaccinations per 1000\") +\n  coord_map() + \n  theme_void() + \n  scale_fill_viridis() + \n  facet_wrap(~date) + \n  theme(legend.position = \"bottom\") \n\n\n\n\n\n\n\n\n[Pause to ponder:] are we bothered by the warning about many-to-many when you run the code above?\n\n\nOther cool state maps\n\nstatebin (square representation of states)\n\nlibrary(statebins) # may need to install\n\nvacc_mar13 |&gt;\n  mutate(State = str_to_title(State)) |&gt;\n  statebins(state_col = \"State\",\n            value_col = \"people_vaccinated_per100\") + \n1  theme_statebins() +\n  labs(fill = \"People Vaccinated per 100\")\n\n\n1\n\nOne nice layout. You can customize with usual ggplot themes.\n\n\n\n\n\n\n\n\n\n\n\n[Pause to ponder:] Why might one use a map like above instead of our previous choropleth maps?\nI used this example to create the code above. The original graph is located here.\n\n\n\nInteractive point maps with leaflet\nTo add even more power and value to your plots, we can add interactivity. For now, we will use the leaflet package, but later in the course we will learn even more powerful and flexible approaches for creating interactive plots and webpages.\nFor instance, here is a really simple plot with a pop-up window:\n\nlibrary(leaflet)\n\nleaflet() |&gt; \n1  addTiles() |&gt;\n2  setView(-93.1832, 44.4597, zoom = 17) |&gt;\n3  addPopups(-93.1832, 44.4597, 'Here is the &lt;b&gt;Regents Hall of Mathematical Sciences&lt;/b&gt;, home of the Statistics and Data Science program at St. Olaf College')\n\n\n1\n\naddTiles() uses OpenStreetMap, an awesome open-source mapping resource, as the default tile layer (background map)\n\n2\n\nsetView() centers the map at a specific latitude and longitude, then zoom controls how much of the surrounding area is shown\n\n3\n\nadd a popup message (with html formatting) that can be clicked on or off\n\n\n\n\n\n\n\n\nLeaflet is not part of the tidyverse, but the structure of its code is pretty similar and it also plays well with piping.\nLet’s try pop-up messages with a data set containing Airbnb listings in the Boston area:\n\nleaflet() |&gt;\n    addTiles() |&gt;\n    setView(lng = mean(airbnb.df$Long), lat = mean(airbnb.df$Lat), \n            zoom = 13) |&gt; \n    addCircleMarkers(data = airbnb.df,\n        lat = ~ Lat, \n        lng = ~ Long, \n        popup = ~ AboutListing, \n        radius = ~ S_Accomodates,  \n        # These last options describe how the circles look\n        weight = 2,\n        color = \"red\", \n        fillColor = \"yellow\")\n\n\n\n\n\n[Pause to ponder:] List similarities and differences between leaflet plots and ggplots.\n\n\nInteractive choropleth maps with leaflet\nOK. Now let’s see if we can put things together and duplicate the interactive choropleth map found here showing population density by state in the US.\n\nA preview to shapefiles and the sf package\n\n1library(sf)\n2states &lt;- read_sf(\"https://rstudio.github.io/leaflet/json/us-states.geojson\")\n3class(states)\nstates\n\n\n1\n\nsf stands for “simple features”\n\n2\n\nFrom https://leafletjs.com/examples/choropleth/us-states.js\n\n3\n\nNote that states has class sf in addition to the usual tbl and df\n\n\n\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\nSimple feature collection with 52 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -188.9049 ymin: 17.92956 xmax: -65.6268 ymax: 71.35163\nGeodetic CRS:  WGS 84\n# A tibble: 52 × 4\n   id    name                  density                                  geometry\n   &lt;chr&gt; &lt;chr&gt;                   &lt;dbl&gt;                        &lt;MULTIPOLYGON [°]&gt;\n 1 01    Alabama                 94.6  (((-87.3593 35.00118, -85.60667 34.98475…\n 2 02    Alaska                   1.26 (((-131.602 55.11798, -131.5692 55.28229…\n 3 04    Arizona                 57.0  (((-109.0425 37.00026, -109.048 31.33163…\n 4 05    Arkansas                56.4  (((-94.47384 36.50186, -90.15254 36.4963…\n 5 06    California             242.   (((-123.2333 42.00619, -122.3789 42.0116…\n 6 08    Colorado                49.3  (((-107.9197 41.00391, -105.729 40.99843…\n 7 09    Connecticut            739.   (((-73.05353 42.03905, -71.79931 42.0226…\n 8 10    Delaware               464.   (((-75.41409 39.80446, -75.5072 39.68396…\n 9 11    District of Columbia 10065    (((-77.03526 38.99387, -76.90929 38.8952…\n10 12    Florida                353.   (((-85.49714 30.99754, -85.00421 31.0030…\n# ℹ 42 more rows\n\n\nFor maps in leaflet that show boundaries and not just points, we need to input a shapefile rather than a series of latitude-longitude combinations like we did for the maps package. In the example we’re emulating, they use the read_sf() function from the sf package to read in data. While our us_states data frame from the maps package contained 15537 rows, our simple features object states contains only 52 rows - one per state. Importantly, states contains a column called geometry, which is a “multipolygon” with all the information necessary to draw a specific state. Also, while states can be treated as a tibble or data frame, it is also an sf class object with a specific “geodetic coordinate reference system”. Again, take Spatial Data Analysis for more on shapefiles and simple features!\nNote also that the authors of this example have already merged state population densities with state geometries, but if we wanted to merge in other state characteristics using the name column as a key, we could definitely do this!\nFirst we’ll start with a static plot using a simple features object and geom_sf():\n\n# Create density bins as on the webpage\nstate_plotting_sf &lt;- states |&gt;\n  mutate(density_intervals = cut(density, n = 8,\n          breaks = c(0, 10, 20, 50, 100, 200, 500, 1000, Inf))) |&gt;\n  filter(!(name %in% c(\"Alaska\", \"Hawaii\", \"Puerto Rico\")))\n\nggplot(data = state_plotting_sf) + \n  geom_sf(aes(fill = density_intervals), colour = \"white\", linetype = 2) + \n#  geom_sf_label(aes(label = density)) +   # labels too busy here\n  theme_void() +  \n  scale_fill_brewer(palette = \"YlOrRd\") \n\n\n\n\n\n\n\n\nNow let’s use leaflet to create an interactive plot!\n\n# Create our own category bins for population densities\n#   and assign the yellow-orange-red color palette\nbins &lt;- c(0, 10, 20, 50, 100, 200, 500, 1000, Inf)\npal &lt;- colorBin(\"YlOrRd\", domain = states$density, bins = bins)\n\n# Create labels that pop up when we hover over a state.  The labels must\n#   be part of a list where each entry is tagged as HTML code.\nlibrary(htmltools)\nlibrary(glue)\n\nstates &lt;- states |&gt;\n  mutate(labels = str_c(name, \": \", density, \" people / sq mile\"))\n\n# If want more HTML formatting, use these lines instead of those above:\n#states &lt;- states |&gt;\n#  mutate(labels = glue(\"&lt;strong&gt;{name}&lt;/strong&gt;&lt;br/&gt;{density} people / #mi&lt;sup&gt;2&lt;/sup&gt;\"))\n\nlabels &lt;- lapply(states$labels, HTML)\n\nleaflet(states) |&gt;\n  setView(-96, 37.8, 4) |&gt;\n  addTiles() |&gt;\n  addPolygons(\n    fillColor = ~pal(density),\n    weight = 2,\n    opacity = 1,\n    color = \"white\",\n    dashArray = \"3\",\n    fillOpacity = 0.7,\n    highlightOptions = highlightOptions(\n      weight = 5,\n      color = \"#666\",\n      dashArray = \"\",\n      fillOpacity = 0.7,\n      bringToFront = TRUE),\n    label = labels,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n      textsize = \"15px\",\n      direction = \"auto\")) |&gt;\n  addLegend(pal = pal, values = ~density, opacity = 0.7, title = NULL,\n    position = \"bottomright\")\n\n\n\n\n\n[Pause to ponder:] Pick several formatting options in the code above, determine what they do, and then change them to create a customized look.\n\n\n\nOn Your Own\nThe states dataset in the poliscidata package contains 135 variables on each of the 50 US states. See here for more detail.\nYour task is to create a two meaningful choropleth plots, one using a numeric variable and one using a categorical variable from poliscidata::states. You should make two versions of each plot: a static plot using the maps package and ggplot(), and an interactive plot using the sf package and leaflet(). Write a sentence or two describing what you can learn from each plot.\nHere’s some R code and hints to get you going:\n\n# Get info to draw US states for geom_polygon (connect the lat-long points)\nlibrary(maps)\nstates_polygon &lt;- as_tibble(map_data(\"state\")) |&gt;\n  select(region, group, order, lat, long)\n\n# See what the state (region) levels look like in states_polygon\nunique(states_polygon$region)\n\n [1] \"alabama\"              \"arizona\"              \"arkansas\"            \n [4] \"california\"           \"colorado\"             \"connecticut\"         \n [7] \"delaware\"             \"district of columbia\" \"florida\"             \n[10] \"georgia\"              \"idaho\"                \"illinois\"            \n[13] \"indiana\"              \"iowa\"                 \"kansas\"              \n[16] \"kentucky\"             \"louisiana\"            \"maine\"               \n[19] \"maryland\"             \"massachusetts\"        \"michigan\"            \n[22] \"minnesota\"            \"mississippi\"          \"missouri\"            \n[25] \"montana\"              \"nebraska\"             \"nevada\"              \n[28] \"new hampshire\"        \"new jersey\"           \"new mexico\"          \n[31] \"new york\"             \"north carolina\"       \"north dakota\"        \n[34] \"ohio\"                 \"oklahoma\"             \"oregon\"              \n[37] \"pennsylvania\"         \"rhode island\"         \"south carolina\"      \n[40] \"south dakota\"         \"tennessee\"            \"texas\"               \n[43] \"utah\"                 \"vermont\"              \"virginia\"            \n[46] \"washington\"           \"west virginia\"        \"wisconsin\"           \n[49] \"wyoming\"             \n\n# Get info to draw US states for geom_sf and leaflet (simple features object \n#   with multipolygon geometry column)\nlibrary(sf)\nstates_sf &lt;- read_sf(\"https://rstudio.github.io/leaflet/json/us-states.geojson\") |&gt;\n  select(name, geometry)\n\n# See what the state (name) levels look like in states_sf\nunique(states_sf$name)\n\n [1] \"Alabama\"              \"Alaska\"               \"Arizona\"             \n [4] \"Arkansas\"             \"California\"           \"Colorado\"            \n [7] \"Connecticut\"          \"Delaware\"             \"District of Columbia\"\n[10] \"Florida\"              \"Georgia\"              \"Hawaii\"              \n[13] \"Idaho\"                \"Illinois\"             \"Indiana\"             \n[16] \"Iowa\"                 \"Kansas\"               \"Kentucky\"            \n[19] \"Louisiana\"            \"Maine\"                \"Maryland\"            \n[22] \"Massachusetts\"        \"Michigan\"             \"Minnesota\"           \n[25] \"Mississippi\"          \"Missouri\"             \"Montana\"             \n[28] \"Nebraska\"             \"Nevada\"               \"New Hampshire\"       \n[31] \"New Jersey\"           \"New Mexico\"           \"New York\"            \n[34] \"North Carolina\"       \"North Dakota\"         \"Ohio\"                \n[37] \"Oklahoma\"             \"Oregon\"               \"Pennsylvania\"        \n[40] \"Rhode Island\"         \"South Carolina\"       \"South Dakota\"        \n[43] \"Tennessee\"            \"Texas\"                \"Utah\"                \n[46] \"Vermont\"              \"Virginia\"             \"Washington\"          \n[49] \"West Virginia\"        \"Wisconsin\"            \"Wyoming\"             \n[52] \"Puerto Rico\"         \n\n# Load in state-wise data for filling our choropleth maps\n#   (Note that I selected my two variables of interest to simplify)\nlibrary(poliscidata)   # may have to install first\npolisci_data &lt;- as_tibble(poliscidata::states) |&gt;\n  select(state, carfatal07, cook_index3)\n\n# See what the state (state) levels look like in polisci_data\nunique(polisci_data$state)   # can't see trailing spaces but can see\n\n [1] Alaska                                    \n [2] Alabama                                   \n [3] Arkansas                                  \n [4] Arizona                                   \n [5] California                                \n [6] Colorado                                  \n [7] Connecticut                               \n [8] Delaware                                  \n [9] Florida                                   \n[10] Georgia                                   \n[11] Hawaii                                    \n[12] Iowa                                      \n[13] Idaho                                     \n[14] Illinois                                  \n[15] Indiana                                   \n[16] Kansas                                    \n[17] Kentucky                                  \n[18] Louisiana                                 \n[19] Massachusetts                             \n[20] Maryland                                  \n[21] Maine                                     \n[22] Michigan                                  \n[23] Minnesota                                 \n[24] Missouri                                  \n[25] Mississippi                               \n[26] Montana                                   \n[27] NorthCarolina                             \n[28] NorthDakota                               \n[29] Nebraska                                  \n[30] NewHampshire                              \n[31] NewJersey                                 \n[32] NewMexico                                 \n[33] Nevada                                    \n[34] NewYork                                   \n[35] Ohio                                      \n[36] Oklahoma                                  \n[37] Oregon                                    \n[38] Pennsylvania                              \n[39] RhodeIsland                               \n[40] SouthCarolina                             \n[41] SouthDakota                               \n[42] Tennessee                                 \n[43] Texas                                     \n[44] Utah                                      \n[45] Virginia                                  \n[46] Vermont                                   \n[47] Washington                                \n[48] Wisconsin                                 \n[49] WestVirginia                              \n[50] Wyoming                                   \n50 Levels: Alabama                                    ...\n\n                             #   lack of internal spaces\nprint(polisci_data)   # can see trailing spaces\n\n# A tibble: 50 × 3\n   state                                        carfatal07 cook_index3\n   &lt;fct&gt;                                             &lt;dbl&gt; &lt;fct&gt;      \n 1 \"Alaska                                    \"       15.2 More Rep   \n 2 \"Alabama                                   \"       25.9 More Rep   \n 3 \"Arkansas                                  \"       23.7 More Rep   \n 4 \"Arizona                                   \"       17.6 Even       \n 5 \"California                                \"       11.7 More Dem   \n 6 \"Colorado                                  \"       12.3 Even       \n 7 \"Connecticut                               \"        8.7 More Dem   \n 8 \"Delaware                                  \"       13.6 More Dem   \n 9 \"Florida                                   \"       18.1 Even       \n10 \"Georgia                                   \"       18.5 Even       \n# ℹ 40 more rows\n\n\nR code hints:\n\nstringr functions like str_squish and str_to_lower and str_replace_all (be sure to carefully look at your keys!)\n*_join functions (make sure they preserve classes)\nfilter so that you only have 48 contiguous states (and maybe DC)\nfor help with colors: https://rstudio.github.io/leaflet/reference/colorNumeric.html\nbe sure labels pop up when scrolling with leaflet\n\n\n# Make sure all keys have the same format before joining:\n#   all lower case, no internal or external spaces\n\n\n# Now we can merge data sets together for the static and the interactive plots\n\n\n# Merge with states_polygon (static)\n\n# Check that merge worked for 48 contiguous states\n\n\n# Merge with states_sf (static or interactive)\n\n# Check that merge worked for 48 contiguous states\n\nNumeric variable (static plot):\nNumeric variable (interactive plot):\n\n# it's okay to skip a legend here\n\nCategorical variable (static plot):\n\n# be really careful with matching color order to factor level order\n\nCategorical variable (interactive plot):\n\n# may use colorFactor() here",
    "crumbs": [
      "Creating informative maps"
    ]
  },
  {
    "objectID": "06_data_types.html",
    "href": "06_data_types.html",
    "title": "Data types",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\nThis leans on parts of R4DS Chapter 27: A field guide to base R, in addition to parts of the first edition of R4DS.\n# Initial packages required\nlibrary(tidyverse)",
    "crumbs": [
      "Data types"
    ]
  },
  {
    "objectID": "06_data_types.html#what-is-a-vector",
    "href": "06_data_types.html#what-is-a-vector",
    "title": "Data types",
    "section": "What is a vector?",
    "text": "What is a vector?\nWe’ve seen them:\n\n1:5 \n\n[1] 1 2 3 4 5\n\nc(3, 6, 1, 7)\n\n[1] 3 6 1 7\n\nc(\"a\", \"b\", \"c\")\n\n[1] \"a\" \"b\" \"c\"\n\nx &lt;- c(0:3, NA)\nis.na(x)\n\n[1] FALSE FALSE FALSE FALSE  TRUE\n\nsqrt(x)\n\n[1] 0.000000 1.000000 1.414214 1.732051       NA\n\n\nThis doesn’t really fit the mathematical definition of a vector (direction and magnitude)… it’s really just some numbers (or letters, or TRUE’s…) strung together.",
    "crumbs": [
      "Data types"
    ]
  },
  {
    "objectID": "06_data_types.html#types-of-vectors",
    "href": "06_data_types.html#types-of-vectors",
    "title": "Data types",
    "section": "Types of vectors",
    "text": "Types of vectors\nAtomic vectors are homogeneous… they can contain only one “type”. Types include logical, integer, double, and character (Also complex and raw, but we will ignore those).\nLists can be heterogeneous…. they can be made up of vectors of different types, or even of other lists!\nNULL denotes the absence of a vector (whereas NA denotes absence of a value in a vector).\nLet’s check out some vector types:\n\nx &lt;- c(0:3, NA)\ntypeof(x)\n\n[1] \"integer\"\n\nsqrt(x)\n\n[1] 0.000000 1.000000 1.414214 1.732051       NA\n\ntypeof(sqrt(x))\n\n[1] \"double\"\n\n\n[Pause to Ponder:] State the types of the following vectors, then use typeof() to check:\n\nis.na(x)\n\n[1] FALSE FALSE FALSE FALSE  TRUE\n\nx &gt; 2\n\n[1] FALSE FALSE FALSE  TRUE    NA\n\nc(\"apple\", \"banana\", \"pear\")\n\n[1] \"apple\"  \"banana\" \"pear\"  \n\n\nA logical vector can be implicitly coerced to numeric - T to 1 and F to 0\n\nx &lt;- sample(1:20, 100, replace = TRUE)\ny &lt;- x &gt; 10\nis_logical(y)\n\n[1] TRUE\n\nas.numeric(y)\n\n  [1] 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1\n [38] 0 0 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1\n [75] 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0\n\nsum(y)  # how many are greater than 10?\n\n[1] 44\n\nmean(y) # what proportion are greater than 10?\n\n[1] 0.44\n\n\nIf there are multiple data types in a vector, then the most complex type wins, because you cannot mix types in a vector (although you can in a list)\n\ntypeof(c(TRUE, 1L))\n\n[1] \"integer\"\n\ntypeof(c(1L, 1.5))\n\n[1] \"double\"\n\ntypeof(c(1.5, \"a\"))\n\n[1] \"character\"\n\n\nIntegers are whole numbers. “double” refers to “Double-precision” representation of fractional values… don’t worry about the details here (Google it if you care), but just recognize that computers have to round at some point. “Double-precision” tries to store numbers precisely and efficiently.\nBut weird stuff can happen:\n\ny &lt;- sqrt(2) ^2\ny\n\n[1] 2\n\ny == 2\n\n[1] FALSE\n\n\nthe function near is better here:\n\nnear(y, 2)\n\n[1] TRUE\n\n\nAnd doubles have a couple extra possible values: Inf, -Inf, and NaN, in addition to NA:\n\n1/0\n\n[1] Inf\n\n-1/0\n\n[1] -Inf\n\n0/0\n\n[1] NaN\n\nInf*0\n\n[1] NaN\n\nInf/Inf\n\n[1] NaN\n\nInf/NA\n\n[1] NA\n\nInf*NA\n\n[1] NA\n\n\nIt’s not a good idea to check for special values (NA, NaN, Inf, -Inf) with ==. Use these instead:\n\nis.finite(Inf)\n\n[1] FALSE\n\nis.infinite(Inf)\n\n[1] TRUE\n\nis.finite(NA)\n\n[1] FALSE\n\nis.finite(NaN)\n\n[1] FALSE\n\nis.infinite(NA)\n\n[1] FALSE\n\nis.infinite(NaN)\n\n[1] FALSE\n\nis.na(NA)\n\n[1] TRUE\n\nis.na(NaN)\n\n[1] TRUE\n\nis.nan(NA)\n\n[1] FALSE\n\nis.nan(NaN)\n\n[1] TRUE\n\nis.na(Inf)\n\n[1] FALSE\n\nis.nan(Inf)\n\n[1] FALSE\n\n\nWhy not use == ?\n\n# Sometimes it works how you think it would:\n1/0\n\n[1] Inf\n\n1/0 == Inf\n\n[1] TRUE\n\n# Sometimes it doesn't (Because NA is contagious!)\n0/0\n\n[1] NaN\n\n0/0 == NaN    \n\n[1] NA\n\nNA == NA \n\n[1] NA\n\nx &lt;- c(0, 1, 1/0, 0/0)\n# Doesn't work well\nx == NA\n\n[1] NA NA NA NA\n\nx == Inf\n\n[1] FALSE FALSE  TRUE    NA\n\n# Works better\nis.na(x)\n\n[1] FALSE FALSE FALSE  TRUE\n\nis.infinite(x)\n\n[1] FALSE FALSE  TRUE FALSE\n\n\nAnother note: technically, each type of vector has its own type of NA… this usually doesn’t matter, but is good to know in case one day you get very very strange errors.",
    "crumbs": [
      "Data types"
    ]
  },
  {
    "objectID": "06_data_types.html#augmented-vectors",
    "href": "06_data_types.html#augmented-vectors",
    "title": "Data types",
    "section": "Augmented vectors",
    "text": "Augmented vectors\nVectors may carry additional metadata in the form of attributes which create augmented vectors.\n\nFactors are built on top of integer vectors\nDates and date-times are built on top of numeric (either integer or double) vectors\nData frames and tibbles are built on top of lists",
    "crumbs": [
      "Data types"
    ]
  },
  {
    "objectID": "06_data_types.html#naming-items-in-vectors",
    "href": "06_data_types.html#naming-items-in-vectors",
    "title": "Data types",
    "section": "Naming items in vectors",
    "text": "Naming items in vectors\nEach element of a vector can be named, either when it is created or with setnames from package purrr.\n\nx &lt;- c(a = 1, b = 2, c = 3)\nx\n\na b c \n1 2 3 \n\n\nThis is more commonly used when you’re dealing with lists or tibbles (which are just a special kind of list!)\n\ntibble(x = 1:4, y = 5:8)\n\n# A tibble: 4 × 2\n      x     y\n  &lt;int&gt; &lt;int&gt;\n1     1     5\n2     2     6\n3     3     7\n4     4     8",
    "crumbs": [
      "Data types"
    ]
  },
  {
    "objectID": "06_data_types.html#subsetting-vectors",
    "href": "06_data_types.html#subsetting-vectors",
    "title": "Data types",
    "section": "Subsetting vectors",
    "text": "Subsetting vectors\nSo many ways to do this.\nI. Subset with numbers.\nUse positive integers to keep elements at those positions:\n\nx &lt;- c(\"one\", \"two\", \"three\", \"four\", \"five\")\nx[1]\n\n[1] \"one\"\n\nx[4]\n\n[1] \"four\"\n\nx[1:2]\n\n[1] \"one\" \"two\"\n\n\n[Pause to Ponder:] How would you extract values 1 and 3?\nYou can also repeat values:\n\nx[c(1, 1, 3, 3, 5, 5, 2, 2, 4, 4, 4)]\n\n [1] \"one\"   \"one\"   \"three\" \"three\" \"five\"  \"five\"  \"two\"   \"two\"   \"four\" \n[10] \"four\"  \"four\" \n\n\nUse negative integers to drop elements:\n\nx[-3]\n\n[1] \"one\"  \"two\"  \"four\" \"five\"\n\n\n[Pause to Ponder:] How would you drop values 2 and 4?\nWhat happens if you mix positive and negative values?\n\nx[c(1, -1)]\n\nError in x[c(1, -1)]: only 0's may be mixed with negative subscripts\n\n\nYou can just subset with 0… this isn’t usually helpful, except perhaps for testing weird cases when you write functions:\n\nx[0]\n\ncharacter(0)\n\n\n\nSubset with a logical vector (“Logical subsetting”).\n\n\nx == \"one\"\n\n[1]  TRUE FALSE FALSE FALSE FALSE\n\nx[x == \"one\"]\n\n[1] \"one\"\n\ny &lt;- c(10, 3, NA, 5, 8, 1, NA)\nis.na(y)\n\n[1] FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE\n\ny[!is.na(y)]\n\n[1] 10  3  5  8  1\n\n\n[Pause to Ponder:] Extract values of y that are less than or equal to 5 (what happens to NAs?). Then extract all non-missing values of y that are less than or equal to 5\n\nIf named, subset with a character vector.\n\n\nz &lt;- c(abc = 1, def = 2, xyz = 3)\nz[\"abc\"]\n\nabc \n  1 \n\n# A slightly more useful example:\nsummary(y)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n    1.0     3.0     5.0     5.4     8.0    10.0       2 \n\nsummary(y)[\"Min.\"]\n\nMin. \n   1 \n\n\n[Pause to Ponder:] Extract abc and xyz from the vector z, and then extract the mean from summary(y)\nNote: Using $ is just for lists (and tibbles, since tibbles are lists)! Not atomic vectors!\n\nz$abc\n\nError in z$abc: $ operator is invalid for atomic vectors\n\n\n\nBlank space. (Don’t subset).\n\n\nx\n\n[1] \"one\"   \"two\"   \"three\" \"four\"  \"five\" \n\nx[]\n\n[1] \"one\"   \"two\"   \"three\" \"four\"  \"five\" \n\n\nThis seems kind of silly. But blank is useful for higher-dimensional objects… like a matrix, or data frame. But our book doesn’t use matrices, so this may be the last one you see this semester:\n\nz &lt;- matrix(1:8, nrow= 2)\nz\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    3    5    7\n[2,]    2    4    6    8\n\nz[1, ]\n\n[1] 1 3 5 7\n\nz[, 1]\n\n[1] 1 2\n\nz[, -3]\n\n     [,1] [,2] [,3]\n[1,]    1    3    7\n[2,]    2    4    8\n\n\nWe could use this with tibbles too, but it is generally better to use the column names (more readable, and less likely to get the wrong columns by accident), and you should probably use select, filter, or slice:\n\nmpg[, 1:2]\n\n# A tibble: 234 × 2\n   manufacturer model     \n   &lt;chr&gt;        &lt;chr&gt;     \n 1 audi         a4        \n 2 audi         a4        \n 3 audi         a4        \n 4 audi         a4        \n 5 audi         a4        \n 6 audi         a4        \n 7 audi         a4        \n 8 audi         a4 quattro\n 9 audi         a4 quattro\n10 audi         a4 quattro\n# ℹ 224 more rows\n\nmpg[1:3, ]\n\n# A tibble: 3 × 11\n  manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class \n  &lt;chr&gt;        &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; \n1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa…\n2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa…\n3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa…",
    "crumbs": [
      "Data types"
    ]
  },
  {
    "objectID": "06_data_types.html#recycling",
    "href": "06_data_types.html#recycling",
    "title": "Data types",
    "section": "Recycling",
    "text": "Recycling\nWhat does R do with vectors:\n\n1:5 + 1:5\n\n[1]  2  4  6  8 10\n\n1:5 * 1:5\n\n[1]  1  4  9 16 25\n\n1:5 + 2\n\n[1] 3 4 5 6 7\n\n1:5 * 2\n\n[1]  2  4  6  8 10\n\n\nThis last two lines makes sense… but R is doing something important here, called recycling. In other words, it is really doing this:\n\n1:5 * c(2, 2, 2, 2, 2)\n\n[1]  2  4  6  8 10\n\n\nYou never need to do this explicit iteration! (This is different from some other more general purpose computing languages…. R was built for analyzing data, so this type of behavior is really desirable!)\nR can recycle longer vectors too, and only warns you if lengths are not multiples of each other:\n\n1:10 + 1:2\n\n [1]  2  4  4  6  6  8  8 10 10 12\n\n1:10 + 1:3\n\nWarning in 1:10 + 1:3: longer object length is not a multiple of shorter object\nlength\n\n\n [1]  2  4  6  5  7  9  8 10 12 11\n\n\nHowever, functions within the tidyverse will not allow you to recycle anything other than scalars (math word for single number… in R, a vector of length 1).\n\n#OK:\ntibble(x = 1:4, y = 1)\n\n# A tibble: 4 × 2\n      x     y\n  &lt;int&gt; &lt;dbl&gt;\n1     1     1\n2     2     1\n3     3     1\n4     4     1\n\n#not OK:\ntibble(x = 1:4, y = 1:2)\n\nError in `tibble()`:\n! Tibble columns must have compatible sizes.\n• Size 4: Existing data.\n• Size 2: Column `y`.\nℹ Only values of size one are recycled.\n\n\nTo intentionally recycle, use rep:\n\nrep(1:3, times = 2)\n\n[1] 1 2 3 1 2 3\n\nrep(1:3, each = 2)\n\n[1] 1 1 2 2 3 3",
    "crumbs": [
      "Data types"
    ]
  },
  {
    "objectID": "06_data_types.html#lists",
    "href": "06_data_types.html#lists",
    "title": "Data types",
    "section": "Lists",
    "text": "Lists\nLists can contain a mix of objects, even other lists.\nAs noted previously, tibbles are an augmented list. Augmented lists have additional attributes. For example, the names of the columns in a tibble.\nAnother list you may have encountered in a stats class is output from lm, linear regression:\n\nmpg_model &lt;- lm(hwy ~ cty, data = mpg)\n\nmpg_model\n\n\nCall:\nlm(formula = hwy ~ cty, data = mpg)\n\nCoefficients:\n(Intercept)          cty  \n      0.892        1.337  \n\ntypeof(mpg_model)\n\n[1] \"list\"\n\nstr(mpg_model)\n\nList of 12\n $ coefficients : Named num [1:2] 0.892 1.337\n  ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"cty\"\n $ residuals    : Named num [1:234] 4.0338 0.0214 3.3588 1.0214 3.7087 ...\n  ..- attr(*, \"names\")= chr [1:234] \"1\" \"2\" \"3\" \"4\" ...\n $ effects      : Named num [1:234] -358.566 -86.887 3.121 0.787 3.458 ...\n  ..- attr(*, \"names\")= chr [1:234] \"(Intercept)\" \"cty\" \"\" \"\" ...\n $ rank         : int 2\n $ fitted.values: Named num [1:234] 25 29 27.6 29 22.3 ...\n  ..- attr(*, \"names\")= chr [1:234] \"1\" \"2\" \"3\" \"4\" ...\n $ assign       : int [1:2] 0 1\n $ qr           :List of 5\n  ..$ qr   : num [1:234, 1:2] -15.2971 0.0654 0.0654 0.0654 0.0654 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:234] \"1\" \"2\" \"3\" \"4\" ...\n  .. .. ..$ : chr [1:2] \"(Intercept)\" \"cty\"\n  .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  ..$ qraux: num [1:2] 1.07 1.06\n  ..$ pivot: int [1:2] 1 2\n  ..$ tol  : num 1e-07\n  ..$ rank : int 2\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 232\n $ xlevels      : Named list()\n $ call         : language lm(formula = hwy ~ cty, data = mpg)\n $ terms        :Classes 'terms', 'formula'  language hwy ~ cty\n  .. ..- attr(*, \"variables\")= language list(hwy, cty)\n  .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:2] \"hwy\" \"cty\"\n  .. .. .. ..$ : chr \"cty\"\n  .. ..- attr(*, \"term.labels\")= chr \"cty\"\n  .. ..- attr(*, \"order\")= int 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. ..- attr(*, \"predvars\")= language list(hwy, cty)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. ..- attr(*, \"names\")= chr [1:2] \"hwy\" \"cty\"\n $ model        :'data.frame':  234 obs. of  2 variables:\n  ..$ hwy: int [1:234] 29 29 31 30 26 26 27 26 25 28 ...\n  ..$ cty: int [1:234] 18 21 20 21 16 18 18 18 16 20 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language hwy ~ cty\n  .. .. ..- attr(*, \"variables\")= language list(hwy, cty)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"hwy\" \"cty\"\n  .. .. .. .. ..$ : chr \"cty\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"cty\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(hwy, cty)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"hwy\" \"cty\"\n - attr(*, \"class\")= chr \"lm\"\n\n\nThere are three ways to extract from a list. Check out the pepper shaker analogy in Section 27.3.3 (note: shaker = list)\n\n[] returns new, smaller list (fewer pepper packs in shaker)\n[[]] drills down one level (individual pepper packs not in shaker)\n\nI. [ to extract a sub-list. The result is a list.\n\nmpg_model[1]\n\n$coefficients\n(Intercept)         cty \n  0.8920411   1.3374556 \n\ntypeof(mpg_model[1])\n\n[1] \"list\"\n\n\nyou can also do this by name, rather than number:\n\nmpg_model[\"coefficients\"]\n\n$coefficients\n(Intercept)         cty \n  0.8920411   1.3374556 \n\n\n\n[[ extracts a single component from the list… It removes a level of hierarchy\n\n\nmpg_model[[1]]\n\n(Intercept)         cty \n  0.8920411   1.3374556 \n\ntypeof(mpg_model[[1]])\n\n[1] \"double\"\n\n\nAgain, it can be done by name instead:\n\nmpg_model[[\"coefficients\"]]\n\n(Intercept)         cty \n  0.8920411   1.3374556 \n\n\n\n$ is a shorthand way of extracting elements by name… it is similar to [[ in that it removes a level of hierarchy. You don’t need quotes. (We’ve seen this with tibbles before too!)\n\n\nmpg_model$coefficients\n\n(Intercept)         cty \n  0.8920411   1.3374556",
    "crumbs": [
      "Data types"
    ]
  },
  {
    "objectID": "06_data_types.html#str",
    "href": "06_data_types.html#str",
    "title": "Data types",
    "section": "str",
    "text": "str\nThe str function allows us to see the structure of a list, as well as any attributes.\n\nmpg\n\n# A tibble: 234 × 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n 2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n 3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n 4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n 5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n 6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n 7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n 8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n 9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n# ℹ 224 more rows\n\nstr(mpg)\n\ntibble [234 × 11] (S3: tbl_df/tbl/data.frame)\n $ manufacturer: chr [1:234] \"audi\" \"audi\" \"audi\" \"audi\" ...\n $ model       : chr [1:234] \"a4\" \"a4\" \"a4\" \"a4\" ...\n $ displ       : num [1:234] 1.8 1.8 2 2 2.8 2.8 3.1 1.8 1.8 2 ...\n $ year        : int [1:234] 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 ...\n $ cyl         : int [1:234] 4 4 4 4 6 6 6 4 4 4 ...\n $ trans       : chr [1:234] \"auto(l5)\" \"manual(m5)\" \"manual(m6)\" \"auto(av)\" ...\n $ drv         : chr [1:234] \"f\" \"f\" \"f\" \"f\" ...\n $ cty         : int [1:234] 18 21 20 21 16 18 18 18 16 20 ...\n $ hwy         : int [1:234] 29 29 31 30 26 26 27 26 25 28 ...\n $ fl          : chr [1:234] \"p\" \"p\" \"p\" \"p\" ...\n $ class       : chr [1:234] \"compact\" \"compact\" \"compact\" \"compact\" ...\n\nmpg_model\n\n\nCall:\nlm(formula = hwy ~ cty, data = mpg)\n\nCoefficients:\n(Intercept)          cty  \n      0.892        1.337  \n\nstr(mpg_model)\n\nList of 12\n $ coefficients : Named num [1:2] 0.892 1.337\n  ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"cty\"\n $ residuals    : Named num [1:234] 4.0338 0.0214 3.3588 1.0214 3.7087 ...\n  ..- attr(*, \"names\")= chr [1:234] \"1\" \"2\" \"3\" \"4\" ...\n $ effects      : Named num [1:234] -358.566 -86.887 3.121 0.787 3.458 ...\n  ..- attr(*, \"names\")= chr [1:234] \"(Intercept)\" \"cty\" \"\" \"\" ...\n $ rank         : int 2\n $ fitted.values: Named num [1:234] 25 29 27.6 29 22.3 ...\n  ..- attr(*, \"names\")= chr [1:234] \"1\" \"2\" \"3\" \"4\" ...\n $ assign       : int [1:2] 0 1\n $ qr           :List of 5\n  ..$ qr   : num [1:234, 1:2] -15.2971 0.0654 0.0654 0.0654 0.0654 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:234] \"1\" \"2\" \"3\" \"4\" ...\n  .. .. ..$ : chr [1:2] \"(Intercept)\" \"cty\"\n  .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  ..$ qraux: num [1:2] 1.07 1.06\n  ..$ pivot: int [1:2] 1 2\n  ..$ tol  : num 1e-07\n  ..$ rank : int 2\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 232\n $ xlevels      : Named list()\n $ call         : language lm(formula = hwy ~ cty, data = mpg)\n $ terms        :Classes 'terms', 'formula'  language hwy ~ cty\n  .. ..- attr(*, \"variables\")= language list(hwy, cty)\n  .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:2] \"hwy\" \"cty\"\n  .. .. .. ..$ : chr \"cty\"\n  .. ..- attr(*, \"term.labels\")= chr \"cty\"\n  .. ..- attr(*, \"order\")= int 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. ..- attr(*, \"predvars\")= language list(hwy, cty)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. ..- attr(*, \"names\")= chr [1:2] \"hwy\" \"cty\"\n $ model        :'data.frame':  234 obs. of  2 variables:\n  ..$ hwy: int [1:234] 29 29 31 30 26 26 27 26 25 28 ...\n  ..$ cty: int [1:234] 18 21 20 21 16 18 18 18 16 20 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language hwy ~ cty\n  .. .. ..- attr(*, \"variables\")= language list(hwy, cty)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"hwy\" \"cty\"\n  .. .. .. .. ..$ : chr \"cty\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"cty\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(hwy, cty)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"hwy\" \"cty\"\n - attr(*, \"class\")= chr \"lm\"\n\n\nAs you can see, the mpg_model is a very complicated list with lots of attributes. The elements of the list can be all different types.\nThe last attribute is the object class, which it lists as lm.\n\nclass(mpg_model)\n\n[1] \"lm\"\n\n\nNow let’s see how extracting from a list works with a tibble (since a tibble is built on top of a list).\n\nugly_data &lt;- tibble(\n  truefalse = c(\"TRUE\", \"FALSE\", \"NA\"),\n  numbers = c(\"1\", \"2\", \"3\"),\n  dates = c(\"2010-01-01\", \"1979-10-14\", \"2013-08-17\"),\n  more_numbers = c(\"1\", \"231\", \".\")\n)\nugly_data\n\n# A tibble: 3 × 4\n  truefalse numbers dates      more_numbers\n  &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;       \n1 TRUE      1       2010-01-01 1           \n2 FALSE     2       1979-10-14 231         \n3 NA        3       2013-08-17 .           \n\nstr(ugly_data)   # we've seen str before... stands for \"structure\"\n\ntibble [3 × 4] (S3: tbl_df/tbl/data.frame)\n $ truefalse   : chr [1:3] \"TRUE\" \"FALSE\" \"NA\"\n $ numbers     : chr [1:3] \"1\" \"2\" \"3\"\n $ dates       : chr [1:3] \"2010-01-01\" \"1979-10-14\" \"2013-08-17\"\n $ more_numbers: chr [1:3] \"1\" \"231\" \".\"\n\npretty_data &lt;- ugly_data |&gt; \n  mutate(truefalse = parse_logical(truefalse),\n         numbers = parse_number(numbers),\n         dates = parse_date(dates),\n         more_numbers = parse_number(more_numbers))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `more_numbers = parse_number(more_numbers)`.\nCaused by warning:\n! 1 parsing failure.\nrow col expected actual\n  3  -- a number      .\n\npretty_data\n\n# A tibble: 3 × 4\n  truefalse numbers dates      more_numbers\n  &lt;lgl&gt;       &lt;dbl&gt; &lt;date&gt;            &lt;dbl&gt;\n1 TRUE            1 2010-01-01            1\n2 FALSE           2 1979-10-14          231\n3 NA              3 2013-08-17           NA\n\nstr(pretty_data)\n\ntibble [3 × 4] (S3: tbl_df/tbl/data.frame)\n $ truefalse   : logi [1:3] TRUE FALSE NA\n $ numbers     : num [1:3] 1 2 3\n $ dates       : Date[1:3], format: \"2010-01-01\" \"1979-10-14\" ...\n $ more_numbers: num [1:3] 1 231 NA\n  ..- attr(*, \"problems\")= tibble [1 × 4] (S3: tbl_df/tbl/data.frame)\n  .. ..$ row     : int 3\n  .. ..$ col     : int NA\n  .. ..$ expected: chr \"a number\"\n  .. ..$ actual  : chr \".\"\n\n# Get a smaller tibble\npretty_data[1]\n\n# A tibble: 3 × 1\n  truefalse\n  &lt;lgl&gt;    \n1 TRUE     \n2 FALSE    \n3 NA       \n\nclass(pretty_data[1])\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\ntypeof(pretty_data[1])\n\n[1] \"list\"\n\npretty_data[2:3]\n\n# A tibble: 3 × 2\n  numbers dates     \n    &lt;dbl&gt; &lt;date&gt;    \n1       1 2010-01-01\n2       2 1979-10-14\n3       3 2013-08-17\n\npretty_data[1, 3:4]\n\n# A tibble: 1 × 2\n  dates      more_numbers\n  &lt;date&gt;            &lt;dbl&gt;\n1 2010-01-01            1\n\npretty_data[\"dates\"]\n\n# A tibble: 3 × 1\n  dates     \n  &lt;date&gt;    \n1 2010-01-01\n2 1979-10-14\n3 2013-08-17\n\npretty_data[c(\"dates\", \"more_numbers\")]\n\n# A tibble: 3 × 2\n  dates      more_numbers\n  &lt;date&gt;            &lt;dbl&gt;\n1 2010-01-01            1\n2 1979-10-14          231\n3 2013-08-17           NA\n\npretty_data |&gt; select(dates, more_numbers) \n\n# A tibble: 3 × 2\n  dates      more_numbers\n  &lt;date&gt;            &lt;dbl&gt;\n1 2010-01-01            1\n2 1979-10-14          231\n3 2013-08-17           NA\n\npretty_data |&gt; select(dates, more_numbers) |&gt; slice(1:2) \n\n# A tibble: 2 × 2\n  dates      more_numbers\n  &lt;date&gt;            &lt;dbl&gt;\n1 2010-01-01            1\n2 1979-10-14          231\n\n# Remove a level of hierarchy - drill down one level to get a new object\npretty_data$dates\n\n[1] \"2010-01-01\" \"1979-10-14\" \"2013-08-17\"\n\nclass(pretty_data$dates)\n\n[1] \"Date\"\n\ntypeof(pretty_data$dates)\n\n[1] \"double\"\n\npretty_data[[1]]\n\n[1]  TRUE FALSE    NA\n\nclass(pretty_data[[1]])\n\n[1] \"logical\"\n\ntypeof(pretty_data[[1]])\n\n[1] \"logical\"\n\n\n[Pause to Ponder:] Predict what these lines will produce BEFORE running them:\n\npretty_data[[c(\"dates\", \"more_numbers\")]]\n\nError in `pretty_data[[c(\"dates\", \"more_numbers\")]]`:\n! Can't extract column with `c(\"dates\", \"more_numbers\")`.\n✖ Subscript `c(\"dates\", \"more_numbers\")` must be size 1, not 2.\n\npretty_data[[2]][[3]]\n\n[1] 3\n\npretty_data[[2]][3]\n\n[1] 3\n\npretty_data[[2]][c(TRUE, FALSE, TRUE)]\n\n[1] 1 3\n\npretty_data[[1]][c(1, 2, 1, 2)]\n\n[1]  TRUE FALSE  TRUE FALSE",
    "crumbs": [
      "Data types"
    ]
  },
  {
    "objectID": "06_data_types.html#generic-functions",
    "href": "06_data_types.html#generic-functions",
    "title": "Data types",
    "section": "Generic functions",
    "text": "Generic functions\nAnother important feature of R is generic functions. Some functions, like plot and summary for example, behave very differently depending on the class of their input.\n\nclass(mpg)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nsummary(mpg)\n\n manufacturer          model               displ            year     \n Length:234         Length:234         Min.   :1.600   Min.   :1999  \n Class :character   Class :character   1st Qu.:2.400   1st Qu.:1999  \n Mode  :character   Mode  :character   Median :3.300   Median :2004  \n                                       Mean   :3.472   Mean   :2004  \n                                       3rd Qu.:4.600   3rd Qu.:2008  \n                                       Max.   :7.000   Max.   :2008  \n      cyl           trans               drv                 cty       \n Min.   :4.000   Length:234         Length:234         Min.   : 9.00  \n 1st Qu.:4.000   Class :character   Class :character   1st Qu.:14.00  \n Median :6.000   Mode  :character   Mode  :character   Median :17.00  \n Mean   :5.889                                         Mean   :16.86  \n 3rd Qu.:8.000                                         3rd Qu.:19.00  \n Max.   :8.000                                         Max.   :35.00  \n      hwy             fl               class          \n Min.   :12.00   Length:234         Length:234        \n 1st Qu.:18.00   Class :character   Class :character  \n Median :24.00   Mode  :character   Mode  :character  \n Mean   :23.44                                        \n 3rd Qu.:27.00                                        \n Max.   :44.00                                        \n\nclass(mpg_model)\n\n[1] \"lm\"\n\nsummary(mpg_model)\n\n\nCall:\nlm(formula = hwy ~ cty, data = mpg)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.3408 -1.2790  0.0214  1.0338  4.0461 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.89204    0.46895   1.902   0.0584 .  \ncty          1.33746    0.02697  49.585   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.752 on 232 degrees of freedom\nMultiple R-squared:  0.9138,    Adjusted R-squared:  0.9134 \nF-statistic:  2459 on 1 and 232 DF,  p-value: &lt; 2.2e-16\n\n\nAs a simpler case, consider the mean function.\n\nmean\n\nfunction (x, ...) \nUseMethod(\"mean\")\n&lt;bytecode: 0x000002a3cdb313a8&gt;\n&lt;environment: namespace:base&gt;\n\n\nAs a generic function, we can see what methods are available:\n\nmethods(mean)\n\n[1] mean.Date        mean.default     mean.difftime    mean.POSIXct    \n[5] mean.POSIXlt     mean.quosure*    mean.vctrs_vctr*\nsee '?methods' for accessing help and source code\n\n\n\nmean(c(20, 21, 23))\n\n[1] 21.33333\n\nlibrary(lubridate)\ndate_test &lt;- ymd(c(\"2020-03-20\", \"2020-03-21\", \"2020-03-23\"))\nmean(date_test)\n\n[1] \"2020-03-21\"",
    "crumbs": [
      "Data types"
    ]
  },
  {
    "objectID": "06_data_types.html#what-makes-tibbles-special",
    "href": "06_data_types.html#what-makes-tibbles-special",
    "title": "Data types",
    "section": "What makes Tibbles special?",
    "text": "What makes Tibbles special?\nTibbles are lists that: - have names attributes (column/variable names) as well as row.names attributes. - have elements that are all vectors of the same length\n\nattributes(mpg)\n\n$class\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n$row.names\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n[109] 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n[127] 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n[145] 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n[163] 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n[181] 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n[199] 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216\n[217] 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234\n\n$names\n [1] \"manufacturer\" \"model\"        \"displ\"        \"year\"         \"cyl\"         \n [6] \"trans\"        \"drv\"          \"cty\"          \"hwy\"          \"fl\"          \n[11] \"class\"",
    "crumbs": [
      "Data types"
    ]
  },
  {
    "objectID": "06_data_types.html#on-your-own",
    "href": "06_data_types.html#on-your-own",
    "title": "Data types",
    "section": "On Your Own",
    "text": "On Your Own\n\nThe dataset roster includes 24 names (the first 24 alphabetically on this list of names). Let’s suppose this is our class, and you want to divide students into 6 groups. Modify the code below using the rep function to create groups in two different ways.\n\n\nbabynames &lt;- read_csv(\"https://joeroith.github.io/264_spring_2025/Data/babynames_2000.csv\")\n\nroster &lt;- babynames |&gt;\n  sample_n(size = 24) |&gt;\n  select(name) \n\nroster |&gt;\n  mutate(group_method1 = , \n         group_method2 = )\n\n\nHere’s is a really crazy list that tells you some stuff about data science.\n\n\ndata_sci &lt;- list(first = c(\"first it must work\", \"then it can be\" , \"pretty\"),\n                 DRY = c(\"Do not\", \"Repeat\", \"Yourself\"),\n                 dont_forget = c(\"garbage\", \"in\", \"out\"),\n                 our_first_tibble = mpg,\n                 integers = 1:25,\n                 doubles = sqrt(1:25),\n                 tidyverse = c(pack1 = \"ggplot2\", pack2 = \"dplyr\", \n                               pack3 = \"lubridate\", etc = \"and more!\"),\n                 opinion = list(\"MSCS 264 is\",  \"awesome!\", \"amazing!\", \"rainbows!\")\n                  )\n\nUse str to learn about data_sci.\nNow, figure out how to get exactly the following outputs. Bonus points if you can do it more than one way!\n[1] “first it must work” “then it can be” “pretty”\n$DRY [1] “Do not” “Repeat” “Yourself”\n[1] 3 1 4 1 5 9 3\n  pack1         etc \n“ggplot2” “and more!”\n[1] “rainbows!”\n[1] “garbage” “in” “garbage” “out”\n# A tibble: 234 x 2\n     hwy   cty\n   &lt;int&gt; &lt;int&gt;\n 1    29    18\n 2    29    21\n 3    31    20\n 4    30    21\n 5    26    16\n 6    26    18\n 7    27    18\n 8    26    18\n 9    25    16\n10    28    20\n# … with 224 more rows\n[[1]] [1] “MSCS 264 is”\n[[2]] [1] “amazing!”",
    "crumbs": [
      "Data types"
    ]
  },
  {
    "objectID": "03_functions.html",
    "href": "03_functions.html",
    "title": "Functions and tidy evaluation",
    "section": "",
    "text": "Based on Chapter 25 from R for Data Science\nYou can download this .qmd file from here. Just hit the Download Raw File button.",
    "crumbs": [
      "Functions and tidy evaluation"
    ]
  },
  {
    "objectID": "03_functions.html#vector-functions",
    "href": "03_functions.html#vector-functions",
    "title": "Functions and tidy evaluation",
    "section": "Vector functions",
    "text": "Vector functions\n\nExample 1: Rescale variables from 0 to 1.\nThis code creates a 10 x 4 tibble filled with random values taken from a normal distribution with mean 0 and SD 1\n\ndf &lt;- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\ndf\n\n# A tibble: 10 × 4\n        a      b       c       d\n    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1  2.25   0.199  0.0169 -1.66  \n 2  1.17   0.391 -2.30    0.543 \n 3  0.951 -0.742  0.643  -0.384 \n 4  1.03  -1.11  -1.73   -1.45  \n 5 -0.412 -2.43   1.05    0.599 \n 6  0.243 -1.81  -0.625   1.59  \n 7 -0.618 -0.683 -0.219   0.250 \n 8  0.472  0.435 -0.136   0.111 \n 9 -1.00   0.829 -0.785   1.38  \n10 -1.48   0.427 -0.792   0.0569\n\n\nThis code below for rescaling variables from 0 to 1 is ripe for functions… we did it four times!\nIt’s easiest to start with working code and turn it into a function.\n\ndf$a &lt;- (df$a - min(df$a)) / (max(df$a) - min(df$a))\ndf$b &lt;- (df$b - min(df$b)) / (max(df$b) - min(df$b))\ndf$c &lt;- (df$c - min(df$c)) / (max(df$c) - min(df$c))\ndf$d &lt;- (df$d - min(df$d)) / (max(df$d) - min(df$d))\ndf\n\n# A tibble: 10 × 4\n       a     b     c      d\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 1     0.807 0.692 0     \n 2 0.709 0.866 0     0.678 \n 3 0.652 0.518 0.879 0.392 \n 4 0.672 0.406 0.170 0.0631\n 5 0.286 0     1     0.695 \n 6 0.462 0.191 0.501 1     \n 7 0.231 0.536 0.622 0.588 \n 8 0.523 0.879 0.646 0.545 \n 9 0.128 1     0.453 0.937 \n10 0     0.877 0.451 0.528 \n\n\nNotice first what changes and what stays the same in each line. Then, if we look at the first line above, we see we have one value we’re using over and over: df$a. So our function will have one input. We’ll start with our code from that line, then replace the input (df$a) with x. We should give our function a name that explains what it does. The name should be a verb.\n\n# I'm going to show you how to write the function in class! \n# I have it in the code already below, but don't look yet!\n# Let's try to write it together first!\n\n. . . . . . . . .\n\n# Our function (first draft!)\nrescale01 &lt;- function(x) {\n  (x - min(x)) / (max(x) - min(x))\n}\n\nNote the general form of a function:\n\nname &lt;- function(arguments) {\n  body\n}\n\nEvery function contains 3 essential components:\n\nA name. The name should clearly evoke what the function does; hence, it is often a verb (action). Here we’ll use rescale01 because this function rescales a vector to lie between 0 and 1. snake_case is good; CamelCase is just okay.\nThe arguments. The arguments are things that vary across calls and they are usually nouns - first the data, then other details. Our analysis above tells us that we have just one; we’ll call it x because this is the conventional name for a numeric vector, but you can use any word.\nThe body. The body is the code that’s repeated across all the calls. By default a function will return the last statement; use return() to specify a return value\n\n\nSummary: Functions should be written for both humans and computers!\n\nOnce we have written a function we like, then we need to test it with different inputs!\n\ntemp &lt;- c(4, 6, 8, 9)\nrescale01(temp)\n\n[1] 0.0 0.4 0.8 1.0\n\ntemp0 &lt;- c(4, 6, 8, 9, NA)\nrescale01(temp0)\n\n[1] NA NA NA NA NA\n\n\nOK, so NA’s don’t work the way we want them to.\n\nrescale01 &lt;- function(x) {\n  (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))\n}\nrescale01(temp)\n\n[1] 0.0 0.4 0.8 1.0\n\nrescale01(temp0)\n\n[1] 0.0 0.4 0.8 1.0  NA\n\n\nWe can continue to improve our function. Here is another method, which uses the existing range function within R to avoid 3 max/min executions:\n\nrescale01 &lt;- function(x) {\n  rng &lt;- range(x, na.rm = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\nrescale01(temp)\n\n[1] 0.0 0.4 0.8 1.0\n\nrescale01(c(0, 5, 10))\n\n[1] 0.0 0.5 1.0\n\nrescale01(c(-10, 0, 10))\n\n[1] 0.0 0.5 1.0\n\nrescale01(c(1, 2, 3, NA, 5))\n\n[1] 0.00 0.25 0.50   NA 1.00\n\n\nWe should continue testing unusual inputs. Think carefully about how you want this function to behave… the current behavior is to include the Inf (infinity) value when calculating the range. You get strange output everywhere, but it’s pretty clear that there is a problem right away when you use the function. In the example below (rescale1), you ignore the infinity value when calculating the range. The function returns Inf for one value, and sensible stuff for the rest. In many cases this may be useful, but it could also hide a problem until you get deeper into an analysis.\n\nx &lt;- c(1:10, Inf)\nrescale01(x)\n\n [1]   0   0   0   0   0   0   0   0   0   0 NaN\n\nrescale1 &lt;- function(x) {\n  rng &lt;- range(x, na.rm = TRUE, finite = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\nrescale1(x)\n\n [1] 0.0000000 0.1111111 0.2222222 0.3333333 0.4444444 0.5555556 0.6666667\n [8] 0.7777778 0.8888889 1.0000000       Inf\n\n\nNow we’ve used functions to simplify original example. We will learn to simplify further in iterations (Ch 26)\n\ndf &lt;- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n# add a little noise\ndf$a[5] = NA\ndf$b[6] = Inf\ndf\n\n# A tibble: 10 × 4\n         a        b      c      d\n     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 -2.26     0.323  -0.923  1.11 \n 2 -1.60     0.0799 -0.670 -1.28 \n 3 -0.0323  -0.257   1.67   1.70 \n 4 -0.842   -0.983  -1.29  -1.17 \n 5 NA       -1.22    0.946 -0.608\n 6 -0.286  Inf       0.299  1.50 \n 7 -0.309   -0.318  -0.722  0.479\n 8  1.05    -1.95    0.863 -0.577\n 9  0.918    0.240   0.785 -0.653\n10 -0.906   -0.568  -1.13  -0.377\n\ndf$a_new &lt;- rescale1(df$a)\ndf$b_new &lt;- rescale1(df$b)\ndf$c_new &lt;- rescale1(df$c)\ndf$d_new &lt;- rescale1(df$d)\ndf\n\n# A tibble: 10 × 8\n         a        b      c      d  a_new   b_new  c_new  d_new\n     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 -2.26     0.323  -0.923  1.11   0       1     0.123  0.802 \n 2 -1.60     0.0799 -0.670 -1.28   0.199   0.893 0.209  0     \n 3 -0.0323  -0.257   1.67   1.70   0.672   0.745 1      1     \n 4 -0.842   -0.983  -1.29  -1.17   0.428   0.426 0      0.0365\n 5 NA       -1.22    0.946 -0.608 NA       0.321 0.754  0.225 \n 6 -0.286  Inf       0.299  1.50   0.596 Inf     0.536  0.933 \n 7 -0.309   -0.318  -0.722  0.479  0.589   0.718 0.191  0.590 \n 8  1.05    -1.95    0.863 -0.577  1       0     0.726  0.235 \n 9  0.918    0.240   0.785 -0.653  0.959   0.963 0.700  0.210 \n10 -0.906   -0.568  -1.13  -0.377  0.409   0.608 0.0526 0.303 \n\ndf |&gt;\n  select(1:4) |&gt;\n  mutate(a_new = rescale1(a),\n         b_new = rescale1(b),\n         c_new = rescale1(c),\n         d_new = rescale1(d))\n\n# A tibble: 10 × 8\n         a        b      c      d  a_new   b_new  c_new  d_new\n     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 -2.26     0.323  -0.923  1.11   0       1     0.123  0.802 \n 2 -1.60     0.0799 -0.670 -1.28   0.199   0.893 0.209  0     \n 3 -0.0323  -0.257   1.67   1.70   0.672   0.745 1      1     \n 4 -0.842   -0.983  -1.29  -1.17   0.428   0.426 0      0.0365\n 5 NA       -1.22    0.946 -0.608 NA       0.321 0.754  0.225 \n 6 -0.286  Inf       0.299  1.50   0.596 Inf     0.536  0.933 \n 7 -0.309   -0.318  -0.722  0.479  0.589   0.718 0.191  0.590 \n 8  1.05    -1.95    0.863 -0.577  1       0     0.726  0.235 \n 9  0.918    0.240   0.785 -0.653  0.959   0.963 0.700  0.210 \n10 -0.906   -0.568  -1.13  -0.377  0.409   0.608 0.0526 0.303 \n\n# Even better - from Chapter 26\ndf |&gt; \n  select(1:4) |&gt;\n  mutate(across(a:d, rescale1, .names = \"{.col}_new\"))\n\n# A tibble: 10 × 8\n         a        b      c      d  a_new   b_new  c_new  d_new\n     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 -2.26     0.323  -0.923  1.11   0       1     0.123  0.802 \n 2 -1.60     0.0799 -0.670 -1.28   0.199   0.893 0.209  0     \n 3 -0.0323  -0.257   1.67   1.70   0.672   0.745 1      1     \n 4 -0.842   -0.983  -1.29  -1.17   0.428   0.426 0      0.0365\n 5 NA       -1.22    0.946 -0.608 NA       0.321 0.754  0.225 \n 6 -0.286  Inf       0.299  1.50   0.596 Inf     0.536  0.933 \n 7 -0.309   -0.318  -0.722  0.479  0.589   0.718 0.191  0.590 \n 8  1.05    -1.95    0.863 -0.577  1       0     0.726  0.235 \n 9  0.918    0.240   0.785 -0.653  0.959   0.963 0.700  0.210 \n10 -0.906   -0.568  -1.13  -0.377  0.409   0.608 0.0526 0.303 \n\n\n\n\nOptions for handling NAs in functions\nBefore we try some practice problems, let’s consider various options for handling NAs in functions. We used the na.rm option within functions like min, max, and range in order to take care of missing values. But there are alternative approaches:\n\nfilter/remove the NA values before rescaling\ncreate an if statement to check if there are NAs; return an error if NAs exist\ncreate a removeNAs option in the function we are creating\n\nLet’s take a look at each alternative approach in turn:\n\nFilter/remove the NA values before rescaling\n\ndf &lt;- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\ndf$a[5] = NA\ndf\n\n# A tibble: 10 × 4\n         a       b      c      d\n     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1  0.712   0.800   1.48  -0.198\n 2 -0.917  -0.567   0.958  0.620\n 3 -0.169   0.615  -0.474 -0.933\n 4  0.224   0.737  -0.653  1.07 \n 5 NA      -0.0748 -0.797 -0.344\n 6 -1.38    2.50    1.68  -0.935\n 7  0.503  -1.61   -0.135 -2.29 \n 8  0.553  -0.403   0.468 -0.864\n 9  0.0574  0.411  -1.32  -0.628\n10 -0.958   1.93    0.471  1.52 \n\nrescale_basic &lt;- function(x) {\n  (x - min(x)) / (max(x) - min(x))\n}\n\ndf |&gt;\n  filter(!is.na(a)) |&gt;\n  mutate(new_a = rescale_basic(a))\n\n# A tibble: 9 × 5\n        a      b      c      d new_a\n    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1  0.712   0.800  1.48  -0.198 1    \n2 -0.917  -0.567  0.958  0.620 0.223\n3 -0.169   0.615 -0.474 -0.933 0.580\n4  0.224   0.737 -0.653  1.07  0.767\n5 -1.38    2.50   1.68  -0.935 0    \n6  0.503  -1.61  -0.135 -2.29  0.900\n7  0.553  -0.403  0.468 -0.864 0.924\n8  0.0574  0.411 -1.32  -0.628 0.688\n9 -0.958   1.93   0.471  1.52  0.203\n\n\n[Pause to Ponder:] Do you notice anything in the output above that gives you pause?\n\n\nCreate an if statement to check if there are NAs; return an error if NAs exist\nFirst, here’s an example involving weighted means:\n\n# Create function to calculate weighted mean\nwt_mean &lt;- function(x, w) {\n  sum(x * w) / sum(w)\n}\nwt_mean(c(1, 10), c(1/3, 2/3))\n\n[1] 7\n\nwt_mean(1:6, 1:3)\n\n[1] 7.666667\n\n\n[Pause to Ponder:] Why is the answer to the last call above 7.67? Aren’t we taking a weighted mean of 1-6, all of which are below 7?\n\n# update function to handle cases where data and weights of unequal length\nwt_mean &lt;- function(x, w) {\n  if (length(x) != length(w)) {\n    stop(\"`x` and `w` must be the same length\", call. = FALSE)\n  } else {\n  sum(w * x) / sum(w)\n  }  \n}\nwt_mean(1:6, 1:3) \n\nError: `x` and `w` must be the same length\n\n# should produce an error now if weights and data different lengths\n#  - nice example of if and else\n\n[Pause to Ponder:] What does the call. option do?\nNow let’s apply this to our rescaling function\n\nrescale_w_error &lt;- function(x) {\n  if (is.na(sum(x))) {\n    stop(\"`x` cannot have NAs\", call. = FALSE)\n  } else {\n    (x - min(x)) / (max(x) - min(x))\n  }  \n}\n\ntemp &lt;- c(4, 6, 8, 9)\nrescale_w_error(temp)\n\n[1] 0.0 0.4 0.8 1.0\n\ntemp &lt;- c(4, 6, 8, 9, NA)\nrescale_w_error(temp)\n\nError: `x` cannot have NAs\n\n\n[Pause to Ponder:] Why can’t we just use if (is.na(x)) instead of is.na(sum(x))?\n\n\nCreate a removeNAs option in the function we are creating\n\nrescale_NAoption &lt;- function(x, removeNAs = FALSE) {\n  (x - min(x, na.rm = removeNAs)) / \n    (max(x, na.rm = removeNAs) - min(x, na.rm = removeNAs))\n} \n\ntemp &lt;- c(4, 6, 8, 9)\nrescale_NAoption(temp)\n\n[1] 0.0 0.4 0.8 1.0\n\ntemp &lt;- c(4, 6, 8, 9, NA)\nrescale_NAoption(temp, removeNAs = TRUE)\n\n[1] 0.0 0.4 0.8 1.0  NA\n\n\nOK, but all the other summary stats functions use na.rm as the input, so to be consistent, it’s probably better to do something slightly awkward like this:\n\nrescale_NAoption &lt;- function(x, na.rm = FALSE) {\n  (x - min(x, na.rm = na.rm)) / \n    (max(x, na.rm = na.rm) - min(x, na.rm = na.rm))\n} \n\ntemp &lt;- c(4, 6, 8, 9, NA)\nrescale_NAoption(temp, na.rm = TRUE)\n\n[1] 0.0 0.4 0.8 1.0  NA\n\n\nwt_mean() is an example of a “summary function (single value output) instead of a”mutate function” (vector output) like rescale01(). Here’s another summary function to produce the mean absolute percentage error:\n\nmape &lt;- function(actual, predicted) {\n  sum(abs((actual - predicted) / actual)) / length(actual)\n}\n\ny &lt;- c(2,6,3,8,5)\nyhat &lt;- c(2.5, 5.1, 4.4, 7.8, 6.1)\nmape(actual = y, predicted = yhat)\n\n[1] 0.2223333",
    "crumbs": [
      "Functions and tidy evaluation"
    ]
  },
  {
    "objectID": "03_functions.html#data-frame-functions",
    "href": "03_functions.html#data-frame-functions",
    "title": "Functions and tidy evaluation",
    "section": "Data frame functions",
    "text": "Data frame functions\nThese work like dplyr verbs, taking a data frame as the first argument, and then returning a data frame or a vector.\n\nDemonstration of tidy evaluation in functions\n\n# Start with working code then functionize\nggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +\n  geom_point(size = 0.75) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\nmake_plot &lt;- function(dataset, xvar, yvar, pt_size = 0.75)  {\n  ggplot(data = dataset, mapping = aes(x = xvar, y = yvar)) +\n    geom_point(size = pt_size) +\n    geom_smooth()\n}\n\nmake_plot(dataset = mpg, xvar = cty, yvar = hwy)  # Error!\n\nError in `geom_point()`:\n! Problem while computing aesthetics.\nℹ Error occurred in the 1st layer.\nCaused by error:\n! object 'cty' not found\n\n\nThe problem is tidy evaluation, which makes most common coding easier, but makes some less common things harder. Key terms to understand tidy evaluation:\n\nenv-variables = live in the environment (mpg)\ndata-variables = live in data frame or tibble (cty)\ndata masking = tidyverse use data-variables as if they are env-variables. That is, you don’t always need mpg$cty to access cty in tidyverse\n\nThe key idea behind data masking is that it blurs the line between the two different meanings of the word “variable”:\n\nenv-variables are “programming” variables that live in an environment. They are usually created with &lt;-.\ndata-variables are “statistical” variables that live in a data frame. They usually come from data files (e.g. .csv, .xls), or are created manipulating existing variables.\n\nThe solution is to embrace {{ }} data-variables which are user inputs into functions. One way to remember what’s happening, as suggested by our book authors, is to think of {{ }} as looking down a tunnel — {{ var }} will make a dplyr function look inside of var rather than looking for a variable called var. Thus, embracing a variable tells dplyr to use the value stored inside the argument, not the argument as the literal variable name.\nSee Section 25.3 of R4DS for more details (and there are plenty!).\n\n# This will work to make our plot!\nmake_plot &lt;- function(dataset, xvar, yvar, pt_size = 0.75)  {\n  ggplot(data = dataset, mapping = aes(x = {{ xvar }}, y = {{ yvar }})) +\n    geom_point(size = pt_size) +\n    geom_smooth()\n}\n\nmake_plot(dataset = mpg, xvar = cty, yvar = hwy)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nI often wish it were easier to get my own custom summary statistics for numeric variables in EDA rather than using mosaic::favstats(). Using group_by() and summarise() from the tidyverse reads clearly but takes so many lines, but if I only had to write the code once…\n\nsummary6 &lt;- function(data, var) {\n  data |&gt; summarize(\n    mean = mean({{ var }}, na.rm = TRUE),\n    median = median({{ var }}, na.rm = TRUE),\n    sd = sd({{ var }}, na.rm = TRUE),\n    IQR = IQR({{ var }}, na.rm = TRUE),\n    n = n(),\n    n_miss = sum(is.na({{ var }})),\n    .groups = \"drop\"    # to leave the data in an ungrouped state\n  )\n}\n\nmpg |&gt; summary6(hwy)\n\n# A tibble: 1 × 6\n   mean median    sd   IQR     n n_miss\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;\n1  23.4     24  5.95     9   234      0\n\n\nEven cooler, I can use my new function with group_by()!\n\nmpg |&gt; \n  group_by(drv) |&gt;\n  summary6(hwy)\n\n# A tibble: 3 × 7\n  drv    mean median    sd   IQR     n n_miss\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;\n1 4      19.2     18  4.08     5   103      0\n2 f      28.2     28  4.21     3   106      0\n3 r      21       21  3.66     7    25      0\n\n\nYou can even pass conditions into a function using the embrace:\n[Pause to Ponder:] Predict what the code below will do, and (only) then run it to check. Think about: why do we have sort = sort? why not embrace df? why didn’t we need n in the arguments?\n\nnew_function &lt;- function(df, var, condition, sort = TRUE) {\n  df |&gt;\n    filter({{ condition }}) |&gt;\n    count({{ var }}, sort = sort) |&gt;\n    mutate(prop = n / sum(n))\n}\n\nmpg |&gt; new_function(var = manufacturer, \n                    condition = manufacturer %in% c(\"audi\", \n                                                    \"honda\", \n                                                    \"hyundai\", \n                                                    \"nissan\", \n                                                    \"subaru\", \n                                                    \"toyota\", \n                                                    \"volkswagen\")\n                    )\n\n\n\nData-masking vs. tidy-selection (Section 25.3.4)\nWhy doesn’t the following code work?\n\ncount_missing &lt;- function(df, group_vars, x_var) {\n  df |&gt; \n    group_by({{ group_vars }}) |&gt; \n    summarize(\n      n_miss = sum(is.na({{ x_var }})),\n      .groups = \"drop\"\n    )\n}\n\nflights |&gt; \n  count_missing(c(year, month, day), dep_time)\n\nError in `group_by()`:\nℹ In argument: `c(year, month, day)`.\nCaused by error:\n! `c(year, month, day)` must be size 336776 or 1, not 1010328.\n\n\nThe problem is that group_by() uses data-masking rather than tidy-selection; it is selecting certain variables rather than evaluating values of those variables. These are the two most common subtypes of tidy evaluation:\n\nData-masking is used in functions like arrange(), filter(), mutate(), and summarize() that compute with variables. Data masking is an R feature that blends programming variables that live inside environments (env-variables) with statistical variables stored in data frames (data-variables).\n\nTidy-selection is used for functions like select(), relocate(), and rename() that select variables. Tidy selection provides a concise dialect of R for selecting variables based on their names or properties.\n\nMore detail can be found here.\nThe error above can be solved by using the pick() function, which uses tidy selection inside of data masking:\n\ncount_missing &lt;- function(df, group_vars, x_var) {\n  df |&gt; \n    group_by(pick({{ group_vars }})) |&gt; \n    summarize(\n      n_miss = sum(is.na({{ x_var }})),\n      .groups = \"drop\"\n  )\n}\n\nflights |&gt; \n  count_missing(c(year, month, day), dep_time)\n\n# A tibble: 365 × 4\n    year month   day n_miss\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1  2013     1     1      4\n 2  2013     1     2      8\n 3  2013     1     3     10\n 4  2013     1     4      6\n 5  2013     1     5      3\n 6  2013     1     6      1\n 7  2013     1     7      3\n 8  2013     1     8      4\n 9  2013     1     9      5\n10  2013     1    10      3\n# ℹ 355 more rows\n\n\n[Pause to Ponder:] Here’s another nice use of pick(). Predict what the function will do, then run the code to see if you are correct.\n\n# Source: https://twitter.com/pollicipes/status/1571606508944719876\nnew_function &lt;- function(data, rows, cols) {\n  data |&gt; \n    count(pick(c({{ rows }}, {{ cols }}))) |&gt; \n    pivot_wider(\n      names_from = {{ cols }}, \n      values_from = n,\n      names_sort = TRUE,\n      values_fill = 0\n    )\n}\n\nmpg |&gt; new_function(c(manufacturer, model), cyl)",
    "crumbs": [
      "Functions and tidy evaluation"
    ]
  },
  {
    "objectID": "03_functions.html#plot-functions",
    "href": "03_functions.html#plot-functions",
    "title": "Functions and tidy evaluation",
    "section": "Plot functions",
    "text": "Plot functions\nLet’s say you find yourself making a lot of histograms:\n\nflights |&gt; \n  ggplot(aes(x = dep_time)) +\n  geom_histogram(bins = 25)\n\n\n\n\n\n\n\nflights |&gt; \n  ggplot(aes(x = air_time)) +\n  geom_histogram(bins = 35)\n\n\n\n\n\n\n\n\nJust use embrace to create a histogram-making function\n\nhistogram &lt;- function(df, var, bins = NULL) {\n  df |&gt; \n    ggplot(aes(x = {{ var }})) + \n    geom_histogram(bins = bins)\n}\n\nflights |&gt; histogram(air_time, 35)\n\n\n\n\n\n\n\n\nSince histogram() returns a ggplot, you can add any layers you want\n\nflights |&gt; \n  histogram(air_time, 35) +\n  labs(x = \"Flight time (minutes)\", y = \"Number of flights\")\n\n\n\n\n\n\n\n\nYou can also combine data wrangling with plotting. Note that we need the “walrus operator” (:=) since the variable name on the left is being generated with user-supplied data.\n\n# sort counts with highest values at top and counts on x-axis\nsorted_bars &lt;- function(df, var) {\n  df |&gt; \n    mutate({{ var }} := fct_rev(fct_infreq({{ var }})))  |&gt;\n    ggplot(aes(y = {{ var }})) +\n    geom_bar()\n}\n\nflights |&gt; sorted_bars(carrier)\n\n\n\n\n\n\n\n\nFinally, it would be really helpful to label plots based on user inputs. This is a bit more complicated, but still do-able!\nFor this, we’ll need the rlang package. rlang is a low-level package that’s used by just about every other package in the tidyverse because it implements tidy evaluation (as well as many other useful tools).\nCheck out the following update of our histogram() function which uses the englue() function from the rlang package:\n\nhistogram &lt;- function(df, var, bins) {\n  label &lt;- rlang::englue(\"A histogram of {{var}} with binwidth {bins}\")\n  \n  df |&gt; \n    ggplot(aes(x = {{ var }})) + \n    geom_histogram(bins = bins) + \n    labs(title = label)\n}\n\nflights |&gt; histogram(air_time, 35)\n\n\n\n\n\n\n\n\n\nOn Your Own\n\nRewrite this code snippet as a function: x / sum(x, na.rm = TRUE). This code creates weights which sum to 1, where NA values are ignored. Test it for at least two different vectors. (Make sure at least one has NAs!)\nCreate a function to calculate the standard error of a variable, where SE = square root of the variance divided by the sample size. Hint: start with a vector like x &lt;- 0:5 or x &lt;- gss_cat$age and write code to find the SE of x, then turn it into a function to handle any vector x. Note: var is the function to find variance in R and sqrt does square root. length may also be handy. Test your function on two vectors that do not include NAs (i.e. do not worry about removing NAs at this point).\nUse your se function within summarize to get a table of the mean and s.e. of hwy and cty by class in the mpg dataset.\nUse your se function within summarize to get a table of the mean and s.e. of arr_delay and dep_delay by carrier in the flights dataset. Why does the output look like this?\nMake your se function handle NAs with an na.rm option. Test your new function (you can call it se again) on a vector that doesn’t include NA and on the same vector with an added NA. Be sure to check that it gives the expected output with na.rm = TRUE and na.rm = FALSE. Make na.rm = FALSE the default value. Repeat #4. (Hint: be sure when you divide by sample size you don’t count any NAs)\nCreate both_na(), a function that takes two vectors of the same length and returns how many positions have an NA in both vectors. Hint: create two vectors like test_x &lt;- c(1, 2, 3, NA, NA) and test_y &lt;- c(NA, 1, 2, 3, NA) and write code that works for test_x and test_y, then turn it into a function that can handle any x and y. (In this case, the answer would be 1, since both vectors have NA in the 5th position.) Test it for at least one more combination of x and y.\nRun your code from (6) with the following two vectors: test_x &lt;- c(1, 2, 3, NA, NA, NA) and test_y &lt;- c(NA, 1, 2, 3, NA). Did you get the output you wanted or expected? Modify your function using if, else, and stop to print an error if x and y are not the same length. Then test again with test_x, test_y and the sets of vectors you used in (6).\nHere is a way to get not_cancelled flights in the flights dataset:\n\n\nnot_cancelled &lt;- flights |&gt; \n  filter(!is.na(dep_delay), !is.na(arr_delay))\n\nIs it necessary to check is.na for both departure and arrival? Using summarize, find the number of flights missing departure delay, arrival delay, and both. (Use your new function!)\n\nRead the code for each of the following three functions, puzzle out what they do, and then brainstorm better names.\n\n\nf1 &lt;- function(time1, time2) {\n  hour1 &lt;- time1 %/% 100\n  min1 &lt;- time1 %% 100\n  hour2 &lt;- time2 %/% 100\n  min2 &lt;- time2 %% 100\n  \n  (hour2 - hour1)*60 + (min2 - min1)\n}\n\n\nf2 &lt;- function(lengthcm, widthcm) {\n  (lengthcm / 2.54) * (widthcm / 2.54)\n}\n\n\nf3 &lt;- function(x) {\n  fct_collapse(x, \"non answer\" = c(\"No answer\", \"Refused\", \n                                   \"Don't know\", \"Not applicable\"))\n}\n\n\nExplain what the following function does and demonstrate by running foo1(x) with a few appropriately chosen vectors x. (Hint: set x and run the “guts” of the function piece by piece.)\n\n\nfoo1 &lt;- function(x) {\n  diff &lt;- x[-1] - x[1:(length(x) - 1)]\n  sum(diff &lt; 0)\n}\n\n\nThe foo1() function doesn’t perform well if a vector has missing values. Amend foo1() so that it produces a helpful error message and stops if there are any missing values in the input vector. Show that it works with appropriately chosen vectors x. Be sure you add error = TRUE to your R chunk, or else knitting will fail!\nWrite a function called greet using if, else if, and else to print out “good morning” if it’s before 12 PM, “good afternoon” if it’s between 12 PM and 5 PM, and “good evening” if it’s after 5 PM. Your function should work if you input a time like: greet(time = \"2018-05-03 17:38:01 CDT\") or if you input the current time with greet(time = Sys.time()). [Hint: check out the hour function in the lubridate package]\nModify the summary6() function from earlier to add an argument that gives the user an option to remove missing values, if any exist. Show that your function works for (a) the hwy variable in mpg_tbl &lt;- as_tibble(mpg), and (b) the age variable in gss_cat.\nAdd an argument to (13) to produce summary statistics by group for a second variable (you should now have 4 possible inputs to your function). Show that your function works for (a) the hwy variable in mpg_tbl &lt;- as_tibble(mpg) grouped by drv, and (b) the age variable in gss_cat grouped by partyid.\nCreate a function that has a vector as the input and returns the last value. (Note: Be sure to use a name that does not write over an existing function!)\nSave your final table from (14) and write a function to draw a scatterplot of a measure of center (mean or median - user can choose) vs. a measure of spread (sd or IQR - user can choose), with points sized by sample size, to see if there is constant variance. Each point should be labeled with partyid, and the plot title should reflect the variables chosen by the user.\n\nHint: start with a ggplot with no user input, and then functionize:\n\nlibrary(ggrepel)\nparty_age |&gt;\n  ggplot(aes(x = mean, y = sd)) + \n    geom_point(aes(size = n)) +\n    geom_smooth(method = lm) +\n    geom_label_repel(aes(label = partyid)) +\n    labs(title = \"Mean vs SD\")",
    "crumbs": [
      "Functions and tidy evaluation"
    ]
  },
  {
    "objectID": "github_intro.html",
    "href": "github_intro.html",
    "title": "Intro to GitHub",
    "section": "",
    "text": "Version Control (GitHub)\nIn order to collaborate on an R project (or any coding project in general), data scientists typically use a version control system like GitHub. With GitHub, you never have to save files as Final.docx, Final2.docx, Newfinal.docx, RealFinal.docx, nothisisreallyit.docx, etc. You update code like .qmd and .Rmd and data files, recording descriptions of any changes. Then if you ever want to go back to an earlier version, GitHub can facilitate that. Or if you want to make your work public, others can see it and even suggest changes, but you are ultimately in control of any changes that get made.\nAnd, you can have multiple collaborators with access to the same set of files. While it can be dicey if multiple people have the file open and make changes at the same time; if you do this with GitHub, it is at least POSSIBLE to get it straightened out, and the second person who tries to save will get warned. If you are both just using a common folder on RStudio, you can easily write over and erase each other’s work. (If you use a common folder, be sure that one person is editing at a time to prevent this).\nIn order to begin to get familiar with GitHub, we will use it to create a course folder for you.\n\n\nGetting started on GitHub and connecting to RStudio\n\nCreate a GitHub account at github.com. It’s usually okay to hit “Skip Personalization” at the bottom of the screen after entering an email, username, and password (you might have to enable 2-factor authentication as well). There are a few bonuses you can get as a student that you might consider.\n\nYou may choose to use a non-St. Olaf email address to ensure you’ll have access to your GitHub account after you graduate.\n\nObtain a personal access token (PAT) in GitHub using the following steps:\n\nClick your profile picture/symbol in the upper right of your GitHub page. Then select Settings &gt; Developer settings &gt; Personal access tokens &gt; Tokens (classic).\nClick “Generate new token (classic)” and give a descriptive name like “My PAT for RStudio”. Note that the default expiration is 30 days; I did a Custom setting through the end of the semester.\nSelect scopes and permissions; I often select: repo, workflow, gist, and user.\nClick “Generate token”. Copy your personal access token and store it somewhere.\n\nStore your credentials in RStudio using the following steps:\n\nIn the console, type library(credentials). You might have to install the credentials package first.\nThen type set_github_pat() and hit Return. You can sign in with the browser, or you can choose the Token option, where you can copy in your personal access token\n\n\nAlert! If the steps in (3) don’t work, you may have to install Git on your computer first. This chapter in “Happy Git with R” provides nice guidance for installing Git. Installing Git for Windows seems to work well on most Windows machines, using this site seems to work well for macOS, and a command like sudo apt-get install git can often work nicely in Linux. Once Git is installed, restart RStudio, and it will usually magically find Git. If not, there’s some good advice in this chapter of “Happy Git with R”. If you ever get frustrated with Git, remember that No one is giving out Git Nerd merit badges! Just muddle through until you figure out something that works for you!\n\n\nCreating an R project (local) that’s connected to GitHub (cloud)\n\nIn your GitHub account, click the \\(+ \\nabla\\) (+down arrow) button near the top right and select New Repository (repo). Put something like “SDS264_S25” for your repository (repo) name; use simple but descriptive names that avoid spaces. Check Private for now; you can turn a repository Public if you want later. Check Add a ReadMe File. Finally hit Create Repository and copy the URL once your repo has been created; the URL should be something like github.com/username/SDS264_S25.\nGo into your RStudio and select File &gt; New Project &gt; Version Control &gt; Git. For the repository URL paste in the URL for the repository you just created. A project directory named “SDS264_S25” will be created in your specified folder (which you can navigate to).\n\nNotice that you are now starting with a blank slate! Nothing in the environment or history. Also note where it says your project name in the top right corner.\nAt this point your should have a GitHub repo called “SDS264_S25” connected to an R project named “SDS264_S25”. The basic framework is set!\nHere is an illustration (source) of the process of using GitHub to manage version control and collaboration, followed by greater detail about each step:\n\n\n\nCreating new files in RStudio (local)\n\nYou can download our first file with in-class exercises here. Just hit the Download Raw File button and note where the file is saved on your computer. Use File &gt; Open File in RStudio to open up 01_review164.qmd. Then, use File &gt; Save As to navigate to the SDS264_S25 folder on your computer and save a copy there. You can even add your name or a few answers and re-save your file.\n\n\n\nMaking sure the connection to GitHub is ready\n\n[If necessary] You may need one of these two steps when using GitHub for the first name in a new R Project, even though you likely did them while installing GitHub.\n\nIn the console, type library(credentials). Then type set_github_pat(), hit Return, and copy in your personal access token.\nIn the Terminal window (this should be the tab next to Console), type the following two lines (be precise with the dashes and spaces!!):\n\n\n\ngit config --global user.name \"YOUR USER NAME\"\n\n\ngit config --global user.email \"YOUR EMAIL ASSOCIATED WITH GITHUB\"\n\n\n\nPushing your work to GitHub (cloud)\n\nWe now want to “push” the changes made in 01_review164.qmd to your GitHub repo (the changes have only been made in your local RStudio for now).\n\nunder the Git tab in the Environment panel, check the boxes in front of all modified files to “stage” your changes. To select a large number of files, check the top box, scroll down to the bottom of the list, and then shift-click the final box\nclick the Commit tab to “commit” your changes (like saving a file locally) along with a message describing the changes you made. GitHub guides you by by showing your old code (red) and new code (green), to make sure you approve of the changes. This first time, all code should be green since we’ve only added new things rather than modifying previously saved files.\n“push” your changes to GitHub (an external website) by clicking the green Up arrow. Refresh your GitHub account to see new files in the user_name.github.io repo!\n\n\n\n\nModifying files that have already been pushed to GitHub\n\nMake and save a change (anything) to your file 01_review164.qmd in RStudio. Now go back under the Git tab and “push” these new changes to GitHub. You’ll have to go through the same process of Stage, Commit, and Push, although this time you’ll see only your newest changes in green when you Commit. Confirm that your changes appear in GitHub.\n\n\n\nPulling work from GitHub\nBefore you start a new session of working on a project in RStudio, you should always Pull changes from GitHub first. Most of the time there will be nothing new, but if a collaborator made changes since the last time you worked on a file, you want to make sure you’re working with the latest and greatest version. If not, you’ll end up trying to Push changes made to an old version, and GitHub will balk and produce Merge Conflict messages. We’ll see how to handle Merge Conflicts later, but it’s a bit of a pain and best avoided!\n\nGo into 01_review164.qmd on GitHub and hit the Edit icon. Add a line anywhere, and then scroll down to hit Commit Changes. (This is not recommended and for illustrative purposes only! You will likely never edit directly in GitHub, but we’re emulating what might happen if a collaborator makes changes since the last time you worked on a document.) Now go back to RStudio and, under the Git tab, “Pull” the changes from GitHub into your R project folder. (Use the blue Down arrow). Confirm that your changes now appear in RStudio. Before you start working on the R server, you should always Pull any changes that might have been made on GitHub (especially if you’re working on a team!), since things can get dicey if you try to merge new changes from RStudio with new changes on GitHub.\n\n\n\nA bit more about R projects\n\nTo see the power of projects, select File &gt; Close Project and Don’t Save the workspace image. Then, select File &gt; Recent Projects &gt; SDS264_S25; you will get a clean Environment and Console once again, but History shows the commands you ran by hand, active Rmd and qmd files appear in the Source panel, and Files contains the Rmd, qmd, html, and csv files produced by your last session. And you can stage, commit and push the changes and the new file to GitHub!",
    "crumbs": [
      "Intro to GitHub"
    ]
  },
  {
    "objectID": "tech_setup.html",
    "href": "tech_setup.html",
    "title": "Tech Setup",
    "section": "",
    "text": "Ideally before class on Thurs Feb 6, and definitely before class on Tues Feb 11, you should follow these instructions to set up the software that we’ll be using throughout the semester. Even if you’ve already downloaded both R and RStudio, you’ll want to re-download to make sure that you have the most current versions.\n\nRequired: Download R and RStudio\n\nFIRST: Download R here.\n\nIn the top section, you will see three links “Download R for …”\nChoose the link that corresponds to your computer.\nAs of the start of this semester, the latest version of R is 4.4.2 (“Pile of Leaves”).\n\nSECOND: Download RStudio here.\n\nClick the button under step 2 to install the version of RStudio recommended for your computer.\nAs of the start of this semester, the latest version of RStudio is 2024.12.0 (Build 467).\n\nTHIRD: Check that when you go to File &gt; New Project &gt; New Directory, you see “Quarto Website” as an option.\n\n\nSuggested: Watch this video from Lisa Lendway at Macalester describing key configuration options for RStudio.\n\nSuggested: Change the default file download location for your internet browser.\n\nGenerally by default, internet browsers automatically save all files to the Downloads folder on your computer. In that case, you have to grab files from Downloads and move them to a more appropriate storage spot. You can change this option so that your browser asks you where to save each file before downloading it.\nThis page has information on how to do this for the most common browsers.\n\n\nRequired: Install required packages.\n\nAn R package is an extra bit of functionality that will help us in our data analysis efforts in a variety of ways. Many contributors create open source packages that can be added to base R to perform certain tasks in new and better ways.\nFor now, we’ll just make sure the tidyverse package is installed. Open RStudio and click on the Packages tab in the bottom right pane. Click the Install button and type “tidyverse” (without quotes) in the pop-up box. Click the Install button at the bottom of the pop-up box.\nYou will see a lot of text from status messages appearing in the Console as the packages are being installed. Wait until you see the &gt; again.\nEnter the command library(tidyverse) in the Console and hit Enter.\n\nQuit RStudio. You’re done setting up!\n\n\nOptional: For a refresher on RStudio features, watch this video. It also shows you how to customize the layout and color scheme of RStudio.",
    "crumbs": [
      "Tech Setup"
    ]
  },
  {
    "objectID": "04_code_quality.html",
    "href": "04_code_quality.html",
    "title": "Code quality",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.",
    "crumbs": [
      "Code quality"
    ]
  },
  {
    "objectID": "04_code_quality.html#code-style",
    "href": "04_code_quality.html#code-style",
    "title": "Code quality",
    "section": "Code style",
    "text": "Code style\nWe are going to take a timeout at this point to focus a little on code quality. Chapter 4 in R4DS provides a nice introduction to code style and why it’s important. As they do in that chapter, we will follow the tidyverse style guide in this class.\nBased on those resources, can you improve this poorly styled code chunk using the data from our DS1 Review activity?\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nVACCINE.DATA &lt;- read_csv(\"https://joeroith.github.io/264_spring_2025/Data/vaccinations_2021.csv\")\n\nRows: 3053 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): state, county, region, metro_status\ndbl (10): rural_urban_code, perc_complete_vac, tot_pop, votes_Trump, votes_B...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nVACCINE.DATA |&gt; filter(state %in% c(\"Minnesota\",\"Iowa\",\"Wisconsin\",\"North Dakota\",\"South Dakota\")) |&gt;\n  mutate(state_ordered=fct_reorder2(state,perc_Biden,perc_complete_vac),prop_Biden=perc_Biden/100,prop_complete_vac=perc_complete_vac/100) |&gt;\nggplot(mapping = aes(x = prop_Biden, y = prop_complete_vac, \n                       color = state_ordered)) +\ngeom_point() + geom_smooth(se = FALSE) +\nlabs(color = \"State\", x = \"Proportion of Biden votes\",\n     y = \"Proportion completely vaccinated\", title = \"The positive relationship between Biden votes and \\n vaccination rates by county differs by state\") +     \ntheme(axis.title = element_text(size=10), plot.title = element_text(size=12))  \n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'",
    "crumbs": [
      "Code quality"
    ]
  },
  {
    "objectID": "04_code_quality.html#code-comments",
    "href": "04_code_quality.html#code-comments",
    "title": "Code quality",
    "section": "Code comments",
    "text": "Code comments\nPlease read Fostering Better Coding Practices for Data Scientists, which lays out a nice case for the importance of teaching good coding practices. In particular, their Top 10 List can help achieve the four Cs (correctness, clarity, containment, and consistency) that typify high-quality code:\n\nChoose good names.\nFollow a style guide consistently.\nCreate documents using tools that support reproducible workflows.\nSelect a coherent, minimal, yet powerful tool kit.\nDon’t Repeat Yourself (DRY).\nTake advantage of a functional programming style.\nEmploy consistency checks.\nLearn how to debug and to ask for help.\nGet (version) control of the situation.\nBe multilingual.\n\nPlease also read the Stack Overflow blog on Best Practices for Writing Code Comments with their set of 9 rules:\n\nRule 1: Comments should not duplicate the code.\nRule 2: Good comments do not excuse unclear code.\nRule 3: If you can’t write a clear comment, there may be a problem with the code.\nRule 4: Comments should dispel confusion, not cause it.\nRule 5: Explain unidiomatic code in comments.\nRule 6: Provide links to the original source of copied code.\nRule 7: Include links to external references where they will be most helpful.\nRule 8: Add comments when fixing bugs.\nRule 9: Use comments to mark incomplete implementations.\n\nIn your projects and homework for this course, we will look for good style and good commenting to optimize your abilities as a collaborating data scientist!",
    "crumbs": [
      "Code quality"
    ]
  },
  {
    "objectID": "05_iteration.html",
    "href": "05_iteration.html",
    "title": "Iteration",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\nThis leans on parts of R4DS Chapter 26: Iteration, in addition to parts of the first edition of R4DS.\n# Initial packages required\nlibrary(tidyverse)",
    "crumbs": [
      "Iteration"
    ]
  },
  {
    "objectID": "05_iteration.html#iteration",
    "href": "05_iteration.html#iteration",
    "title": "Iteration",
    "section": "Iteration",
    "text": "Iteration\nReducing duplication of code will reduce errors and make debugging much easier. We’ve already seen how functions (Ch 25) can help reduce duplication by extracting repeated patterns of code. Another tool is iteration, when you find you’re doing the same thing to multiple inputs – repeating the same operation on different columns or datasets.\nHere we’ll see two important iteration paradigms: imperative programming and functional programming.",
    "crumbs": [
      "Iteration"
    ]
  },
  {
    "objectID": "05_iteration.html#imperation-programming-for-iteration",
    "href": "05_iteration.html#imperation-programming-for-iteration",
    "title": "Iteration",
    "section": "Imperation programming for iteration",
    "text": "Imperation programming for iteration\nExamples: for loops and while loops\nPros: relatively easy to learn, make iteration very explicit so it’s obvious what’s happening, not as inefficient as some people believe\nCons: require lots of bookkeeping code that’s duplicated for every loop\nEvery for loop has three components:\n\noutput - plan ahead and allocate enough space for output\nsequence - determines what to loop over; cycles through different values of \\(i\\)\nbody - code that does the work; run repeatedly with different values of \\(i\\)\n\n\ndf &lt;- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\ndf\n\n# A tibble: 10 × 4\n        a       b      c      d\n    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1  0.786 -0.682  -0.590 -0.611\n 2 -0.653 -0.238   0.138 -0.725\n 3 -0.110  0.0195 -0.173 -1.02 \n 4 -1.83  -0.176  -0.915  0.770\n 5 -0.952  1.45   -1.10  -1.54 \n 6 -1.44   0.406  -1.29  -0.196\n 7 -1.17  -0.208  -3.23   0.375\n 8 -0.519  1.37    0.109 -1.44 \n 9  1.42  -0.463  -1.26   0.253\n10 -0.887 -0.738   1.27   0.332\n\n# want median of each column (w/o cutting and pasting)\n#   Be careful using square brackets vs double square brackets when\n#   selecting elements\nmedian(df[[1]])\n\n[1] -0.7698718\n\nmedian(df[1])\n\nError in median.default(df[1]): need numeric data\n\ndf[1]\n\n# A tibble: 10 × 1\n        a\n    &lt;dbl&gt;\n 1  0.786\n 2 -0.653\n 3 -0.110\n 4 -1.83 \n 5 -0.952\n 6 -1.44 \n 7 -1.17 \n 8 -0.519\n 9  1.42 \n10 -0.887\n\ndf[[1]]\n\n [1]  0.7862419 -0.6530381 -0.1097493 -1.8310446 -0.9523553 -1.4445551\n [7] -1.1661358 -0.5191700  1.4231118 -0.8867055\n\nclass(df[1])\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nclass(df[[1]])\n\n[1] \"numeric\"\n\n# basic for loop to take median of each column\noutput &lt;- vector(\"double\", ncol(df))  # 1. output\nfor (i in 1:4) {                      # 2. sequence\n  output[[i]] &lt;- median(df[[i]])      # 3. body\n}\noutput\n\n[1] -0.7698718 -0.1919085 -0.7526727 -0.4034961\n\n# ?seq_along - a safer option if had zero length vectors\noutput &lt;- vector(\"double\", ncol(df))  # 1. output\nfor (i in seq_along(df)) {            # 2. sequence\n  output[[i]] &lt;- median(df[[i]])      # 3. body\n}\noutput\n\n[1] -0.7698718 -0.1919085 -0.7526727 -0.4034961\n\n# use [[.]] even if don't have to signal working with single elements\n\n# alternative solution - don't hardcode in \"4\"\noutput &lt;- vector(\"double\", ncol(df))  # 1. output\nfor(i in 1:ncol(df)) {                # 2. sequence\n  output[i] &lt;- median(df[[i]])        # 3. body\n}\noutput\n\n[1] -0.7698718 -0.1919085 -0.7526727 -0.4034961\n\n# another approach - no double square brackets since df not a tibble\ndf &lt;- as.data.frame(df)\noutput &lt;- vector(\"double\", ncol(df))  # 1. output\nfor(i in 1:ncol(df)) {                # 2. sequence\n  output[i] &lt;- median(df[,i])         # 3. body\n}\noutput\n\n[1] -0.7698718 -0.1919085 -0.7526727 -0.4034961\n\n\nOne advantage of seq_along(): works with unknown output length. However, the second approach below is much more efficient, since each iteration doesn’t copy all data from previous iterations.\n[Pause to Ponder:] What does the code below do? Be prepared to explain both chunks line-by-line!\n\n# for loop: unknown output length\n\nmeans &lt;- c(0, 1, 2)\noutput &lt;- double()\nfor (i in seq_along(means)) {\n  n &lt;- sample(100, 1)\n  output &lt;- c(output, rnorm(n, means[[i]]))\n}\nstr(output)        ## inefficient\n\n num [1:193] -0.692 1.195 -0.808 -0.956 0.647 ...\n\nout &lt;- vector(\"list\", length(means))\nfor (i in seq_along(means)) {\n  n &lt;- sample(100, 1)\n  out[[i]] &lt;- rnorm(n, means[[i]])\n}\nstr(out)           ## more efficient\n\nList of 3\n $ : num [1:88] 0.15004 -0.13345 -0.10121 0.42445 -0.00207 ...\n $ : num [1:17] -0.106 1.607 1.419 0.222 0.892 ...\n $ : num [1:59] 0.0893 3.2684 2.0144 0.9979 1.7632 ...\n\nstr(unlist(out))   ## flatten list of vectors into single vector\n\n num [1:164] 0.15004 -0.13345 -0.10121 0.42445 -0.00207 ...\n\n\nFinally, the while() loop can be used with unknown sequence length. This is used more in simulation than in data analysis.\n[Pause to Ponder:] What does the following code do?\n\nflip &lt;- function() sample(c(\"T\", \"H\"), 1)\nflips &lt;- 0\nnheads &lt;- 0\nwhile (nheads &lt; 3) {\n  if (flip() == \"H\") {\n    nheads &lt;- nheads + 1\n  } else {\n    nheads &lt;- 0\n  }\n  flips &lt;- flips + 1\n}\nflips\n\n[1] 12",
    "crumbs": [
      "Iteration"
    ]
  },
  {
    "objectID": "05_iteration.html#using-iteration-for-simulation",
    "href": "05_iteration.html#using-iteration-for-simulation",
    "title": "Iteration",
    "section": "Using iteration for simulation",
    "text": "Using iteration for simulation\nThis applet contains data from a 2005 study on the use of dolphin-facilitated therapy on the treatment of depression. In that study, 10 of the 15 subjects (67%) assigned to dolphin therapy showed improvement, compared to only 3 of the 15 subjects (20%) assigned to the control group. But with such small sample sizes, is this significant evidence that the dolphin group had greater improvement of their depressive symptoms? To answer that question, we can use simulation to conduct a randomization test.\nWe will simulate behavior in the “null world” where there is no real effect of treatment. In that case, the 13 total improvers would have improved no matter the treatment assigned, and the 17 total non-improvers would have not improved no matter the treatment assigned. So in the “null world”, treatment is a meaningless label that can be just as easily shuffled among subjects without any effect. In that world, the fact we observed a 47 percentage point difference in success rates (67 - 20) was just random luck. But we should ask: how often would we expect a difference as large as 47% by chance, assuming we’re living in the null world where there is no effect of treatment?\nYou could think about simulating this situation with the following steps:\n\nwrite code to calculate the difference in success rates in the observed data\nwrite a loop to calculate the differences in success rates from 1000 simulated data sets from the null world. Store those 1000 simulated differences\ncalculate how often we found a difference in the null world as large as that found in the observed data. In statistics, when this probability is below .05, we typically reject the null world, and conclude that there is likely a real difference between the two groups (i.e. a “statistically significant” difference)\n\n[Pause to Ponder:] Fill in Step 2 in the second R chunk below to carry out the three steps above. (The first R chunk provides some preliminary code.) Then describe what you can conclude from this study based on your plot and “p_value” from Step 3.\n\n### Preliminary code ###\n\n# generate a tibble with our observed data\ndolphin_data &lt;- tibble(treatment = rep(c(\"Dolphin\", \"Control\"), each = 15),\n                       improve = c(rep(\"Yes\", 10), rep(\"No\", 5), \n                                   rep(\"Yes\", 3), rep(\"No\", 12)))\nprint(dolphin_data, n = Inf)\n\n# A tibble: 30 × 2\n   treatment improve\n   &lt;chr&gt;     &lt;chr&gt;  \n 1 Dolphin   Yes    \n 2 Dolphin   Yes    \n 3 Dolphin   Yes    \n 4 Dolphin   Yes    \n 5 Dolphin   Yes    \n 6 Dolphin   Yes    \n 7 Dolphin   Yes    \n 8 Dolphin   Yes    \n 9 Dolphin   Yes    \n10 Dolphin   Yes    \n11 Dolphin   No     \n12 Dolphin   No     \n13 Dolphin   No     \n14 Dolphin   No     \n15 Dolphin   No     \n16 Control   Yes    \n17 Control   Yes    \n18 Control   Yes    \n19 Control   No     \n20 Control   No     \n21 Control   No     \n22 Control   No     \n23 Control   No     \n24 Control   No     \n25 Control   No     \n26 Control   No     \n27 Control   No     \n28 Control   No     \n29 Control   No     \n30 Control   No     \n\n# `sample()` can be used to shuffle the treatments among the 30 subjects\nsample(dolphin_data$treatment)\n\n [1] \"Dolphin\" \"Dolphin\" \"Dolphin\" \"Control\" \"Dolphin\" \"Dolphin\" \"Dolphin\"\n [8] \"Dolphin\" \"Dolphin\" \"Control\" \"Dolphin\" \"Control\" \"Dolphin\" \"Control\"\n[15] \"Control\" \"Control\" \"Dolphin\" \"Dolphin\" \"Control\" \"Control\" \"Dolphin\"\n[22] \"Control\" \"Dolphin\" \"Control\" \"Dolphin\" \"Control\" \"Control\" \"Control\"\n[29] \"Control\" \"Control\"\n\n\n\n### Fill in Step 2 and remove \"eval: FALSE\" ###\n\n# Step 1\ndolphin_summary &lt;- dolphin_data |&gt;\n  group_by(treatment) |&gt;\n  summarize(prop_yes = mean(improve == \"Yes\"))\ndolphin_summary\nobserved_diff &lt;- dolphin_summary[[2]][2] - dolphin_summary[[2]][1]\n\n# Step 2\n\n### Write a loop to create 1000 simulated differences from the null world\n\n# Step 3\nnull_world &lt;- tibble(simulated_diffs = simulated_diffs)\nggplot(null_world, aes(x = simulated_diffs)) +\n  geom_histogram() +\n  geom_vline(xintercept = observed_diff, color = \"red\")\n\np_value &lt;- sum(abs(simulated_diffs) &gt;= abs(observed_diff)) / 1000\np_value\n\nYou have written code to conduct a randomization test for the difference in two proportions, a powerful test of statistical significance that is demonstrated in the original applet!",
    "crumbs": [
      "Iteration"
    ]
  },
  {
    "objectID": "05_iteration.html#functional-programming-for-iteration",
    "href": "05_iteration.html#functional-programming-for-iteration",
    "title": "Iteration",
    "section": "Functional programming for iteration",
    "text": "Functional programming for iteration\nExamples: map functions and across()\nPros: less code, fewer errors, code that’s easier to read; takes advantage of fact that R is a functional programming language\nCons: little more complicated to master vocabulary and use – a step up in abstraction\nR is a functional programming language. This means that it’s possible to wrap up for loops in a function, and call that function instead of using the for loop directly. Passing one function to another is a very powerful coding approach!!\n\n# Below you can avoid writing separate functions for mean, median, \n#   SD, etc. by column\ndf &lt;- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\ncol_summary &lt;- function(df, fun) {\n  out &lt;- vector(\"double\", length(df))\n  for (i in seq_along(df)) {\n    out[i] &lt;- fun(df[[i]])\n  }\n  out\n}\ncol_summary(df, median)\n\n[1] 0.14666779 0.37184210 0.08700853 0.48526124\n\ncol_summary(df, mean)\n\n[1]  0.02540934  0.49165160 -0.27493683  0.20658251\n\ncol_summary(df, IQR)\n\n[1] 1.224688 1.437133 1.793563 1.262816\n\n\nThe purrr package provides map functions to eliminate need for for loops, plus it makes code easier to read!\n\n# using map functions for summary stats by column as above\nmap_dbl(df, mean)\n\n          a           b           c           d \n 0.02540934  0.49165160 -0.27493683  0.20658251 \n\nmap_dbl(df, median)\n\n         a          b          c          d \n0.14666779 0.37184210 0.08700853 0.48526124 \n\nmap_dbl(df, sd)\n\n        a         b         c         d \n1.0198013 0.9503438 1.2372369 1.1197180 \n\nmap_dbl(df, mean, trim = 0.5)\n\n         a          b          c          d \n0.14666779 0.37184210 0.08700853 0.48526124 \n\n# map_dbl means make a double vector\n# can also do map() for list, map_lgl(), map_int(), and map_chr()\n\n# even more clear\ndf |&gt; map_dbl(mean)\n\n          a           b           c           d \n 0.02540934  0.49165160 -0.27493683  0.20658251 \n\ndf |&gt; map_dbl(median)\n\n         a          b          c          d \n0.14666779 0.37184210 0.08700853 0.48526124 \n\ndf |&gt; map_dbl(sd)\n\n        a         b         c         d \n1.0198013 0.9503438 1.2372369 1.1197180 \n\n\nThe across() function from dplyr also works well:\n\ndf |&gt; summarize(\n  n = n(),\n  across(.cols = a:d, .fns = median, .names = \"median_{.col}\")\n)\n\n# A tibble: 1 × 5\n      n median_a median_b median_c median_d\n  &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1    10    0.147    0.372   0.0870    0.485\n\n# other ways to repeat across the numeric columns of df:\ndf |&gt; summarize(\n  n = n(),\n  across(everything(), median, .names = \"median_{.col}\")\n)\n\n# A tibble: 1 × 6\n      n median_a median_b median_c median_d median_n\n  &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;int&gt;\n1    10    0.147    0.372   0.0870    0.485       10\n\ndf |&gt; summarize(\n  n = n(),\n  across(where(is.numeric), median, .names = \"median_{.col}\")\n)\n\n# A tibble: 1 × 6\n      n median_a median_b median_c median_d median_n\n  &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;int&gt;\n1    10    0.147    0.372   0.0870    0.485       10\n\n# Here \"across\" effectively expands to the following code.  Note that \n#   across() will write over old columns unless you change the name!\ndf |&gt; \n  summarize(\n    median_a = median(a),\n    median_b = median(b),\n    median_c = median(c),\n    median_d = median(d),\n    n = n()\n  )\n\n# A tibble: 1 × 5\n  median_a median_b median_c median_d     n\n     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n1    0.147    0.372   0.0870    0.485    10\n\n# And if we're worried about NAs, we can't call median directly, we\n#   must create a new function that we can pass options into\ndf_miss &lt;- df\ndf_miss[2, 1] &lt;- NA\ndf_miss[4:5, 2] &lt;- NA\ndf_miss\n\n# A tibble: 10 × 4\n        a       b       c        d\n    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1 -0.352  0.186   0.0120 -0.00363\n 2 NA      1.46   -1.65    1.41   \n 3  0.116  1.40   -1.75    1.21   \n 4  0.178 NA       0.667   0.508  \n 5  0.283 NA       1.74    1.20   \n 6  1.67   1.40   -0.456   0.742  \n 7  0.879 -0.384  -2.18   -1.13   \n 8 -0.542 -1.26    0.162  -2.11   \n 9 -1.80   0.0766  0.179   0.462  \n10 -1.09   1.55    0.528  -0.232  \n\ndf_miss |&gt; \n  summarize(\n    across(\n      a:d,\n      list(\n        median = \\(x) median(x, na.rm = TRUE),\n        n_miss = \\(x) sum(is.na(x))\n      ),\n      .names = \"{.fn}_{.col}\"\n    ),\n    n = n(),\n  )\n\n# A tibble: 1 × 9\n  median_a n_miss_a median_b n_miss_b median_c n_miss_c median_d n_miss_d     n\n     &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt; &lt;int&gt;\n1    0.116        1    0.794        2   0.0870        0    0.485        0    10\n\n# where \\ is shorthand for an anonymous function - i.e. you could\n#   replace \"\\\" with \"function\" if you like typing more letters :)\n\n# across-like functions can also be used with filter():\n\n# same as df_miss |&gt; filter(is.na(a) | is.na(b) | is.na(c) | is.na(d))\ndf_miss |&gt; filter(if_any(a:d, is.na))\n\n# A tibble: 3 × 4\n       a     b      c     d\n   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 NA      1.46 -1.65  1.41 \n2  0.178 NA     0.667 0.508\n3  0.283 NA     1.74  1.20 \n\n# same as df_miss |&gt; filter(is.na(a) & is.na(b) & is.na(c) & is.na(d))\ndf_miss |&gt; filter(if_all(a:d, is.na))\n\n# A tibble: 0 × 4\n# ℹ 4 variables: a &lt;dbl&gt;, b &lt;dbl&gt;, c &lt;dbl&gt;, d &lt;dbl&gt;\n\n\nWhen you input a list of functions (like the lubridate functions below), across() assigns default names as columnname_functionname:\n\nlibrary(lubridate)\nexpand_dates &lt;- function(df) {\n  df |&gt; \n    mutate(\n      across(where(is.Date), list(year = year, month = month, day = mday))\n    )\n}\n\ndf_date &lt;- tibble(\n  name = c(\"Amy\", \"Bob\"),\n  date = ymd(c(\"2009-08-03\", \"2010-01-16\"))\n)\n\ndf_date |&gt; \n  expand_dates()\n\n# A tibble: 2 × 5\n  name  date       date_year date_month date_day\n  &lt;chr&gt; &lt;date&gt;         &lt;dbl&gt;      &lt;dbl&gt;    &lt;int&gt;\n1 Amy   2009-08-03      2009          8        3\n2 Bob   2010-01-16      2010          1       16\n\n\nHere the default is to summarize all numeric columns, but as with all functions, we can override the default if we choose:\n\nsummarize_means &lt;- function(df, summary_vars = where(is.numeric)) {\n  df |&gt; \n    summarize(\n      across({{ summary_vars }}, \\(x) mean(x, na.rm = TRUE)),\n      n = n(),\n      .groups = \"drop\"\n    )\n}\ndiamonds |&gt; \n  group_by(cut) |&gt; \n  summarize_means()\n\n# A tibble: 5 × 9\n  cut       carat depth table price     x     y     z     n\n  &lt;ord&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 Fair      1.05   64.0  59.1 4359.  6.25  6.18  3.98  1610\n2 Good      0.849  62.4  58.7 3929.  5.84  5.85  3.64  4906\n3 Very Good 0.806  61.8  58.0 3982.  5.74  5.77  3.56 12082\n4 Premium   0.892  61.3  58.7 4584.  5.97  5.94  3.65 13791\n5 Ideal     0.703  61.7  56.0 3458.  5.51  5.52  3.40 21551\n\ndiamonds |&gt; \n  group_by(cut) |&gt; \n  summarize_means(c(carat, x:z))\n\n# A tibble: 5 × 6\n  cut       carat     x     y     z     n\n  &lt;ord&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 Fair      1.05   6.25  6.18  3.98  1610\n2 Good      0.849  5.84  5.85  3.64  4906\n3 Very Good 0.806  5.74  5.77  3.56 12082\n4 Premium   0.892  5.97  5.94  3.65 13791\n5 Ideal     0.703  5.51  5.52  3.40 21551\n\n\npivot_longer() with group_by() and summarize() also provides a nice solution:\n\nlong &lt;- df |&gt; \n  pivot_longer(a:d) |&gt; \n  group_by(name) |&gt; \n  summarize(\n    median = median(value),\n    mean = mean(value)\n  )\nlong\n\n# A tibble: 4 × 3\n  name  median    mean\n  &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 a     0.147   0.0254\n2 b     0.372   0.492 \n3 c     0.0870 -0.275 \n4 d     0.485   0.207 \n\n\nHere are a couple of other nice features of map functions: - perform analyses (like fitting a line) by subgroup - extracting components from a model or elements by position\n\n# fit linear model to each group based on cylinder\n#   - split designed to split into new dfs (unlike group_by)\n#   - map returns a vector or list, which can be limiting\nmap = purrr::map\nmodels &lt;- split(mtcars, mtcars$cyl) |&gt;\n  map(function(df) lm(mpg ~ wt, data = df))\nmodels\n\n$`4`\n\nCall:\nlm(formula = mpg ~ wt, data = df)\n\nCoefficients:\n(Intercept)           wt  \n     39.571       -5.647  \n\n\n$`6`\n\nCall:\nlm(formula = mpg ~ wt, data = df)\n\nCoefficients:\n(Intercept)           wt  \n      28.41        -2.78  \n\n\n$`8`\n\nCall:\nlm(formula = mpg ~ wt, data = df)\n\nCoefficients:\n(Intercept)           wt  \n     23.868       -2.192  \n\nmodels[[1]]\n\n\nCall:\nlm(formula = mpg ~ wt, data = df)\n\nCoefficients:\n(Intercept)           wt  \n     39.571       -5.647  \n\n# shortcut using purrr - 1-sided formulas\nmodels &lt;- split(mtcars, mtcars$cyl) |&gt; \n  map(~lm(mpg ~ wt, data = .))\nmodels\n\n$`4`\n\nCall:\nlm(formula = mpg ~ wt, data = .)\n\nCoefficients:\n(Intercept)           wt  \n     39.571       -5.647  \n\n\n$`6`\n\nCall:\nlm(formula = mpg ~ wt, data = .)\n\nCoefficients:\n(Intercept)           wt  \n      28.41        -2.78  \n\n\n$`8`\n\nCall:\nlm(formula = mpg ~ wt, data = .)\n\nCoefficients:\n(Intercept)           wt  \n     23.868       -2.192  \n\n# extract named components from each model\nstr(models)\n\nList of 3\n $ 4:List of 12\n  ..$ coefficients : Named num [1:2] 39.57 -5.65\n  .. ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"wt\"\n  ..$ residuals    : Named num [1:11] -3.6701 2.8428 1.0169 5.2523 -0.0513 ...\n  .. ..- attr(*, \"names\")= chr [1:11] \"Datsun 710\" \"Merc 240D\" \"Merc 230\" \"Fiat 128\" ...\n  ..$ effects      : Named num [1:11] -88.433 10.171 0.695 6.231 1.728 ...\n  .. ..- attr(*, \"names\")= chr [1:11] \"(Intercept)\" \"wt\" \"\" \"\" ...\n  ..$ rank         : int 2\n  ..$ fitted.values: Named num [1:11] 26.5 21.6 21.8 27.1 30.5 ...\n  .. ..- attr(*, \"names\")= chr [1:11] \"Datsun 710\" \"Merc 240D\" \"Merc 230\" \"Fiat 128\" ...\n  ..$ assign       : int [1:2] 0 1\n  ..$ qr           :List of 5\n  .. ..$ qr   : num [1:11, 1:2] -3.317 0.302 0.302 0.302 0.302 ...\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:11] \"Datsun 710\" \"Merc 240D\" \"Merc 230\" \"Fiat 128\" ...\n  .. .. .. ..$ : chr [1:2] \"(Intercept)\" \"wt\"\n  .. .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  .. ..$ qraux: num [1:2] 1.3 1.5\n  .. ..$ pivot: int [1:2] 1 2\n  .. ..$ tol  : num 1e-07\n  .. ..$ rank : int 2\n  .. ..- attr(*, \"class\")= chr \"qr\"\n  ..$ df.residual  : int 9\n  ..$ xlevels      : Named list()\n  ..$ call         : language lm(formula = mpg ~ wt, data = .)\n  ..$ terms        :Classes 'terms', 'formula'  language mpg ~ wt\n  .. .. ..- attr(*, \"variables\")= language list(mpg, wt)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"mpg\" \"wt\"\n  .. .. .. .. ..$ : chr \"wt\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"wt\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: 0x0000023438312f48&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(mpg, wt)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"wt\"\n  ..$ model        :'data.frame':   11 obs. of  2 variables:\n  .. ..$ mpg: num [1:11] 22.8 24.4 22.8 32.4 30.4 33.9 21.5 27.3 26 30.4 ...\n  .. ..$ wt : num [1:11] 2.32 3.19 3.15 2.2 1.61 ...\n  .. ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language mpg ~ wt\n  .. .. .. ..- attr(*, \"variables\")= language list(mpg, wt)\n  .. .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. .. ..$ : chr [1:2] \"mpg\" \"wt\"\n  .. .. .. .. .. ..$ : chr \"wt\"\n  .. .. .. ..- attr(*, \"term.labels\")= chr \"wt\"\n  .. .. .. ..- attr(*, \"order\")= int 1\n  .. .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. .. ..- attr(*, \"response\")= int 1\n  .. .. .. ..- attr(*, \".Environment\")=&lt;environment: 0x0000023438312f48&gt; \n  .. .. .. ..- attr(*, \"predvars\")= language list(mpg, wt)\n  .. .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"wt\"\n  ..- attr(*, \"class\")= chr \"lm\"\n $ 6:List of 12\n  ..$ coefficients : Named num [1:2] 28.41 -2.78\n  .. ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"wt\"\n  ..$ residuals    : Named num [1:7] -0.125 0.584 1.929 -0.69 0.355 ...\n  .. ..- attr(*, \"names\")= chr [1:7] \"Mazda RX4\" \"Mazda RX4 Wag\" \"Hornet 4 Drive\" \"Valiant\" ...\n  ..$ effects      : Named num [1:7] -52.235 -2.427 2.111 -0.353 0.679 ...\n  .. ..- attr(*, \"names\")= chr [1:7] \"(Intercept)\" \"wt\" \"\" \"\" ...\n  ..$ rank         : int 2\n  ..$ fitted.values: Named num [1:7] 21.1 20.4 19.5 18.8 18.8 ...\n  .. ..- attr(*, \"names\")= chr [1:7] \"Mazda RX4\" \"Mazda RX4 Wag\" \"Hornet 4 Drive\" \"Valiant\" ...\n  ..$ assign       : int [1:2] 0 1\n  ..$ qr           :List of 5\n  .. ..$ qr   : num [1:7, 1:2] -2.646 0.378 0.378 0.378 0.378 ...\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:7] \"Mazda RX4\" \"Mazda RX4 Wag\" \"Hornet 4 Drive\" \"Valiant\" ...\n  .. .. .. ..$ : chr [1:2] \"(Intercept)\" \"wt\"\n  .. .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  .. ..$ qraux: num [1:2] 1.38 1.12\n  .. ..$ pivot: int [1:2] 1 2\n  .. ..$ tol  : num 1e-07\n  .. ..$ rank : int 2\n  .. ..- attr(*, \"class\")= chr \"qr\"\n  ..$ df.residual  : int 5\n  ..$ xlevels      : Named list()\n  ..$ call         : language lm(formula = mpg ~ wt, data = .)\n  ..$ terms        :Classes 'terms', 'formula'  language mpg ~ wt\n  .. .. ..- attr(*, \"variables\")= language list(mpg, wt)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"mpg\" \"wt\"\n  .. .. .. .. ..$ : chr \"wt\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"wt\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: 0x00000234385041b8&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(mpg, wt)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"wt\"\n  ..$ model        :'data.frame':   7 obs. of  2 variables:\n  .. ..$ mpg: num [1:7] 21 21 21.4 18.1 19.2 17.8 19.7\n  .. ..$ wt : num [1:7] 2.62 2.88 3.21 3.46 3.44 ...\n  .. ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language mpg ~ wt\n  .. .. .. ..- attr(*, \"variables\")= language list(mpg, wt)\n  .. .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. .. ..$ : chr [1:2] \"mpg\" \"wt\"\n  .. .. .. .. .. ..$ : chr \"wt\"\n  .. .. .. ..- attr(*, \"term.labels\")= chr \"wt\"\n  .. .. .. ..- attr(*, \"order\")= int 1\n  .. .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. .. ..- attr(*, \"response\")= int 1\n  .. .. .. ..- attr(*, \".Environment\")=&lt;environment: 0x00000234385041b8&gt; \n  .. .. .. ..- attr(*, \"predvars\")= language list(mpg, wt)\n  .. .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"wt\"\n  ..- attr(*, \"class\")= chr \"lm\"\n $ 8:List of 12\n  ..$ coefficients : Named num [1:2] 23.87 -2.19\n  .. ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"wt\"\n  ..$ residuals    : Named num [1:14] 2.374 -1.741 1.455 1.61 -0.381 ...\n  .. ..- attr(*, \"names\")= chr [1:14] \"Hornet Sportabout\" \"Duster 360\" \"Merc 450SE\" \"Merc 450SL\" ...\n  ..$ effects      : Named num [1:14] -56.499 -6.003 0.816 1.22 -0.807 ...\n  .. ..- attr(*, \"names\")= chr [1:14] \"(Intercept)\" \"wt\" \"\" \"\" ...\n  ..$ rank         : int 2\n  ..$ fitted.values: Named num [1:14] 16.3 16 14.9 15.7 15.6 ...\n  .. ..- attr(*, \"names\")= chr [1:14] \"Hornet Sportabout\" \"Duster 360\" \"Merc 450SE\" \"Merc 450SL\" ...\n  ..$ assign       : int [1:2] 0 1\n  ..$ qr           :List of 5\n  .. ..$ qr   : num [1:14, 1:2] -3.742 0.267 0.267 0.267 0.267 ...\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:14] \"Hornet Sportabout\" \"Duster 360\" \"Merc 450SE\" \"Merc 450SL\" ...\n  .. .. .. ..$ : chr [1:2] \"(Intercept)\" \"wt\"\n  .. .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  .. ..$ qraux: num [1:2] 1.27 1.11\n  .. ..$ pivot: int [1:2] 1 2\n  .. ..$ tol  : num 1e-07\n  .. ..$ rank : int 2\n  .. ..- attr(*, \"class\")= chr \"qr\"\n  ..$ df.residual  : int 12\n  ..$ xlevels      : Named list()\n  ..$ call         : language lm(formula = mpg ~ wt, data = .)\n  ..$ terms        :Classes 'terms', 'formula'  language mpg ~ wt\n  .. .. ..- attr(*, \"variables\")= language list(mpg, wt)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"mpg\" \"wt\"\n  .. .. .. .. ..$ : chr \"wt\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"wt\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: 0x0000023434d32ea0&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(mpg, wt)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"wt\"\n  ..$ model        :'data.frame':   14 obs. of  2 variables:\n  .. ..$ mpg: num [1:14] 18.7 14.3 16.4 17.3 15.2 10.4 10.4 14.7 15.5 15.2 ...\n  .. ..$ wt : num [1:14] 3.44 3.57 4.07 3.73 3.78 ...\n  .. ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language mpg ~ wt\n  .. .. .. ..- attr(*, \"variables\")= language list(mpg, wt)\n  .. .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. .. ..$ : chr [1:2] \"mpg\" \"wt\"\n  .. .. .. .. .. ..$ : chr \"wt\"\n  .. .. .. ..- attr(*, \"term.labels\")= chr \"wt\"\n  .. .. .. ..- attr(*, \"order\")= int 1\n  .. .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. .. ..- attr(*, \"response\")= int 1\n  .. .. .. ..- attr(*, \".Environment\")=&lt;environment: 0x0000023434d32ea0&gt; \n  .. .. .. ..- attr(*, \"predvars\")= language list(mpg, wt)\n  .. .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"wt\"\n  ..- attr(*, \"class\")= chr \"lm\"\n\nstr(models[[1]])\n\nList of 12\n $ coefficients : Named num [1:2] 39.57 -5.65\n  ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"wt\"\n $ residuals    : Named num [1:11] -3.6701 2.8428 1.0169 5.2523 -0.0513 ...\n  ..- attr(*, \"names\")= chr [1:11] \"Datsun 710\" \"Merc 240D\" \"Merc 230\" \"Fiat 128\" ...\n $ effects      : Named num [1:11] -88.433 10.171 0.695 6.231 1.728 ...\n  ..- attr(*, \"names\")= chr [1:11] \"(Intercept)\" \"wt\" \"\" \"\" ...\n $ rank         : int 2\n $ fitted.values: Named num [1:11] 26.5 21.6 21.8 27.1 30.5 ...\n  ..- attr(*, \"names\")= chr [1:11] \"Datsun 710\" \"Merc 240D\" \"Merc 230\" \"Fiat 128\" ...\n $ assign       : int [1:2] 0 1\n $ qr           :List of 5\n  ..$ qr   : num [1:11, 1:2] -3.317 0.302 0.302 0.302 0.302 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:11] \"Datsun 710\" \"Merc 240D\" \"Merc 230\" \"Fiat 128\" ...\n  .. .. ..$ : chr [1:2] \"(Intercept)\" \"wt\"\n  .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  ..$ qraux: num [1:2] 1.3 1.5\n  ..$ pivot: int [1:2] 1 2\n  ..$ tol  : num 1e-07\n  ..$ rank : int 2\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 9\n $ xlevels      : Named list()\n $ call         : language lm(formula = mpg ~ wt, data = .)\n $ terms        :Classes 'terms', 'formula'  language mpg ~ wt\n  .. ..- attr(*, \"variables\")= language list(mpg, wt)\n  .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:2] \"mpg\" \"wt\"\n  .. .. .. ..$ : chr \"wt\"\n  .. ..- attr(*, \"term.labels\")= chr \"wt\"\n  .. ..- attr(*, \"order\")= int 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=&lt;environment: 0x0000023438312f48&gt; \n  .. ..- attr(*, \"predvars\")= language list(mpg, wt)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"wt\"\n $ model        :'data.frame':  11 obs. of  2 variables:\n  ..$ mpg: num [1:11] 22.8 24.4 22.8 32.4 30.4 33.9 21.5 27.3 26 30.4 ...\n  ..$ wt : num [1:11] 2.32 3.19 3.15 2.2 1.61 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language mpg ~ wt\n  .. .. ..- attr(*, \"variables\")= language list(mpg, wt)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"mpg\" \"wt\"\n  .. .. .. .. ..$ : chr \"wt\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"wt\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: 0x0000023438312f48&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(mpg, wt)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"wt\"\n - attr(*, \"class\")= chr \"lm\"\n\nstr(summary(models[[1]]))\n\nList of 11\n $ call         : language lm(formula = mpg ~ wt, data = .)\n $ terms        :Classes 'terms', 'formula'  language mpg ~ wt\n  .. ..- attr(*, \"variables\")= language list(mpg, wt)\n  .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:2] \"mpg\" \"wt\"\n  .. .. .. ..$ : chr \"wt\"\n  .. ..- attr(*, \"term.labels\")= chr \"wt\"\n  .. ..- attr(*, \"order\")= int 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=&lt;environment: 0x0000023438312f48&gt; \n  .. ..- attr(*, \"predvars\")= language list(mpg, wt)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"wt\"\n $ residuals    : Named num [1:11] -3.6701 2.8428 1.0169 5.2523 -0.0513 ...\n  ..- attr(*, \"names\")= chr [1:11] \"Datsun 710\" \"Merc 240D\" \"Merc 230\" \"Fiat 128\" ...\n $ coefficients : num [1:2, 1:4] 39.57 -5.65 4.35 1.85 9.1 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:2] \"(Intercept)\" \"wt\"\n  .. ..$ : chr [1:4] \"Estimate\" \"Std. Error\" \"t value\" \"Pr(&gt;|t|)\"\n $ aliased      : Named logi [1:2] FALSE FALSE\n  ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"wt\"\n $ sigma        : num 3.33\n $ df           : int [1:3] 2 9 2\n $ r.squared    : num 0.509\n $ adj.r.squared: num 0.454\n $ fstatistic   : Named num [1:3] 9.32 1 9\n  ..- attr(*, \"names\")= chr [1:3] \"value\" \"numdf\" \"dendf\"\n $ cov.unscaled : num [1:2, 1:2] 1.701 -0.705 -0.705 0.308\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:2] \"(Intercept)\" \"wt\"\n  .. ..$ : chr [1:2] \"(Intercept)\" \"wt\"\n - attr(*, \"class\")= chr \"summary.lm\"\n\nmodels |&gt;\n  map(summary) |&gt; \n  map_dbl(\"r.squared\")\n\n        4         6         8 \n0.5086326 0.4645102 0.4229655 \n\n# can use integer to select elements by position\nx &lt;- list(list(1, 2, 3), list(4, 5, 6), list(7, 8, 9))\nx |&gt; map_dbl(2)\n\n[1] 2 5 8",
    "crumbs": [
      "Iteration"
    ]
  },
  {
    "objectID": "05_iteration.html#iterative-techniques-for-reading-multiple-files",
    "href": "05_iteration.html#iterative-techniques-for-reading-multiple-files",
    "title": "Iteration",
    "section": "Iterative techniques for reading multiple files",
    "text": "Iterative techniques for reading multiple files\n\nlibrary(readxl)\n\n# our usual path doesn't work with excel files\n# read_excel(\"https://joeroith.github.io/264_spring_2025/Data/gapminder/1952.xlsx\")\n\n# this will work if .xlsx files live in a Data folder that's at the same\n#   level as your .qmd file\ngap1952 &lt;- read_excel(\"Data/gapminder/1952.xlsx\")\ngap1957 &lt;- read_excel(\"Data/gapminder/1957.xlsx\")\n\nSince the 1952 and 1957 data have the same 5 columns, if we want to combine this data into a single data set showing time trends, we could simply bind_rows() (after adding a 6th column for year)\n\ngap1952 &lt;- gap1952 |&gt;\n  mutate(year = 1952)\ngap1957 &lt;- gap1957 |&gt;\n  mutate(year = 1957)\ngap_data &lt;- bind_rows(gap1952, gap1957)\n\nOf course, with 10 more years worth of data still left to read in and merge, this process could get pretty onerous. Section 26.3 shows how to automate this process in 3 steps:\n\nuse list.files() to list all the files in a directory\n\n\npaths &lt;- list.files(\"Data/gapminder\", pattern = \"[.]xlsx$\", full.names = TRUE)\npaths\n\n [1] \"Data/gapminder/1952.xlsx\" \"Data/gapminder/1957.xlsx\"\n [3] \"Data/gapminder/1962.xlsx\" \"Data/gapminder/1967.xlsx\"\n [5] \"Data/gapminder/1972.xlsx\" \"Data/gapminder/1977.xlsx\"\n [7] \"Data/gapminder/1982.xlsx\" \"Data/gapminder/1987.xlsx\"\n [9] \"Data/gapminder/1992.xlsx\" \"Data/gapminder/1997.xlsx\"\n[11] \"Data/gapminder/2002.xlsx\" \"Data/gapminder/2007.xlsx\"\n\n\n\nuse purrr::map() to read each of them into a list (we will discuss lists more in 06_data_types.qmd)\n\n\ngap_files &lt;- map(paths, readxl::read_excel)\nlength(gap_files)\n\n[1] 12\n\nstr(gap_files)\n\nList of 12\n $ : tibble [142 × 5] (S3: tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 28.8 55.2 43.1 30 62.5 ...\n  ..$ pop      : num [1:142] 8425333 1282697 9279525 4232095 17876956 ...\n  ..$ gdpPercap: num [1:142] 779 1601 2449 3521 5911 ...\n $ : tibble [142 × 5] (S3: tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 30.3 59.3 45.7 32 64.4 ...\n  ..$ pop      : num [1:142] 9240934 1476505 10270856 4561361 19610538 ...\n  ..$ gdpPercap: num [1:142] 821 1942 3014 3828 6857 ...\n $ : tibble [142 × 5] (S3: tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 32 64.8 48.3 34 65.1 ...\n  ..$ pop      : num [1:142] 10267083 1728137 11000948 4826015 21283783 ...\n  ..$ gdpPercap: num [1:142] 853 2313 2551 4269 7133 ...\n $ : tibble [142 × 5] (S3: tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 34 66.2 51.4 36 65.6 ...\n  ..$ pop      : num [1:142] 11537966 1984060 12760499 5247469 22934225 ...\n  ..$ gdpPercap: num [1:142] 836 2760 3247 5523 8053 ...\n $ : tibble [142 × 5] (S3: tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 36.1 67.7 54.5 37.9 67.1 ...\n  ..$ pop      : num [1:142] 13079460 2263554 14760787 5894858 24779799 ...\n  ..$ gdpPercap: num [1:142] 740 3313 4183 5473 9443 ...\n $ : tibble [142 × 5] (S3: tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 38.4 68.9 58 39.5 68.5 ...\n  ..$ pop      : num [1:142] 14880372 2509048 17152804 6162675 26983828 ...\n  ..$ gdpPercap: num [1:142] 786 3533 4910 3009 10079 ...\n $ : tibble [142 × 5] (S3: tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 39.9 70.4 61.4 39.9 69.9 ...\n  ..$ pop      : num [1:142] 12881816 2780097 20033753 7016384 29341374 ...\n  ..$ gdpPercap: num [1:142] 978 3631 5745 2757 8998 ...\n $ : tibble [142 × 5] (S3: tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 40.8 72 65.8 39.9 70.8 ...\n  ..$ pop      : num [1:142] 13867957 3075321 23254956 7874230 31620918 ...\n  ..$ gdpPercap: num [1:142] 852 3739 5681 2430 9140 ...\n $ : tibble [142 × 5] (S3: tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 41.7 71.6 67.7 40.6 71.9 ...\n  ..$ pop      : num [1:142] 16317921 3326498 26298373 8735988 33958947 ...\n  ..$ gdpPercap: num [1:142] 649 2497 5023 2628 9308 ...\n $ : tibble [142 × 5] (S3: tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 41.8 73 69.2 41 73.3 ...\n  ..$ pop      : num [1:142] 22227415 3428038 29072015 9875024 36203463 ...\n  ..$ gdpPercap: num [1:142] 635 3193 4797 2277 10967 ...\n $ : tibble [142 × 5] (S3: tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 42.1 75.7 71 41 74.3 ...\n  ..$ pop      : num [1:142] 25268405 3508512 31287142 10866106 38331121 ...\n  ..$ gdpPercap: num [1:142] 727 4604 5288 2773 8798 ...\n $ : tibble [142 × 5] (S3: tbl_df/tbl/data.frame)\n  ..$ country  : chr [1:142] \"Afghanistan\" \"Albania\" \"Algeria\" \"Angola\" ...\n  ..$ continent: chr [1:142] \"Asia\" \"Europe\" \"Africa\" \"Africa\" ...\n  ..$ lifeExp  : num [1:142] 43.8 76.4 72.3 42.7 75.3 ...\n  ..$ pop      : num [1:142] 31889923 3600523 33333216 12420476 40301927 ...\n  ..$ gdpPercap: num [1:142] 975 5937 6223 4797 12779 ...\n\ngap_files[[1]]   # pull off the first object in the list (i.e. 1952 data)\n\n# A tibble: 142 × 5\n   country     continent lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia         28.8  8425333      779.\n 2 Albania     Europe       55.2  1282697     1601.\n 3 Algeria     Africa       43.1  9279525     2449.\n 4 Angola      Africa       30.0  4232095     3521.\n 5 Argentina   Americas     62.5 17876956     5911.\n 6 Australia   Oceania      69.1  8691212    10040.\n 7 Austria     Europe       66.8  6927772     6137.\n 8 Bahrain     Asia         50.9   120447     9867.\n 9 Bangladesh  Asia         37.5 46886859      684.\n10 Belgium     Europe       68    8730405     8343.\n# ℹ 132 more rows\n\n\n\nuse purrr::list_rbind() to combine them into a single data frame\n\n\ngap_tidy &lt;- list_rbind(gap_files)\nclass(gap_tidy)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\ngap_tidy\n\n# A tibble: 1,704 × 5\n   country     continent lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia         28.8  8425333      779.\n 2 Albania     Europe       55.2  1282697     1601.\n 3 Algeria     Africa       43.1  9279525     2449.\n 4 Angola      Africa       30.0  4232095     3521.\n 5 Argentina   Americas     62.5 17876956     5911.\n 6 Australia   Oceania      69.1  8691212    10040.\n 7 Austria     Europe       66.8  6927772     6137.\n 8 Bahrain     Asia         50.9   120447     9867.\n 9 Bangladesh  Asia         37.5 46886859      684.\n10 Belgium     Europe       68    8730405     8343.\n# ℹ 1,694 more rows\n\n\nWe could even do all steps in a single pipeline:\n\nlist.files(\"Data/gapminder\", pattern = \"[.]xlsx$\", full.names = TRUE) |&gt;\n  map(readxl::read_excel) |&gt;\n  list_rbind()\n\n# A tibble: 1,704 × 5\n   country     continent lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia         28.8  8425333      779.\n 2 Albania     Europe       55.2  1282697     1601.\n 3 Algeria     Africa       43.1  9279525     2449.\n 4 Angola      Africa       30.0  4232095     3521.\n 5 Argentina   Americas     62.5 17876956     5911.\n 6 Australia   Oceania      69.1  8691212    10040.\n 7 Austria     Europe       66.8  6927772     6137.\n 8 Bahrain     Asia         50.9   120447     9867.\n 9 Bangladesh  Asia         37.5 46886859      684.\n10 Belgium     Europe       68    8730405     8343.\n# ℹ 1,694 more rows\n\n\nNote that we are lacking a 6th column with the year represented by each row of data. Here is one way to solve that issue:\n\n# This extracts file names, which are carried along as data frame names by\n#   map functions\npaths |&gt; set_names(basename)\n\n                 1952.xlsx                  1957.xlsx \n\"Data/gapminder/1952.xlsx\" \"Data/gapminder/1957.xlsx\" \n                 1962.xlsx                  1967.xlsx \n\"Data/gapminder/1962.xlsx\" \"Data/gapminder/1967.xlsx\" \n                 1972.xlsx                  1977.xlsx \n\"Data/gapminder/1972.xlsx\" \"Data/gapminder/1977.xlsx\" \n                 1982.xlsx                  1987.xlsx \n\"Data/gapminder/1982.xlsx\" \"Data/gapminder/1987.xlsx\" \n                 1992.xlsx                  1997.xlsx \n\"Data/gapminder/1992.xlsx\" \"Data/gapminder/1997.xlsx\" \n                 2002.xlsx                  2007.xlsx \n\"Data/gapminder/2002.xlsx\" \"Data/gapminder/2007.xlsx\" \n\n# The middle line ensures that each of the 12 data frames in the list for\n#   gap_files has a name determined by its filepath, unlike the gap_files\n#   we created in step 2 above, which had no names (we could only identify\n#   data frames by their position)\ngap_files &lt;- paths |&gt; \n  set_names(basename) |&gt;  \n  map(readxl::read_excel)\n\n# Now we can extract a particular year by its name:\ngap_files[[\"1962.xlsx\"]]\n\n# A tibble: 142 × 5\n   country     continent lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia         32.0 10267083      853.\n 2 Albania     Europe       64.8  1728137     2313.\n 3 Algeria     Africa       48.3 11000948     2551.\n 4 Angola      Africa       34    4826015     4269.\n 5 Argentina   Americas     65.1 21283783     7133.\n 6 Australia   Oceania      70.9 10794968    12217.\n 7 Austria     Europe       69.5  7129864    10751.\n 8 Bahrain     Asia         56.9   171863    12753.\n 9 Bangladesh  Asia         41.2 56839289      686.\n10 Belgium     Europe       70.2  9218400    10991.\n# ℹ 132 more rows\n\n# Finally, take advantage of the `names_to` argument in list_rbind to \n#   create that 6th column with `year`\ngap_tidy &lt;- paths |&gt; \n  set_names(basename) |&gt; \n  map(readxl::read_excel) |&gt; \n  list_rbind(names_to = \"year\") |&gt; \n  mutate(year = parse_number(year))\n\nYou could then save your result using write_csv so you don’t have to run the reading and wrangling code every time!",
    "crumbs": [
      "Iteration"
    ]
  },
  {
    "objectID": "05_iteration.html#on-your-own",
    "href": "05_iteration.html#on-your-own",
    "title": "Iteration",
    "section": "On Your Own",
    "text": "On Your Own\n\nCompute the mean of every column of the mtcars data set using (a) a for loop, (b) a map function, (c) the across() function, and (d) pivot_longer().\nWrite a function that prints the mean of each numeric column in a data frame. Try it on the iris data set. (Hint: keep(is.numeric))\nEliminate the for loop in each of the following examples by taking advantage of an existing function that works with vectors:\n\n\nout &lt;- \"\"\nfor (x in letters) {\n  out &lt;- stringr::str_c(out, x)\n}\nout\n\n[1] \"abcdefghijklmnopqrstuvwxyz\"\n\nx &lt;- runif(100)\nout &lt;- vector(\"numeric\", length(x))\nout[1] &lt;- x[1]\nfor (i in 2:length(x)) {\n  out[i] &lt;- out[i - 1] + x[i]\n}\nout\n\n  [1]  0.1639015  1.0691047  1.5480135  1.8683295  2.6368307  2.7140133\n  [7]  3.0645709  3.1652381  3.4948846  4.3134845  4.5183310  4.5786219\n [13]  4.7382033  5.5234287  6.1444381  6.4200471  7.3656024  7.9586759\n [19]  8.2884381  9.0403805  9.2447985  9.9412414 10.6403278 10.7210839\n [25] 11.2760369 11.9051739 12.6363257 12.9834102 13.2217874 13.3751490\n [31] 13.6313981 13.6515197 14.4228355 14.8779463 15.0112400 15.2449622\n [37] 15.6812513 16.3589659 17.1779975 17.8331870 18.5487768 19.1013221\n [43] 19.2062564 19.8583187 20.7639967 20.9882819 21.1981665 22.1394593\n [49] 22.2201447 22.8005868 23.2438049 24.1490906 24.6680300 24.8530909\n [55] 25.3312381 26.0847078 26.5177753 27.5103946 28.2507019 28.4449144\n [61] 28.5275117 29.1711222 30.0197209 30.2016165 30.5724364 30.6712924\n [67] 30.8817354 31.4114018 32.0929357 32.1681365 32.5529930 33.4221146\n [73] 33.5290241 33.8108513 33.9401007 33.9757216 34.3482168 34.8319540\n [79] 35.5279099 35.9084711 36.3252878 36.4865327 36.6228728 37.3873802\n [85] 37.8301832 37.9238796 38.5388256 38.7217778 38.9816836 39.1695535\n [91] 39.4724541 40.3275442 40.6950273 41.1742289 41.9791605 42.7716216\n [97] 43.6196722 43.7460805 43.8028619 44.4650194\n\n\n\nCompute the number of unique values in each column of the iris data set using at least 2 of your favorite iteration methods. Bonus points if you can use pivot_longer()!\nCarefully explain each step in the pipeline below:\n\n\nshow_missing &lt;- function(df, group_vars, summary_vars = everything()) {\n  df |&gt; \n    group_by(pick({{ group_vars }})) |&gt; \n    summarize(\n      across({{ summary_vars }}, \\(x) sum(is.na(x))),\n      .groups = \"drop\"\n    ) |&gt;\n    select(where(\\(x) any(x &gt; 0)))\n}\nnycflights13::flights |&gt; show_missing(c(year, month, day))\n\n# A tibble: 365 × 9\n    year month   day dep_time dep_delay arr_time arr_delay tailnum air_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;     &lt;int&gt;    &lt;int&gt;     &lt;int&gt;   &lt;int&gt;    &lt;int&gt;\n 1  2013     1     1        4         4        5        11       0       11\n 2  2013     1     2        8         8       10        15       2       15\n 3  2013     1     3       10        10       10        14       2       14\n 4  2013     1     4        6         6        6         7       2        7\n 5  2013     1     5        3         3        3         3       1        3\n 6  2013     1     6        1         1        1         3       0        3\n 7  2013     1     7        3         3        3         3       1        3\n 8  2013     1     8        4         4        4         7       1        7\n 9  2013     1     9        5         5        7         9       2        9\n10  2013     1    10        3         3        3         3       2        3\n# ℹ 355 more rows\n\n\n\nWrite a function called summary_stats() that allows a user to input a tibble, numeric variables in that tibble, and summary statistics that they would like to see for each variable. Using across(), the function’s output should look like the example below.\n\n\nsummary_stats(mtcars, \n              vars = c(mpg, hp, wt), \n              stat_fcts = list(mean = mean, \n                               median = median, \n                               sd = sd, \n                               IQR = IQR))\n\n#  mpg_mean mpg_median   mpg_sd mpg_IQR  hp_mean hp_median    hp_sd hp_IQR\n#1 20.09062       19.2 6.026948   7.375 146.6875       123 68.56287   83.5\n#  wt_mean wt_median     wt_sd  wt_IQR  n\n#1 3.21725     3.325 0.9784574 1.02875 32\n\n\nThe power of a statistical test is the probability that it rejects the null hypothesis when the null hypothesis is false. In other words, it’s the probability that a statistical test can detect when a true difference exists. The power depends on a number of factors, including:\n\n\nsample size\ntype I error level (probability of declaring there is a statistically significant difference when there really isn’t)\nvariability in the data\nsize of the true difference\n\nThe following steps can be followed to simulate a power calculation using iteration techniques:\n\ngenerate simulated data where there is a true difference or effect\nrun your desired test on the simulated data and record if the null hypothesis was rejected or not (i.e. if the p-value was below .05)\nrepeat (a)-(b) a large number of times and record the total proportion of times that the null hypothesis was rejected; that proportion is the power of your test under those conditions\n\nCreate a power curve for a two-sample t-test by filling in Step C below and then removing eval: FALSE:\n\n# Step A\n\n# set parameters for two-sample t-test\nmean1 &lt;- 100   # mean response in Group 1\ntruediff &lt;- 5    # true mean difference between Groups 1 and 2\nmean2 &lt;- mean1 + truediff   # mean response in Group 2\nsd1 &lt;- 10   # standard deviation in Group 1\nsd2 &lt;- 10   # standard deviation in Group 2\nn1 &lt;- 20    # sample size in Group 1\nn2 &lt;- 20    # sample size in Group 2\nnumsims &lt;- 1000   # number of simulations (iterations) to run\n\n# generate sample data for Groups 1 and 2 based on normal distributions\n#   with the parameters above (note that there is truly a difference in means!)\nsamp1 &lt;- rnorm(n1, mean1, sd1)\nsamp2 &lt;- rnorm(n2, mean2, sd2)\n\n# organize the simulated data into a tibble\nsim_data &lt;- tibble(response = c(samp1, samp2), \n       group = c(rep(\"Group 1\", n1), rep(\"Group 2\", n2)))\nsim_data\n\n# Step B\n\n# exploratory analysis of the simulated data\nmosaic::favstats(response ~ group, data = sim_data)\nggplot(sim_data, aes(x = response, y = group)) +\n  geom_boxplot()\n\n# run a two-sample t-test to see if there is a significant difference\n#   in means between Groups 1 and 2 (i.e. is the p-value &lt; .05?)\np_value &lt;- t.test(x = samp1, y = samp2)$p.value\np_value\np_value &lt; .05   # if TRUE, then we reject the null hypothesis and conclude\n                #   there is a statistically significant difference\n\n# Step C\n\n# find the power = proportion of time null is rejected when\n#   true difference is not 0 (i.e. number of simulated data sets that\n#   result in p-values below .05)",
    "crumbs": [
      "Iteration"
    ]
  },
  {
    "objectID": "miniproject2.html",
    "href": "miniproject2.html",
    "title": "Mini-Project 2: Data Acquisition",
    "section": "",
    "text": "Overview\nIn this project, you will find data on the web, acquire it through API or web scraping techniques, and create a tidy tibble suitable for future analysis\n\nGroups\n\nShen and Tenzin\nGwynnie and Aria\nJa Seng and Sujita\nRachael and Olivia\nMorgan, Solveig, and Ela\nParker and Josh\nJean-Luc and Jordan\nDaniel and Cathal\n\n\n\nTimeline\n\n\n\n\n\nTentative due date\nPoints\n\n\n\n\nStage I\nGitHub Collaboration\nThurs, Mar 13\n10\n\n\nStage II\nProject Submission\nFriday, Mar 21\n90\n\n\n\n\n\n100\n\n\n\n\nNotes: a) we will not be doing an oral assessment piece, but we will be doing partner evaluations b) there will be some time provided in class, but you will be expected to work outside of class time with your partner as well\n\n\n\n\nStage I: GitHub Collaboration\nYou and your partner must (a) set up a GitHub repository where one person is the Owner and the other is a Collaborator, (b) connect the GitHub repository to an R project that you edit in your own RStudio on your laptop, and (c) certify that you and your partner can handle merge conflicts, should they arise.\nTo obtain your “certification” in (c), follow the instructions here while sitting with your partner. Read the entire Chapter 10, and then specifically follow Steps 1-6 in Section 10.2 (don’t worry about the Exercise box that follows Step 6) and then follow Steps 1-11 in Section 10.4 (again don’t worry about the Exercise box that follows Step 11). To receive your “certification”, you will upload screenshots as in Step 11 but with details from you and your partner.\nThe following notes may help as your working through the instructions in Chapter 10:\n\nYou can enter git config pull.rebase false in the Terminal tab in RStudio\nThe Owner needs to check “set up ReadMe” when creating the initial repo\nBoth the Owner and Collaborator need to connect to GitHub through RStudio (using File &gt; New Project &gt; Version Control) after the Owner sets up the initial GitHub repo. Keep checking that RStudio is the same at various stages.\nBefore Step 4 (in Part 1), the Owner may have to remove the .Rproj and .gitignore folders, and they may also have to reload GitHub\nIn Step 8 (Part 2), click Stage even though it looks like the blue box is filled\n\n\n\nStage II: Project Submission\nFor this project, you and your partner will find data on the web that’s not available as a neat .csv file or .xls spreadsheet (it maybe even be spread across multiple webpages), and you will acquire it using techniques from class and tidy it so it’s suitable for a future data analysis (including text analysis!).\nYou must use one (or multiple) of these approaches to data acquisition:\n\nAPIs (using httr or httr2)\nAn API wrapper package (that requires an API key)\nHarvesting data that appears in table form on a webpage (using rvest with html_table)\nHarvesting pieces of data that could appear anywhere on a webpage (using rvest with html_text)\n\nBefore you begin acquiring data, describe your motivations for obtaining the data you plan to gather, including questions you hope to answer. After collecting and tidying your data, describe how you might use your data to investigate questions of interest.\nYour quarto file should contain at least one custom function and possibly iteration techniques as well. Be sure that it is well commented and that it conforms to style guidelines.\n\n\nSubmission and Rubric\nMini-Project 2 must be submitted on Moodle by 11:00 PM on Fri, Mar 21. You should submit a rendered pdf.\nCheck out this rubric for Mini-Project 2.",
    "crumbs": [
      "Mini-Project 2: Data Acquisition"
    ]
  },
  {
    "objectID": "07_apis.html",
    "href": "07_apis.html",
    "title": "Data Acquisition with APIs in R",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\nCredit to Brianna Heggeseth and Leslie Myint from Macalester College for a few of these descriptions and examples.",
    "crumbs": [
      "Data Acquisition with APIs in R"
    ]
  },
  {
    "objectID": "07_apis.html#getting-data-from-websites",
    "href": "07_apis.html#getting-data-from-websites",
    "title": "Data Acquisition with APIs in R",
    "section": "Getting data from websites",
    "text": "Getting data from websites",
    "crumbs": [
      "Data Acquisition with APIs in R"
    ]
  },
  {
    "objectID": "07_apis.html#wrapper-packages",
    "href": "07_apis.html#wrapper-packages",
    "title": "Data Acquisition with APIs in R",
    "section": "Wrapper packages",
    "text": "Wrapper packages\nIn R, it is easiest to use Web APIs through a wrapper package, an R package written specifically for a particular Web API.\n\nThe R development community has already contributed wrapper packages for many large Web APIs (e.g. ZillowR, rtweet, genius, Rspotify, tidycensus, etc.)\nTo find a wrapper package, search the web for “R package” and the name of the website. For example:\n\nSearching for “R Reddit package” returns RedditExtractor\nSearching for “R Weather.com package” returns weatherData\n\nrOpenSci also has a good collection of wrapper packages.\n\nIn particular, tidycensus is a wrapper package that makes it easy to obtain desired census information for mapping and modeling:\nObtaining raw data from the Census Bureau was that easy! Often we will have to obtain and use a secret API key to access the data, but that’s not always necessary with tidycensus.\nNow we can tidy that data and produce plots and analyses. Here’s a decent place to get more information about the variable codes.\n\n# Rename cryptic variables from the census form\nsample_acs_data &lt;- sample_acs_data |&gt;\n  rename(population = B01003_001E,\n         population_moe = B01003_001M,\n         median_income = B19013_001E,\n         median_income_moe = B19013_001M)\n\n# Plot with geom_sf since our data contains 1 row per census tract\n#   with its geometry\nggplot(data = sample_acs_data) + \n  geom_sf(aes(fill = median_income), colour = \"white\", linetype = 2) + \n  theme_void()  \n\n\n\n\n\n\n\n# The whole state of MN is overwhelming, so focus on a single county\nsample_acs_data |&gt;\n  filter(str_detect(NAME, \"Ramsey\")) |&gt;\n  ggplot() + \n    geom_sf(aes(fill = median_income), colour = \"white\", linetype = 2)\n\n\n\n\n\n\n\n# Look for relationships between variables with 1 row per tract\nas_tibble(sample_acs_data) |&gt;\n  ggplot(aes(x = population, y = median_income)) + \n    geom_point() + \n    geom_smooth(method = \"lm\")  \n\n\n\n\n\n\n\n\nExtra resources:\n\ntidycensus: wrapper package that provides an interface to a few census datasets with map geometry included!\n\nFull documentation is available at https://walker-data.com/tidycensus/\n\ncensusapi: wrapper package that offers an interface to all census datasets\n\nFull documentation is available at https://www.hrecht.com/censusapi/\n\n\nget_acs() is one of the functions that is part of tidycensus. Let’s explore what’s going on behind the scenes with get_acs()…",
    "crumbs": [
      "Data Acquisition with APIs in R"
    ]
  },
  {
    "objectID": "07_apis.html#accessing-web-apis-directly",
    "href": "07_apis.html#accessing-web-apis-directly",
    "title": "Data Acquisition with APIs in R",
    "section": "Accessing web APIs directly",
    "text": "Accessing web APIs directly\n\nGetting a Census API key\nMany APIs (and their wrapper packages) require users to obtain a key to use their services.\n\nThis lets organizations keep track of what data is being used.\nIt also rate limits their API and ensures programs don’t make too many requests per day/minute/hour. Be aware that most APIs do have rate limits — especially for their free tiers.\n\nNavigate to https://api.census.gov/data/key_signup.html to obtain a Census API key:\n\nOrganization: St. Olaf College\nEmail: Your St. Olaf email address\n\nYou will get the message:\n\nYour request for a new API key has been successfully submitted. Please check your email. In a few minutes you should receive a message with instructions on how to activate your new key.\n\nCheck your email. Copy and paste your key into a new text file:\n\n(In RStudio) File &gt; New File &gt; Text File (towards the bottom of the menu)\nSave as census_api_key.txt in the same folder as this .qmd.\n\nYou could then read in the key with code like this:\n\n#myapikey &lt;- readLines(\"C:/Users/roith1/Documents/R/SDS264/Inclass_264/census_api_key.txt\")\n\n\n\nHandling API keys\nWhile this works, the problem is once we start backing up our files to GitHub, your API key will also appear on GitHub, and you want to keep your API key secret. Thus, we might use environment variables instead:\nOne way to store a secret across sessions is with environment variables. Environment variables, or envvars for short, are a cross platform way of passing information to processes. For passing envvars to R, you can list name-value pairs in a file called .Renviron in your home directory. The easiest way to edit it is to run:\n\nfile.edit(\"~/.Renviron\")\n\nSys.setenv(PATH = \"path\", VAR1 = \"value1\", VAR2 = \"value2\")\n\nThe file looks something like\nPATH = “path” VAR1 = “value1” VAR2 = “value2” And you can access the values in R using Sys.getenv():\n\nSys.getenv(\"VAR1\")\n#&gt; [1] \"value1\"\n\nNote that .Renviron is only processed on startup, so you’ll need to restart R to see changes.\nAnother option is to use Sys.setenv and Sys.getenv:\n\n# I used the first line to store my CENSUS API key in .Renviron\n#   after uncommenting - should only need to run one time\n# Sys.setenv(CENSUS_API_KEY = \"my personal key\")\n# my_census_api_key &lt;- Sys.getenv(\"CENSUS_API_KEY\")\n\n\n\nNavigating API documentation\nNavigate to the Census API user guide and click on the “Example API Queries” tab.\nLet’s look at the Population Estimates Example and the American Community Survey (ACS) Example. These examples walk us through the steps to incrementally build up a URL to obtain desired data. This URL is known as a web API request.\nhttps://api.census.gov/data/2019/acs/acs1?get=NAME,B02015_009E,B02015_009M&for=state:*\n\nhttps://api.census.gov: This is the base URL.\n\nhttp://: The scheme, which tells your browser or program how to communicate with the web server. This will typically be either http: or https:.\napi.census.gov: The hostname, which is a name that identifies the web server that will process the request.\n\ndata/2019/acs/acs1: The path, which tells the web server how to get to the desired resource.\n\nIn the case of the Census API, this locates a desired dataset in a particular year.\nOther APIs allow search functionality. (e.g., News organizations have article searches.) In these cases, the path locates the search function we would like to call.\n\n?get=NAME,B02015_009E,B02015_009M&for=state:*: The query parameters, which provide the parameters for the function you would like to call.\n\nWe can view this as a string of key-value pairs separated by &. That is, the general structure of this part is key1=value1&key2=value2.\n\n\n\n\n\nkey\nvalue\n\n\n\n\nget\nNAME,B02015_009E,B02015_009M\n\n\nfor\nstate:*\n\n\n\nTypically, each of these URL components will be specified in the API documentation. Sometimes, the scheme, hostname, and path (https://api.census.gov/data/2019/acs/acs1) will be referred to as the endpoint for the API call.\nWe will first use the httr2 package to build up a full URL from its parts.\n\nrequest() creates an API request object using the base URL\nreq_url_path_append() builds up the URL by adding path components separated by /\nreq_url_query() adds the ? separating the endpoint from the query and sets the key-value pairs in the query\n\nThe .multi argument controls how multiple values for a given key are combined.\nThe I() function around \"state:*\" inhibits parsing of special characters like : and *. (It’s known as the “as-is” function.)\nThe backticks around for are needed because for is a reserved word in R (for for-loops). You’ll need backticks whenever the key name has special characters (like spaces, dashes).\nWe can see from here that providing an API key is achieved with key=YOUR_API_KEY.\n\n\n\n# Request total number of Hmong residents and margin of error by state\n#   in 2019, as in the User Guide\nCENSUS_API_KEY &lt;- Sys.getenv(\"CENSUS_API_KEY\")\nreq &lt;- request(\"https://api.census.gov\") |&gt; \n    req_url_path_append(\"data\") |&gt; \n    req_url_path_append(\"2019\") |&gt; \n    req_url_path_append(\"acs\") |&gt; \n    req_url_path_append(\"acs1\") |&gt; \n    req_url_query(get = c(\"NAME\", \"B02015_009E\", \"B02015_009M\"), `for` = I(\"state:*\"), key = CENSUS_API_KEY, .multi = \"comma\")\n\nWhy would we ever use these steps instead of just using the full URL as a string?\n\nTo generalize this code with functions! (This is exactly what wrapper packages do.)\nTo handle special characters\n\ne.g., query parameters might have spaces, which need to be represented in a particular way in a URL (URLs can’t contain spaces)\n\n\nOnce we’ve fully constructed our request, we can use req_perform() to send out the API request and get a response.\n\nresp &lt;- req_perform(req)\nresp\n\nWe see from Content-Type that the format of the response is something called JSON. We can navigate to the request URL to see the structure of this output.\n\nJSON (Javascript Object Notation) is a nested structure of key-value pairs.\nWe can use resp_body_json() to parse the JSON into a nicer format.\n\nWithout simplifyVector = TRUE, the JSON is read in as a list.\n\n\n\nresp_json_list &lt;- resp |&gt; resp_body_json()\nhead(resp_json_list, 2)\n\n[[1]]\n[[1]][[1]]\n[1] \"NAME\"\n\n[[1]][[2]]\n[1] \"B02015_009E\"\n\n[[1]][[3]]\n[1] \"B02015_009M\"\n\n[[1]][[4]]\n[1] \"state\"\n\n\n[[2]]\n[[2]][[1]]\n[1] \"Illinois\"\n\n[[2]][[2]]\n[1] \"655\"\n\n[[2]][[3]]\n[1] \"511\"\n\n[[2]][[4]]\n[1] \"17\"\n\nresp_json_df &lt;- resp |&gt; resp_body_json(simplifyVector = TRUE)\nhead(resp_json_df)\n\n     [,1]       [,2]          [,3]          [,4]   \n[1,] \"NAME\"     \"B02015_009E\" \"B02015_009M\" \"state\"\n[2,] \"Illinois\" \"655\"         \"511\"         \"17\"   \n[3,] \"Georgia\"  \"3162\"        \"1336\"        \"13\"   \n[4,] \"Idaho\"    NA            NA            \"16\"   \n[5,] \"Hawaii\"   \"56\"          \"92\"          \"15\"   \n[6,] \"Indiana\"  \"1344\"        \"1198\"        \"18\"   \n\nresp_json_df &lt;- janitor::row_to_names(resp_json_df, 1)\nhead(resp_json_df)\n\n     NAME       B02015_009E B02015_009M state\n[1,] \"Illinois\" \"655\"       \"511\"       \"17\" \n[2,] \"Georgia\"  \"3162\"      \"1336\"      \"13\" \n[3,] \"Idaho\"    NA          NA          \"16\" \n[4,] \"Hawaii\"   \"56\"        \"92\"        \"15\" \n[5,] \"Indiana\"  \"1344\"      \"1198\"      \"18\" \n[6,] \"Iowa\"     \"685\"       \"705\"       \"19\" \n\n\nAll right, let’s try this! First we’ll grab total population and median household income for all census tracts in MN using 3 approaches\n\n# Next using httr2\nreq &lt;- request(\"https://api.census.gov\") |&gt; \n    req_url_path_append(\"data\") |&gt; \n    req_url_path_append(\"2020\") |&gt; \n    req_url_path_append(\"acs\") |&gt; \n    req_url_path_append(\"acs5\") |&gt; \n    req_url_query(get = c(\"NAME\", \"B01003_001E\", \"B19013_001E\"), `for` = I(\"tract:*\"), `in` = I(\"state:27\"), `in` = I(\"county:053\"), key = CENSUS_API_KEY, .multi = \"comma\")\n\nresp &lt;- req_perform(req)\nresp\nresp_json_df &lt;- resp |&gt; resp_body_json(simplifyVector = TRUE)\nhead(resp_json_df)\n\n     [,1]                                            [,2]         \n[1,] \"NAME\"                                          \"B01003_001E\"\n[2,] \"Census Tract 1.01, Hennepin County, Minnesota\" \"3472\"       \n[3,] \"Census Tract 1.02, Hennepin County, Minnesota\" \"4992\"       \n[4,] \"Census Tract 3, Hennepin County, Minnesota\"    \"3404\"       \n[5,] \"Census Tract 6.01, Hennepin County, Minnesota\" \"4706\"       \n[6,] \"Census Tract 6.03, Hennepin County, Minnesota\" \"3301\"       \n     [,3]          [,4]    [,5]     [,6]    \n[1,] \"B19013_001E\" \"state\" \"county\" \"tract\" \n[2,] \"70927\"       \"27\"    \"053\"    \"000101\"\n[3,] \"46333\"       \"27\"    \"053\"    \"000102\"\n[4,] \"82098\"       \"27\"    \"053\"    \"000300\"\n[5,] \"71122\"       \"27\"    \"053\"    \"000601\"\n[6,] \"96875\"       \"27\"    \"053\"    \"000603\"\n\nresp_json_df &lt;- janitor::row_to_names(resp_json_df, 1)\nhead(resp_json_df)\n\n     NAME                                            B01003_001E B19013_001E\n[1,] \"Census Tract 1.01, Hennepin County, Minnesota\" \"3472\"      \"70927\"    \n[2,] \"Census Tract 1.02, Hennepin County, Minnesota\" \"4992\"      \"46333\"    \n[3,] \"Census Tract 3, Hennepin County, Minnesota\"    \"3404\"      \"82098\"    \n[4,] \"Census Tract 6.01, Hennepin County, Minnesota\" \"4706\"      \"71122\"    \n[5,] \"Census Tract 6.03, Hennepin County, Minnesota\" \"3301\"      \"96875\"    \n[6,] \"Census Tract 11, Hennepin County, Minnesota\"   \"2004\"      \"69509\"    \n     state county tract   \n[1,] \"27\"  \"053\"  \"000101\"\n[2,] \"27\"  \"053\"  \"000102\"\n[3,] \"27\"  \"053\"  \"000300\"\n[4,] \"27\"  \"053\"  \"000601\"\n[5,] \"27\"  \"053\"  \"000603\"\n[6,] \"27\"  \"053\"  \"001100\"\n\nhennepin_httr2 &lt;- as_tibble(resp_json_df) |&gt;\n  mutate(population = parse_number(B01003_001E),\n         median_income = parse_number(B19013_001E)) |&gt;\n  select(-B01003_001E, -B19013_001E, -state, -county)\n  \nhennepin_httr2 |&gt;\n  ggplot(aes(x = population, y = median_income)) + \n    geom_point() + \n    geom_smooth(method = \"lm\")  \n\n\n\n\n\n\n\nsummary(hennepin_httr2$population)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0    2876    3714    3815    4651    9680 \n\nsummary(hennepin_httr2$median_income)\n\n      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n-666666666      61354      80966   -3966166     107232     250001 \n\nsort(hennepin_httr2$population)\n\n  [1]    0  223 1514 1622 1672 1760 1766 1779 1798 1844 1848 1877 1897 1915 1926\n [16] 1935 1942 1973 2000 2004 2012 2013 2017 2038 2058 2061 2067 2092 2111 2123\n [31] 2130 2150 2163 2228 2235 2256 2272 2274 2280 2283 2295 2315 2339 2341 2357\n [46] 2399 2415 2416 2419 2460 2462 2476 2484 2499 2511 2511 2528 2532 2551 2570\n [61] 2594 2605 2625 2656 2658 2668 2670 2675 2681 2724 2738 2756 2763 2780 2796\n [76] 2808 2820 2822 2837 2848 2853 2865 2876 2878 2916 2935 2944 2950 2954 2969\n [91] 2971 2984 2994 3001 3036 3037 3038 3046 3047 3048 3075 3077 3119 3124 3127\n[106] 3138 3150 3152 3162 3168 3193 3222 3224 3224 3225 3236 3251 3274 3298 3301\n[121] 3305 3317 3317 3326 3331 3335 3341 3364 3372 3376 3379 3386 3404 3404 3418\n[136] 3431 3439 3444 3454 3466 3472 3474 3486 3498 3512 3513 3557 3573 3574 3575\n[151] 3585 3607 3628 3631 3634 3654 3656 3666 3671 3673 3676 3687 3703 3710 3714\n[166] 3739 3750 3762 3764 3765 3799 3801 3805 3806 3808 3810 3811 3829 3832 3842\n[181] 3853 3862 3877 3885 3890 3895 3896 3896 3903 3903 3913 3924 3930 3960 3967\n[196] 3972 3974 3976 3978 3980 3989 3995 4008 4010 4013 4025 4036 4063 4086 4097\n[211] 4098 4126 4132 4179 4200 4219 4228 4237 4273 4286 4295 4305 4319 4321 4326\n[226] 4355 4359 4366 4371 4378 4385 4412 4441 4455 4460 4466 4472 4481 4503 4535\n[241] 4584 4587 4591 4613 4622 4629 4651 4665 4671 4678 4693 4696 4706 4713 4718\n[256] 4728 4747 4767 4769 4789 4789 4815 4855 4855 4874 4899 4919 4930 4972 4978\n[271] 4983 4992 5030 5033 5041 5065 5085 5099 5107 5150 5195 5213 5244 5262 5267\n[286] 5295 5305 5313 5364 5366 5385 5386 5415 5442 5459 5507 5510 5515 5541 5541\n[301] 5587 5709 5725 5781 5821 5831 5872 5880 5980 6025 6069 6071 6102 6113 6166\n[316] 6229 6249 6258 6265 6308 6482 6595 6709 6928 7286 7604 7828 9486 9680\n\nsort(hennepin_httr2$median_income)\n\n  [1] -666666666 -666666666      14748      20000      22768      23256\n  [7]      23391      25708      31513      31981      32321      32758\n [13]      34273      35368      35855      36700      37315      37346\n [19]      37413      38286      38554      39420      39605      39609\n [25]      39630      40400      40476      40603      40867      42426\n [31]      42550      42753      43036      43750      44867      45640\n [37]      46157      46333      46596      47139      47197      47688\n [43]      47857      48464      48690      48750      49028      49139\n [49]      49659      50000      50741      50755      50935      51250\n [55]      51513      51705      51923      52169      52304      52370\n [61]      52781      52917      53393      53542      53564      53952\n [67]      54026      54636      55321      55430      55833      56338\n [73]      56955      57469      57802      57875      58426      59013\n [79]      59704      59876      60375      61213      61354      61547\n [85]      62188      62279      62404      62426      62770      63750\n [91]      63990      64250      64333      64621      64676      64792\n [97]      65323      65329      65395      65455      65590      65772\n[103]      66364      66452      66549      66875      67102      67132\n[109]      67473      67614      68114      68158      68369      68417\n[115]      68434      68796      68913      68971      69509      69600\n[121]      70089      70927      70970      71071      71122      71146\n[127]      71250      71670      71818      72054      72102      72766\n[133]      72853      73482      73514      73527      73897      73984\n[139]      74286      74330      74817      75147      75556      75833\n[145]      76111      76164      76417      76792      76839      77500\n[151]      78137      78171      78333      78418      78509      78605\n[157]      78728      79167      79191      79366      79750      80012\n[163]      80080      80350      80966      81341      81341      81411\n[169]      81977      82014      82098      82340      82527      83090\n[175]      83250      83315      83380      84063      84569      84583\n[181]      84792      85078      85221      85938      86106      86111\n[187]      86904      87054      87390      87426      87599      87857\n[193]      88431      88542      88895      89417      89740      89792\n[199]      89891      89922      90167      91230      91250      91333\n[205]      91637      91827      92019      92683      92941      93011\n[211]      93750      94656      95750      95855      95980      96328\n[217]      96378      96667      96856      96875      96983      97609\n[223]      98137      98550      98986      99792      99853     100054\n[229]     100329     100652     100761     101156     101194     101440\n[235]     101578     103049     103531     103611     103750     104242\n[241]     104306     104412     104795     104904     106310     106518\n[247]     107232     107303     108476     108510     109722     110125\n[253]     110339     110694     110729     110774     111364     111635\n[259]     111950     112104     112557     112566     113563     113750\n[265]     114550     115934     116281     116861     117631     118333\n[271]     118594     118697     118828     119214     119821     120769\n[277]     122180     122206     123312     125750     126250     127375\n[283]     127396     130404     130486     131023     131042     132361\n[289]     132604     133333     133472     133504     133859     134250\n[295]     136012     136369     138848     141528     141984     142500\n[301]     142889     143125     143744     143935     144282     144318\n[307]     146328     147237     147672     148512     148611     149934\n[313]     153917     154306     159857     161458     161471     165865\n[319]     176580     178259     179743     179926     180463     185357\n[325]     194417     194882     200438     202098     250001\n\nhennepin_httr2 &lt;- hennepin_httr2 |&gt;\n  mutate(median_income = ifelse(median_income &gt; 0, median_income, NA),\n         population = ifelse(population &gt; 0, population, NA))\n  \nhennepin_httr2 |&gt;\n  ggplot(aes(x = population, y = median_income)) + \n    geom_point() + \n    geom_smooth(method = \"lm\")  \n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n# To make choropleth map by census tract, would need to download US Census\n#   Bureau TIGER geometries using tigris package\n\n\n# Finally using httr\nurl &lt;- str_c(\"https://api.census.gov/data/2020/acs/acs5?get=NAME,B01003_001E,B19013_001E&for=tract:*&in=state:27&in=county:053\", \"&key=\", CENSUS_API_KEY)\nacs5 &lt;- GET(url)\ndetails &lt;- content(acs5, \"parsed\")\n# details \ndetails[[1]]  # variable names\n\n[[1]]\n[1] \"NAME\"\n\n[[2]]\n[1] \"B01003_001E\"\n\n[[3]]\n[1] \"B19013_001E\"\n\n[[4]]\n[1] \"state\"\n\n[[5]]\n[1] \"county\"\n\n[[6]]\n[1] \"tract\"\n\ndetails[[2]]  # list with information on 1st tract\n\n[[1]]\n[1] \"Census Tract 1.01, Hennepin County, Minnesota\"\n\n[[2]]\n[1] \"3472\"\n\n[[3]]\n[1] \"70927\"\n\n[[4]]\n[1] \"27\"\n\n[[5]]\n[1] \"053\"\n\n[[6]]\n[1] \"000101\"\n\nname = character()\npopulation = double()\nmedian_income = double()\ntract = character()\n\nfor(i in 2:330) {\n  name[i-1] &lt;- details[[i]][[1]][1]\n  population[i-1] &lt;- details[[i]][[2]][1]\n  median_income[i-1] &lt;- details[[i]][[3]][1]\n  tract[i-1] &lt;- details[[i]][[6]][1]\n}\nhennepin_httr &lt;- tibble(\n  name = name,\n  population = parse_number(population),\n  median_income = parse_number(median_income),\n  tract = tract\n)\n\n\n\nOn Your Own\n\nWrite a for loop to obtain the Hennepin County data from 2017-2021\nWrite a function to give choices about year, county, and variables\nUse your function from (2) along with map and list_rbind to build a data set for Rice county for the years 2019-2021\n\n\n\nOne more example using an API key\nHere’s an example of getting data from a website that attempts to make imdb movie data available as an API.\nInitial instructions:\n\ngo to omdbapi.com under the API Key tab and request a free API key\nstore your key as discussed earlier\nexplore the examples at omdbapi.com\n\nWe will first obtain data about the movie Coco from 2017.\n\n# I used the first line to store my OMDB API key in .Renviron\n# Sys.setenv(OMDB_KEY = \"paste my omdb key here\")\nmyapikey &lt;- Sys.getenv(\"OMDB_KEY\")\n\n# Find url exploring examples at omdbapi.com\nurl &lt;- str_c(\"http://www.omdbapi.com/?t=Coco&y=2017&apikey=\", myapikey)\n\ncoco &lt;- GET(url)   # coco holds response from server\ncoco               # Status of 200 is good!\n\ndetails &lt;- content(coco, \"parse\")   \ndetails                         # get a list of 25 pieces of information\ndetails$Year                    # how to access details\ndetails[[2]]                    # since a list, another way to access\n\nNow build a data set for a collection of movies\n\n# Must figure out pattern in URL for obtaining different movies\n#  - try searching for others\nmovies &lt;- c(\"Coco\", \"Wonder+Woman\", \"Get+Out\", \n            \"The+Greatest+Showman\", \"Thor:+Ragnarok\")\n\n# Set up empty tibble\nomdb &lt;- tibble(Title = character(), Rated = character(), Genre = character(),\n       Actors = character(), Metascore = double(), imdbRating = double(),\n       BoxOffice = double())\n\n# Use for loop to run through API request process 5 times,\n#   each time filling the next row in the tibble\n#  - can do max of 1000 GETs per day\nfor(i in 1:5) {\n  url &lt;- str_c(\"http://www.omdbapi.com/?t=\",movies[i],\n               \"&apikey=\", myapikey)\n  Sys.sleep(0.5)\n  onemovie &lt;- GET(url)\n  details &lt;- content(onemovie, \"parse\")\n  omdb[i,1] &lt;- details$Title\n  omdb[i,2] &lt;- details$Rated\n  omdb[i,3] &lt;- details$Genre\n  omdb[i,4] &lt;- details$Actors\n  omdb[i,5] &lt;- parse_number(details$Metascore)\n  omdb[i,6] &lt;- parse_number(details$imdbRating)\n  omdb[i,7] &lt;- parse_number(details$BoxOffice)   # no $ and ,'s\n}\n\nomdb\n\n#  could use stringr functions to further organize this data - separate \n#    different genres, different actors, etc.\n\n\n\nOn Your Own (continued)\n\n(Based on final project by Mary Wu and Jenna Graff, MSCS 264, Spring 2024). Start with a small data set on 56 national parks from kaggle, and supplement with columns for the park address (a single column including address, city, state, and zip code) and a list of available activities (a single character column with activities separated by commas) from the park websites themselves.\n\nPreliminaries:\n\nRequest API here\nCheck out API guide\n\n\nnp_kaggle &lt;- read_csv(\"Data/parks.csv\")",
    "crumbs": [
      "Data Acquisition with APIs in R"
    ]
  },
  {
    "objectID": "08_table_scraping.html",
    "href": "08_table_scraping.html",
    "title": "Table Scraping in R",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button."
  },
  {
    "objectID": "08_table_scraping.html#four-steps-to-scraping-data-with-functions-in-the-rvest-library",
    "href": "08_table_scraping.html#four-steps-to-scraping-data-with-functions-in-the-rvest-library",
    "title": "Table Scraping in R",
    "section": "Four steps to scraping data with functions in the rvest library:",
    "text": "Four steps to scraping data with functions in the rvest library:\n\nrobotstxt::paths_allowed() Check if the website allows scraping, and then make sure we scrape “politely”\nread_html(). Input the URL containing the data and turn the html code into an XML file (another markup format that’s easier to work with).\nhtml_nodes(). Extract specific nodes from the XML file by using the CSS path that leads to the content of interest. (use css=“table” for tables.)\nhtml_text(). Extract content of interest from nodes. Might also use html_table() etc."
  },
  {
    "objectID": "08_table_scraping.html#data-scraping-ethics",
    "href": "08_table_scraping.html#data-scraping-ethics",
    "title": "Table Scraping in R",
    "section": "Data scraping ethics",
    "text": "Data scraping ethics\nBefore scraping, we should always check first whether the website allows scraping. We should also consider if there’s any personal or confidential information, and we should be considerate to not overload the server we’re scraping from.\nChapter 24 in R4DS provides a nice overview of some of the important issues to consider. A couple of highlights:\n\nbe aware of terms of service, and, if available, the robots.txt file that some websites will publish to clarify what can and cannot be scraped and other constraints about scraping.\nuse the polite package to scrape public, non-personal, and factual data in a respectful manner\nscrape with a good purpose and request only what you need; in particular, be extremely wary of personally identifiable information\n\nSee this article for more perspective on the ethics of data scraping."
  },
  {
    "objectID": "08_table_scraping.html#when-the-data-is-already-in-table-form",
    "href": "08_table_scraping.html#when-the-data-is-already-in-table-form",
    "title": "Table Scraping in R",
    "section": "When the data is already in table form:",
    "text": "When the data is already in table form:\nIn this example, we will scrape climate data from this website\nThe website already contains data in table form, so we use html_nodes(. , css = \"table\") and html_table()\n\n# check that scraping is allowed (Step 0)\nrobotstxt::paths_allowed(\"https://www.usclimatedata.com/climate/minneapolis/minnesota/united-states/usmn0503\")\n\n\n www.usclimatedata.com                      \n\n\n[1] TRUE\n\n# Step 1: read_html()\nmpls &lt;- read_html(\"https://www.usclimatedata.com/climate/minneapolis/minnesota/united-states/usmn0503\")\n\n# 2: html_nodes()\ntables &lt;- html_nodes(mpls, css = \"table\") \ntables  # have to guesstimate which table contains climate info\n\n{xml_nodeset (8)}\n[1] &lt;table id=\"monthly_table_one\" class=\"table table-hover tablesaw tablesaw- ...\n[2] &lt;table id=\"monthly_table_two\" class=\"table table-hover tablesaw tablesaw- ...\n[3] &lt;table class=\"table table-hover tablesaw tablesaw-mode-swipe mt-4 daily_t ...\n[4] &lt;table class=\"table table-hover tablesaw tablesaw-mode-swipe mt-4 history ...\n[5] &lt;table class=\"table table-striped table-hover tablesaw tablesaw-mode-swip ...\n[6] &lt;table class=\"table table-hover tablesaw geo_table\"&gt;\\n&lt;thead&gt;&lt;tr&gt;\\n&lt;th&gt; &lt; ...\n[7] &lt;table class=\"table table-hover tablesaw datetime_table\" data-tablesaw-hi ...\n[8] &lt;table class=\"table table-hover tablesaw monthly_summary_table\" data-tabl ...\n\n# 3: html_table()\nhtml_table(tables, header = TRUE, fill = TRUE)    # find the right table\n\n[[1]]\n# A tibble: 6 × 7\n  ``                                    JanJa  FebFe  MarMa  AprAp  MayMa  JunJu\n  &lt;chr&gt;                                 &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Average high in ºF Av. high Hi         24    29     41     58     69     79   \n2 Average low in ºF Av. low Lo            8    13     24     37     49     59   \n3 Days with precipitation Days precip.…   8     7     11      9     11     13   \n4 Hours of sunshine Hours sun. Sun      140   166    200    231    272    302   \n5 Av. precipitation in inch Av. precip…   0.9   0.77   1.89   2.66   3.36   4.25\n6 Av. snowfall in inch Snowfall Sn       12     8     10      3      0      0   \n\n[[2]]\n# A tibble: 6 × 7\n  ``                                     JulJu AugAu  SepSe  OctOc  NovNo  DecDe\n  &lt;chr&gt;                                  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Average high in ºF Av. high Hi         83     80    72     58     41     27   \n2 Average low in ºF Av. low Lo           64     62    52     40     26     12   \n3 Days with precipitation Days precip.…  10     10     9      8      8      8   \n4 Hours of sunshine Hours sun. Sun      343    296   237    193    115    112   \n5 Av. precipitation in inch Av. precip…   4.04   4.3   3.08   2.43   1.77   1.16\n6 Av. snowfall in inch Snowfall Sn        0      0     0      1      9     12   \n\n[[3]]\n# A tibble: 31 × 7\n   Day    HighºF LowºF `Prec/moinch` `Prec/yrinch` `Snow/moinch` `Snow/yrinch`\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n 1 1 Jan    23.8   8.3          0.04          0.04          0.39           1  \n 2 2 Jan    23.7   8.2          0.08          0.08          0.71           1.8\n 3 3 Jan    23.6   8.1          0.12          0.12          1.1            2.8\n 4 4 Jan    23.5   7.9          0.12          0.12          1.5            3.8\n 5 5 Jan    23.5   7.8          0.16          0.16          1.81           4.6\n 6 6 Jan    23.4   7.7          0.2           0.2           2.2            5.6\n 7 7 Jan    23.4   7.6          0.24          0.24          2.6            6.6\n 8 8 Jan    23.3   7.5          0.28          0.28          3.11           7.9\n 9 9 Jan    23.3   7.4          0.28          0.28          3.5            8.9\n10 10 Jan   23.3   7.3          0.31          0.31          3.9            9.9\n# ℹ 21 more rows\n\n[[4]]\n# A tibble: 26 × 6\n   Day    HighºF LowºF Precip.inch Snowinch `Snow d.inch`\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;            &lt;dbl&gt;\n 1 01 Dec   32    19   0.07        1.61                 7\n 2 02 Dec   27    12   0.00        0.00                 6\n 3 03 Dec   37.9  19.9 0.00        0.00                 6\n 4 04 Dec   39    24.1 0.00        0.00                 6\n 5 05 Dec   37    21.9 0.00        0.00                 5\n 6 06 Dec   32    17.1 0.00        0.00                 5\n 7 07 Dec   42.1  21.9 0.00        0.00                 5\n 8 08 Dec   41    30.9 0.00        0.00                 5\n 9 09 Dec   34    -0.9 0.16        2.52                 5\n10 10 Dec    8.1  -4   T           T                    7\n# ℹ 16 more rows\n\n[[5]]\n# A tibble: 9 × 4\n  ``                                          `Dec 19`    ``    Normal     \n  &lt;chr&gt;                                       &lt;chr&gt;       &lt;lgl&gt; &lt;chr&gt;      \n1 \"Average high temperature Av. high temp.\"   \"29.9 ºF\"   NA    \"27 ºF\"    \n2 \"Average low temperature Av. low temp.\"     \"14.6 ºF\"   NA    \"12 ºF\"    \n3 \"Total precipitation Total precip.\"         \"0.39 inch\" NA    \"1.16 inch\"\n4 \"Total snowfall Total snowfall\"             \"6.33 inch\" NA    \"12 inch\"  \n5 \"\"                                          \"\"          NA    \"\"         \n6 \"Highest max temperature Highest max temp.\" \"44.1 ºF\"   NA    \"-\"        \n7 \"Lowest max temperature Lowest max temp.\"   \"8.1 ºF\"    NA    \"-\"        \n8 \"Highest min temperature Highest min temp.\" \"32.0 ºF\"   NA    \"-\"        \n9 \"Lowest min temperature Lowest min temp.\"   \"-5.1 ºF\"   NA    \"-\"        \n\n[[6]]\n# A tibble: 10 × 3\n   ``                   ``                ``   \n   &lt;chr&gt;                &lt;chr&gt;             &lt;lgl&gt;\n 1 Country              United States     NA   \n 2 State                Minnesota         NA   \n 3 County               Hennepin          NA   \n 4 City                 Minneapolis       NA   \n 5 Zip code             55401             NA   \n 6 Longitude            -93.27 dec. degr. NA   \n 7 Latitude             44.98 dec. degr.  NA   \n 8 Altitude - Elevation 840ft             NA   \n 9 ICAO                 -                 NA   \n10 IATA                 -                 NA   \n\n[[7]]\n# A tibble: 6 × 3\n  ``          ``              ``   \n  &lt;chr&gt;       &lt;chr&gt;           &lt;lgl&gt;\n1 Local Time  09:39 AM        NA   \n2 Sunrise     07:29 AM        NA   \n3 Sunset      07:15 PM        NA   \n4 Day / Night Day             NA   \n5 Timezone    Chicago -6:00   NA   \n6 Timezone DB America/Chicago NA   \n\n[[8]]\n# A tibble: 6 × 2\n  ``                         ``        \n  &lt;chr&gt;                      &lt;chr&gt;     \n1 Annual high temperature    55ºF      \n2 Annual low temperature     37ºF      \n3 Days per year with precip. 112 days  \n4 Annual hours of sunshine   2607 hours\n5 Average annual precip.     30.61 inch\n6 Av. annual snowfall        55 inch   \n\nmpls_data1 &lt;- html_table(tables, header = TRUE, fill = TRUE)[[1]]  \nmpls_data1\n\n# A tibble: 6 × 7\n  ``                                    JanJa  FebFe  MarMa  AprAp  MayMa  JunJu\n  &lt;chr&gt;                                 &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Average high in ºF Av. high Hi         24    29     41     58     69     79   \n2 Average low in ºF Av. low Lo            8    13     24     37     49     59   \n3 Days with precipitation Days precip.…   8     7     11      9     11     13   \n4 Hours of sunshine Hours sun. Sun      140   166    200    231    272    302   \n5 Av. precipitation in inch Av. precip…   0.9   0.77   1.89   2.66   3.36   4.25\n6 Av. snowfall in inch Snowfall Sn       12     8     10      3      0      0   \n\nmpls_data2 &lt;- html_table(tables, header = TRUE, fill = TRUE)[[2]]  \nmpls_data2\n\n# A tibble: 6 × 7\n  ``                                     JulJu AugAu  SepSe  OctOc  NovNo  DecDe\n  &lt;chr&gt;                                  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Average high in ºF Av. high Hi         83     80    72     58     41     27   \n2 Average low in ºF Av. low Lo           64     62    52     40     26     12   \n3 Days with precipitation Days precip.…  10     10     9      8      8      8   \n4 Hours of sunshine Hours sun. Sun      343    296   237    193    115    112   \n5 Av. precipitation in inch Av. precip…   4.04   4.3   3.08   2.43   1.77   1.16\n6 Av. snowfall in inch Snowfall Sn        0      0     0      1      9     12   \n\n\nNow we wrap the 4 steps above into the bow and scrape functions from the polite package:\n\nsession &lt;- bow(\"https://www.usclimatedata.com/climate/minneapolis/minnesota/united-states/usmn0503\", force = TRUE)\n\nresult &lt;- scrape(session) |&gt;\n  html_nodes(css = \"table\") |&gt; \n  html_table(header = TRUE, fill = TRUE)\nmpls_data1 &lt;- result[[1]]\nmpls_data2 &lt;- result[[2]]\n\nEven after finding the correct tables, there may still be a lot of work to make it tidy!!!\n[Pause to Ponder:] What is each line of code doing below?\n\nbind_cols(mpls_data1, mpls_data2) |&gt;\n  as_tibble() |&gt; \n  select(-`...8`) |&gt;\n  mutate(`...1` = str_extract(`...1`, \"[^ ]+ [^ ]+ [^ ]+\")) |&gt;\n  pivot_longer(cols = c(`JanJa`:`DecDe`), \n               names_to = \"month\", values_to = \"weather\") |&gt;\n  pivot_wider(names_from = `...1`, values_from = weather) |&gt;\n  mutate(month = str_sub(month, 1, 3))  |&gt;\n  rename(avg_high = \"Average high in\",\n         avg_low = \"Average low in\")\n\nNew names:\n• `` -&gt; `...1`\n• `` -&gt; `...8`\n\n\n# A tibble: 12 × 7\n   month avg_high avg_low `Days with precipitation` `Hours of sunshine`\n   &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;                     &lt;dbl&gt;               &lt;dbl&gt;\n 1 Jan         24       8                         8                 140\n 2 Feb         29      13                         7                 166\n 3 Mar         41      24                        11                 200\n 4 Apr         58      37                         9                 231\n 5 May         69      49                        11                 272\n 6 Jun         79      59                        13                 302\n 7 Jul         83      64                        10                 343\n 8 Aug         80      62                        10                 296\n 9 Sep         72      52                         9                 237\n10 Oct         58      40                         8                 193\n11 Nov         41      26                         8                 115\n12 Dec         27      12                         8                 112\n# ℹ 2 more variables: `Av. precipitation in` &lt;dbl&gt;, `Av. snowfall in` &lt;dbl&gt;\n\n# Probably want to rename the rest of the variables too!\n\n\nLeaflet mapping example with data in table form\nLet’s return to our example from 02_maps.qmd where we recreated an interactive choropleth map of population densities by US state. Recall how that plot was very suspicious? The population density data that came with the state geometries from our source seemed incorrect.\nLet’s see if we can use our new web scraping skills to scrape the correct population density data and repeat that plot! Can we go out and find the real statewise population densities, create a tidy data frame, merge that with our state geometry shapefiles, and then regenerate our plot?\nA quick wikipedia search yields this webpage with more reasonable population densities in a nice table format. Let’s see if we can grab this data using our 4 steps to rvesting data!\n\n# check that scraping is allowed (Step 0)\nrobotstxt::paths_allowed(\"https://en.wikipedia.org/wiki/List_of_states_and_territories_of_the_United_States_by_population_density\")\n\n\n en.wikipedia.org                      \n\n\n[1] TRUE\n\n# Step 1: read_html()\npop_dens &lt;- read_html(\"https://en.wikipedia.org/wiki/List_of_states_and_territories_of_the_United_States_by_population_density\")\n\n# 2: html_nodes()\ntables &lt;- html_nodes(pop_dens, css = \"table\") \ntables  # have to guesstimate which table contains our desired info\n\n{xml_nodeset (2)}\n[1] &lt;table class=\"wikitable sortable plainrowheaders sticky-header-multi stat ...\n[2] &lt;table class=\"nowraplinks hlist mw-collapsible mw-collapsed navbox-inner\" ...\n\n# 3: html_table()\nhtml_table(tables, header = TRUE, fill = TRUE)    # find the right table\n\n[[1]]\n# A tibble: 61 × 6\n   Location               Density Density Population `Land area` `Land area`\n   &lt;chr&gt;                  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;      \n 1 Location               /mi2    /km2    Population mi2         km2        \n 2 District of Columbia   11,131  4,297   678,972    61          158        \n 3 New Jersey             1,263   488     9,290,841  7,354       19,047     \n 4 Rhode Island           1,060   409     1,095,962  1,034       2,678      \n 5 Puerto Rico            936     361     3,205,691  3,424       8,868      \n 6 Massachusetts          898     347     7,001,399  7,800       20,202     \n 7 Guam[4]                824     319     172,952    210         543        \n 8 Connecticut            747     288     3,617,176  4,842       12,542     \n 9 U.S. Virgin Islands[4] 737     284     98,750     134         348        \n10 Maryland               637     246     6,180,253  9,707       25,142     \n# ℹ 51 more rows\n\n[[2]]\n# A tibble: 11 × 2\n   .mw-parser-output .navbar{display:inline;font-size:8…¹ .mw-parser-output .n…²\n   &lt;chr&gt;                                                  &lt;chr&gt;                 \n 1 \"List of states and territories of the United States\"  \"List of states and t…\n 2 \"Demographics\"                                         \"Population\\nAfrican …\n 3 \"Economy\"                                              \"Billionaires\\nBudget…\n 4 \"Environment\"                                          \"Botanical gardens\\nC…\n 5 \"Geography\"                                            \"Area\\nBays\\nBeaches\\…\n 6 \"Government\"                                           \"Agriculture commissi…\n 7 \"Health\"                                               \"Changes in life expe…\n 8 \"History\"                                              \"Date of statehood\\nN…\n 9 \"Law\"                                                  \"Abortion\\nAge of con…\n10 \"Miscellaneous\"                                        \"Abbreviations\\nAirpo…\n11 \"Category\\n Commons\\n Portals\"                         \"Category\\n Commons\\n…\n# ℹ abbreviated names:\n#   ¹​`.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:\"[ \"}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:\" ]\"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a&gt;span,.mw-parser-output .navbar a&gt;abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}html.skin-theme-clientpref-night .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}}@media print{.mw-parser-output .navbar{display:none!important}}vteUnited States state-related lists`,\n#   ²​`.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:\"[ \"}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:\" ]\"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a&gt;span,.mw-parser-output .navbar a&gt;abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}html.skin-theme-clientpref-night .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}}@media print{.mw-parser-output .navbar{display:none!important}}vteUnited States state-related lists`\n\ndensity_table &lt;- html_table(tables, header = TRUE, fill = TRUE)[[1]]  \ndensity_table\n\n# A tibble: 61 × 6\n   Location               Density Density Population `Land area` `Land area`\n   &lt;chr&gt;                  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;      \n 1 Location               /mi2    /km2    Population mi2         km2        \n 2 District of Columbia   11,131  4,297   678,972    61          158        \n 3 New Jersey             1,263   488     9,290,841  7,354       19,047     \n 4 Rhode Island           1,060   409     1,095,962  1,034       2,678      \n 5 Puerto Rico            936     361     3,205,691  3,424       8,868      \n 6 Massachusetts          898     347     7,001,399  7,800       20,202     \n 7 Guam[4]                824     319     172,952    210         543        \n 8 Connecticut            747     288     3,617,176  4,842       12,542     \n 9 U.S. Virgin Islands[4] 737     284     98,750     134         348        \n10 Maryland               637     246     6,180,253  9,707       25,142     \n# ℹ 51 more rows\n\n# Perform Steps 0-3 using the polite package\nsession &lt;- bow(\"https://en.wikipedia.org/wiki/List_of_states_and_territories_of_the_United_States_by_population_density\", force = TRUE)\n\nresult &lt;- scrape(session) |&gt;\n  html_nodes(css = \"table\") |&gt; \n  html_table(header = TRUE, fill = TRUE)\ndensity_table &lt;- result[[1]]\ndensity_table\n\n# A tibble: 61 × 6\n   Location               Density Density Population `Land area` `Land area`\n   &lt;chr&gt;                  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;      \n 1 Location               /mi2    /km2    Population mi2         km2        \n 2 District of Columbia   11,131  4,297   678,972    61          158        \n 3 New Jersey             1,263   488     9,290,841  7,354       19,047     \n 4 Rhode Island           1,060   409     1,095,962  1,034       2,678      \n 5 Puerto Rico            936     361     3,205,691  3,424       8,868      \n 6 Massachusetts          898     347     7,001,399  7,800       20,202     \n 7 Guam[4]                824     319     172,952    210         543        \n 8 Connecticut            747     288     3,617,176  4,842       12,542     \n 9 U.S. Virgin Islands[4] 737     284     98,750     134         348        \n10 Maryland               637     246     6,180,253  9,707       25,142     \n# ℹ 51 more rows\n\n\nEven after grabbing our table from wikipedia and setting it in a nice tibble format, there is still some cleaning to do before we can merge this with our state geometries:\n\ndensity_data &lt;- density_table |&gt;\n  select(1, 2, 4, 5) |&gt;\n  filter(!row_number() == 1) |&gt;\n  rename(Land_area = `Land area`) |&gt;\n  mutate(state_name = str_to_lower(as.character(Location)),\n         Density = parse_number(Density),\n         Population = parse_number(Population),\n         Land_area = parse_number(Land_area)) |&gt;\n  select(-Location)\ndensity_data\n\n# A tibble: 60 × 4\n   Density Population Land_area state_name            \n     &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                 \n 1   11131     678972        61 district of columbia  \n 2    1263    9290841      7354 new jersey            \n 3    1060    1095962      1034 rhode island          \n 4     936    3205691      3424 puerto rico           \n 5     898    7001399      7800 massachusetts         \n 6     824     172952       210 guam[4]               \n 7     747    3617176      4842 connecticut           \n 8     737      98750       134 u.s. virgin islands[4]\n 9     637    6180253      9707 maryland              \n10     578      43915        76 american samoa[4]     \n# ℹ 50 more rows\n\n\nAs before, we get core geometry data to draw US states and then we’ll make sure we can merge our new density data into the core files.\n\n# Get info to draw US states for geom_polygon (connect the lat-long points)\nstates_polygon &lt;- as_tibble(map_data(\"state\")) |&gt;\n  select(region, group, order, lat, long)\n\n# See what the state (region) levels look like in states_polygon\nunique(states_polygon$region)\n\n [1] \"alabama\"              \"arizona\"              \"arkansas\"            \n [4] \"california\"           \"colorado\"             \"connecticut\"         \n [7] \"delaware\"             \"district of columbia\" \"florida\"             \n[10] \"georgia\"              \"idaho\"                \"illinois\"            \n[13] \"indiana\"              \"iowa\"                 \"kansas\"              \n[16] \"kentucky\"             \"louisiana\"            \"maine\"               \n[19] \"maryland\"             \"massachusetts\"        \"michigan\"            \n[22] \"minnesota\"            \"mississippi\"          \"missouri\"            \n[25] \"montana\"              \"nebraska\"             \"nevada\"              \n[28] \"new hampshire\"        \"new jersey\"           \"new mexico\"          \n[31] \"new york\"             \"north carolina\"       \"north dakota\"        \n[34] \"ohio\"                 \"oklahoma\"             \"oregon\"              \n[37] \"pennsylvania\"         \"rhode island\"         \"south carolina\"      \n[40] \"south dakota\"         \"tennessee\"            \"texas\"               \n[43] \"utah\"                 \"vermont\"              \"virginia\"            \n[46] \"washington\"           \"west virginia\"        \"wisconsin\"           \n[49] \"wyoming\"             \n\n# Get info to draw US states for geom_sf and leaflet (simple features\n#   object with multipolygon geometry column)\nstates_sf &lt;- read_sf(\"https://rstudio.github.io/leaflet/json/us-states.geojson\") |&gt;\n  select(name, geometry)\n\n# See what the state (name) levels look like in states_sf\nunique(states_sf$name)\n\n [1] \"Alabama\"              \"Alaska\"               \"Arizona\"             \n [4] \"Arkansas\"             \"California\"           \"Colorado\"            \n [7] \"Connecticut\"          \"Delaware\"             \"District of Columbia\"\n[10] \"Florida\"              \"Georgia\"              \"Hawaii\"              \n[13] \"Idaho\"                \"Illinois\"             \"Indiana\"             \n[16] \"Iowa\"                 \"Kansas\"               \"Kentucky\"            \n[19] \"Louisiana\"            \"Maine\"                \"Maryland\"            \n[22] \"Massachusetts\"        \"Michigan\"             \"Minnesota\"           \n[25] \"Mississippi\"          \"Missouri\"             \"Montana\"             \n[28] \"Nebraska\"             \"Nevada\"               \"New Hampshire\"       \n[31] \"New Jersey\"           \"New Mexico\"           \"New York\"            \n[34] \"North Carolina\"       \"North Dakota\"         \"Ohio\"                \n[37] \"Oklahoma\"             \"Oregon\"               \"Pennsylvania\"        \n[40] \"Rhode Island\"         \"South Carolina\"       \"South Dakota\"        \n[43] \"Tennessee\"            \"Texas\"                \"Utah\"                \n[46] \"Vermont\"              \"Virginia\"             \"Washington\"          \n[49] \"West Virginia\"        \"Wisconsin\"            \"Wyoming\"             \n[52] \"Puerto Rico\"         \n\n# See what the state (state_name) levels look like in density_data\nunique(density_data$state_name)   \n\n [1] \"district of columbia\"        \"new jersey\"                 \n [3] \"rhode island\"                \"puerto rico\"                \n [5] \"massachusetts\"               \"guam[4]\"                    \n [7] \"connecticut\"                 \"u.s. virgin islands[4]\"     \n [9] \"maryland\"                    \"american samoa[4]\"          \n[11] \"delaware\"                    \"florida\"                    \n[13] \"new york\"                    \"pennsylvania\"               \n[15] \"ohio\"                        \"northern mariana islands[4]\"\n[17] \"california\"                  \"illinois\"                   \n[19] \"hawaii\"                      \"north carolina\"             \n[21] \"virginia\"                    \"georgia\"                    \n[23] \"indiana\"                     \"south carolina\"             \n[25] \"michigan\"                    \"tennessee\"                  \n[27] \"new hampshire\"               \"washington\"                 \n[29] \"texas\"                       \"kentucky\"                   \n[31] \"wisconsin\"                   \"louisiana\"                  \n[33] \"alabama\"                     \"missouri\"                   \n[35] \"west virginia\"               \"minnesota\"                  \n[37] \"vermont\"                     \"arizona\"                    \n[39] \"mississippi\"                 \"oklahoma\"                   \n[41] \"arkansas\"                    \"iowa\"                       \n[43] \"colorado\"                    \"maine\"                      \n[45] \"oregon\"                      \"utah\"                       \n[47] \"kansas\"                      \"nevada\"                     \n[49] \"nebraska\"                    \"idaho\"                      \n[51] \"new mexico\"                  \"south dakota\"               \n[53] \"north dakota\"                \"montana\"                    \n[55] \"wyoming\"                     \"alaska\"                     \n[57] \"contiguous us\"               \"50 states\"                  \n[59] \"50 states and dc\"            \"united states\"              \n\n# all lower case plus some extraneous rows\n\n\n# Make sure all keys have the same format before joining: all lower case\n\nstates_sf &lt;- states_sf |&gt;\n  mutate(name = str_to_lower(name))\n\n\n# Now we can merge data sets together for the static and the interactive plots\n\n# Merge with states_polygon (static)\ndensity_polygon &lt;- states_polygon |&gt;\n  left_join(density_data, by = c(\"region\" = \"state_name\"))\ndensity_polygon\n\n# A tibble: 15,537 × 8\n   region  group order   lat  long Density Population Land_area\n   &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1 alabama     1     1  30.4 -87.5     101    5108468     50645\n 2 alabama     1     2  30.4 -87.5     101    5108468     50645\n 3 alabama     1     3  30.4 -87.5     101    5108468     50645\n 4 alabama     1     4  30.3 -87.5     101    5108468     50645\n 5 alabama     1     5  30.3 -87.6     101    5108468     50645\n 6 alabama     1     6  30.3 -87.6     101    5108468     50645\n 7 alabama     1     7  30.3 -87.6     101    5108468     50645\n 8 alabama     1     8  30.3 -87.6     101    5108468     50645\n 9 alabama     1     9  30.3 -87.7     101    5108468     50645\n10 alabama     1    10  30.3 -87.8     101    5108468     50645\n# ℹ 15,527 more rows\n\n# Looks like merge worked for 48 contiguous states plus DC\ndensity_polygon |&gt;\n  group_by(region) |&gt;\n  summarise(mean = mean(Density)) |&gt;\n  print(n = Inf)\n\n# A tibble: 49 × 2\n   region                  mean\n   &lt;chr&gt;                  &lt;dbl&gt;\n 1 alabama                101  \n 2 arizona                 65  \n 3 arkansas                59  \n 4 california             250  \n 5 colorado                57  \n 6 connecticut            747  \n 7 delaware               529  \n 8 district of columbia 11131  \n 9 florida                422  \n10 georgia                192  \n11 idaho                   24  \n12 illinois               226  \n13 indiana                192  \n14 iowa                    57  \n15 kansas                  36  \n16 kentucky               115  \n17 louisiana              106  \n18 maine                   45  \n19 maryland               637  \n20 massachusetts          898  \n21 michigan               178  \n22 minnesota               72  \n23 mississippi             63  \n24 missouri                90  \n25 montana                  7.8\n26 nebraska                26  \n27 nevada                  29  \n28 new hampshire          157  \n29 new jersey            1263  \n30 new mexico              17  \n31 new york               415  \n32 north carolina         223  \n33 north dakota            11  \n34 ohio                   288  \n35 oklahoma                59  \n36 oregon                  44  \n37 pennsylvania           290  \n38 rhode island          1060  \n39 south carolina         179  \n40 south dakota            12  \n41 tennessee              173  \n42 texas                  117  \n43 utah                    42  \n44 vermont                 70  \n45 virginia               221  \n46 washington             118  \n47 west virginia           74  \n48 wisconsin              109  \n49 wyoming                  6  \n\n# Remove DC since such an outlier\ndensity_polygon &lt;- density_polygon |&gt;\n  filter(region != \"district of columbia\")\n\n\n# Merge with states_sf (static or interactive)\ndensity_sf &lt;- states_sf |&gt;\n  left_join(density_data, by = c(\"name\" = \"state_name\")) |&gt;\n  filter(!(name %in% c(\"alaska\", \"hawaii\")))\n\n# Looks like merge worked for 48 contiguous states plus DC and PR\nclass(density_sf)\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nprint(density_sf, n = Inf)\n\nSimple feature collection with 50 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -124.7066 ymin: 17.92956 xmax: -65.6268 ymax: 49.38362\nGeodetic CRS:  WGS 84\n# A tibble: 50 × 5\n   name                                    geometry Density Population Land_area\n * &lt;chr&gt;                         &lt;MULTIPOLYGON [°]&gt;   &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1 alabama              (((-87.3593 35.00118, -85.…   101      5108468     50645\n 2 arizona              (((-109.0425 37.00026, -10…    65      7431344    113594\n 3 arkansas             (((-94.47384 36.50186, -90…    59      3067732     52035\n 4 california           (((-123.2333 42.00619, -12…   250     38965193    155779\n 5 colorado             (((-107.9197 41.00391, -10…    57      5877610    103642\n 6 connecticut          (((-73.05353 42.03905, -71…   747      3617176      4842\n 7 delaware             (((-75.41409 39.80446, -75…   529      1031890      1949\n 8 district of columbia (((-77.03526 38.99387, -76… 11131       678972        61\n 9 florida              (((-85.49714 30.99754, -85…   422     22610726     53625\n10 georgia              (((-83.10919 35.00118, -83…   192     11029227     57513\n11 idaho                (((-116.0475 49.00024, -11…    24      1964726     82643\n12 illinois             (((-90.63998 42.51006, -88…   226     12549689     55519\n13 indiana              (((-85.99006 41.75972, -84…   192      6862199     35826\n14 iowa                 (((-91.36842 43.50139, -91…    57      3207004     55857\n15 kansas               (((-101.906 40.00163, -95.…    36      2940546     81759\n16 kentucky             (((-83.90335 38.76931, -83…   115      4526154     39486\n17 louisiana            (((-93.60849 33.01853, -91…   106      4573749     43204\n18 maine                (((-70.70392 43.05776, -70…    45      1395722     30843\n19 maryland             (((-75.99465 37.95325, -76…   637      6180253      9707\n20 massachusetts        (((-70.91752 42.88797, -70…   898      7001399      7800\n21 michigan             (((-83.45424 41.73234, -84…   178     10037261     56539\n22 minnesota            (((-92.0147 46.7054, -92.0…    72      5737915     79627\n23 mississippi          (((-88.47111 34.9957, -88.…    63      2939690     46923\n24 missouri             (((-91.83396 40.60957, -91…    90      6196156     68742\n25 montana              (((-104.0475 49.00024, -10…     7.8    1132812    145546\n26 nebraska             (((-103.3246 43.00299, -10…    26      1978379     76824\n27 nevada               (((-117.0279 42.00071, -11…    29      3194176    109781\n28 new hampshire        (((-71.08183 45.3033, -71.…   157      1402054      8953\n29 new jersey           (((-74.23655 41.14083, -73…  1263      9290841      7354\n30 new mexico           (((-107.4213 37.00026, -10…    17      2114371    121298\n31 new york             (((-73.34381 45.01303, -73…   415     19571216     47126\n32 north carolina       (((-80.97866 36.56211, -80…   223     10835491     48618\n33 north dakota         (((-97.22874 49.00024, -97…    11       783926     69001\n34 ohio                 (((-80.5186 41.9788, -80.5…   288     11785935     40861\n35 oklahoma             (((-100.0877 37.00026, -94…    59      4053824     68595\n36 oregon               (((-123.2113 46.17414, -12…    44      4233358     95988\n37 pennsylvania         (((-79.76278 42.25265, -79…   290     12961683     44743\n38 rhode island         (((-71.19684 41.67757, -71…  1060      1095962      1034\n39 south carolina       (((-82.76414 35.0669, -82.…   179      5373555     30061\n40 south dakota         (((-104.0475 45.94411, -96…    12       919318     75811\n41 tennessee            (((-88.05487 36.49638, -88…   173      7126489     41235\n42 texas                (((-101.8129 36.50186, -10…   117     30503301    261232\n43 utah                 (((-112.1644 41.99523, -11…    42      3417734     82170\n44 vermont              (((-71.50355 45.01303, -71…    70       647464      9217\n45 virginia             (((-75.39766 38.0135, -75.…   221      8715698     39490\n46 washington           (((-117.0334 49.00024, -11…   118      7812880     66456\n47 west virginia        (((-80.5186 40.63695, -80.…    74      1770071     24038\n48 wisconsin            (((-90.41543 46.56848, -90…   109      5910955     54158\n49 wyoming              (((-109.0808 45.00207, -10…     6       584057     97093\n50 puerto rico          (((-66.44834 17.98433, -66…   936      3205691      3424\n\n# Remove DC and PR\ndensity_sf &lt;- density_sf |&gt;\n  filter(name != \"district of columbia\" & name != \"puerto rico\")\n\nNumeric variable (static plot):\n\ndensity_polygon |&gt;\n  ggplot(mapping = aes(x = long, y = lat, group = group)) + \n    geom_polygon(aes(fill = Density), color = \"black\") + \n    labs(fill = \"Population density in 2023 \\n (people per sq mile)\") +\n    coord_map() + \n    theme_void() +  \n    scale_fill_viridis() \n\n\n\n\n\n\n\n\nRemember that the original plot classified densities into our own pre-determined bins before plotting - this might look better!\n\ndensity_polygon &lt;- density_polygon |&gt;\n  mutate(Density_intervals = cut(Density, n = 8,\n          breaks = c(0, 10, 20, 50, 100, 200, 500, 1000, Inf)))\n\ndensity_polygon |&gt;\n  ggplot(mapping = aes(x = long, y = lat, group = group)) + \n    geom_polygon(aes(fill = Density_intervals), color = \"white\",\n                 linetype = 2) + \n    labs(fill = \"Population Density (per sq mile)\") +\n    coord_map() + \n    theme_void() +  \n    scale_fill_brewer(palette = \"YlOrRd\") \n\n\n\n\n\n\n\n\nWe could even create a static plot using geom_sf() using density_sf:\n\ndensity_sf &lt;- density_sf |&gt;\n  mutate(Density_intervals = cut(Density, n = 8,\n          breaks = c(0, 10, 20, 50, 100, 200, 500, 1000, Inf))) \n\nggplot(data = density_sf) + \n  geom_sf(aes(fill = Density_intervals), colour = \"white\", linetype = 2) + \n  theme_void() +  \n  scale_fill_brewer(palette = \"YlOrRd\") \n\n\n\n\n\n\n\n\nBut… why not make an interactive plot instead?\n\ndensity_sf &lt;- density_sf |&gt;\n  mutate(labels = str_c(name, \": \", Density, \" people per sq mile in 2023\"))\n\nlabels &lt;- lapply(density_sf$labels, HTML)\npal &lt;- colorNumeric(\"YlOrRd\", density_sf$Density)\n\nleaflet(density_sf) |&gt;\n  setView(-96, 37.8, 4) |&gt;\n  addTiles() |&gt;\n  addPolygons(\n    weight = 2,\n    opacity = 1,\n    color = ~ pal(density_sf$Density),\n    dashArray = \"3\",\n    fillOpacity = 0.7,\n    highlightOptions = highlightOptions(\n      weight = 5,\n      color = \"#666\",\n      dashArray = \"\",\n      fillOpacity = 0.7,\n      bringToFront = TRUE),\n    label = labels,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n      textsize = \"15px\",\n      direction = \"auto\")) \n\n\n\n\n# should use addLegend() but not trivial without pre-set bins\n\nHere’s an interactive plot with our own bins:\n\n# Create our own category bins for population densities\n#   and assign the yellow-orange-red color palette\nbins &lt;- c(0, 10, 20, 50, 100, 200, 500, 1000, Inf)\npal &lt;- colorBin(\"YlOrRd\", domain = density_sf$Density, bins = bins)\n\n# Create labels that pop up when we hover over a state.  The labels must\n#   be part of a list where each entry is tagged as HTML code.\ndensity_sf &lt;- density_sf |&gt;\n  mutate(labels = str_c(name, \": \", Density, \" people / sq mile\"))\nlabels &lt;- lapply(density_sf$labels, HTML)\n\n# If want more HTML formatting, use these lines instead of those above:\n# states &lt;- states |&gt;\n#   mutate(labels = glue(\"&lt;strong&gt;{name}&lt;/strong&gt;&lt;br/&gt;{density} people / \n#   mi&lt;sup&gt;2&lt;/sup&gt;\"))\n# labels &lt;- lapply(states$labels, HTML)\n\nleaflet(density_sf) |&gt;\n  setView(-96, 37.8, 4) |&gt;\n  addTiles() |&gt;\n  addPolygons(\n    fillColor = ~pal(Density),\n    weight = 2,\n    opacity = 1,\n    color = \"white\",\n    dashArray = \"3\",\n    fillOpacity = 0.7,\n    highlightOptions = highlightOptions(\n      weight = 5,\n      color = \"#666\",\n      dashArray = \"\",\n      fillOpacity = 0.7,\n      bringToFront = TRUE),\n    label = labels,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n      textsize = \"15px\",\n      direction = \"auto\")) |&gt;\n  addLegend(pal = pal, values = ~Density, opacity = 0.7, title = NULL,\n    position = \"bottomright\")\n\n\n\n\n\n\n\nOn Your Own\n\nUse the rvest package and html_table to read in the table of data found at the link here and create a scatterplot of land area versus the 2022 estimated population. I give you some starter code below; fill in the “???” and be sure you can explain what EVERY line of code does and why it’s necessary.\n\n\ncity_pop &lt;- read_html(\"https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\")\n\npop &lt;- html_nodes(???, ???)\nhtml_table(pop, header = TRUE, fill = TRUE)  # find right table\npop2 &lt;- html_table(pop, header = TRUE, fill = TRUE)[[???]]\npop2\n\n# perform the steps above with the polite package\nsession &lt;- bow(\"https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\", force = TRUE)\n\nresult &lt;- scrape(session) |&gt;\n  html_nodes(???) |&gt;\n  html_table(header = TRUE, fill = TRUE)\npop2 &lt;- result[[???]]\npop2\n\npop3 &lt;- as_tibble(pop2[,c(1:6,8)]) |&gt;\n  slice(???) |&gt;\n  rename(`State` = `ST`,\n         `Estimate2023` = `2023estimate`,\n         `Census` = `2020census`,\n         `Area` = `2020 land area`,\n         `Density` = `2020 density`) |&gt;\n  mutate(Estimate2023 = parse_number(Estimate2023),\n         Census = parse_number(Census),\n         Change = ???   # get rid of % but preserve +/-,\n         Area = parse_number(Area),\n         Density = parse_number(Density)) |&gt; \n  mutate(City = str_replace(City, \"\\\\[.*$\", \"\"))\npop3\n\n# pick out unusual points\noutliers &lt;- pop3 |&gt; \n  filter(Estimate2023 &gt; ??? | Area &gt; ???)\n\n# This will work if don't turn variables from chr to dbl, but in that \n#  case notice how axes are just evenly spaced categorical variables\nggplot(pop3, aes(x = ???, y = ???)) +\n  geom_point()  +\n  geom_smooth() +\n  ggrepel::geom_label_repel(data = ???, aes(label = ???))\n\n\nWe would like to create a tibble with 4 years of data (2001-2004) from the Minnesota Wild hockey team. Specifically, we are interested in the “Scoring Regular Season” table from this webpage and the similar webpages from 2002, 2003, and 2004. Your final tibble should have 6 columns: player, year, age, pos (position), gp (games played), and pts (points).\n\nYou should (a) write a function called hockey_stats with inputs for team and year to scrape data from the “scoring Regular Season” table, and (b) use iteration techniques to scrape and combine 4 years worth of data. Here are some functions you might consider:\n\nrow_to_names(row_number = 1) from the janitor package\nclean_names() also from the janitor package\nbow() and scrape() from the polite package\nstr_c() from the stringr package (for creating urls with user inputs)\nmap2() and list_rbind() for iterating and combining years\n\nTry following these steps:\n\nBe sure you can find and clean the correct table from the 2021 season.\nOrganize your rvest code from (1) into functions from the polite package.\nPlace the code from (2) into a function where the user can input a team and year. You would then adjust the url accordingly and produce a clean table for the user.\nUse map2 and list_rbind to build one data set containing Minnesota Wild data from 2001-2004."
  },
  {
    "objectID": "09_web_scraping.html",
    "href": "09_web_scraping.html",
    "title": "Web Scraping in R",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\nCredit to Brianna Heggeseth and Leslie Myint from Macalester College for a few of these descriptions and examples.",
    "crumbs": [
      "Web Scraping in R"
    ]
  },
  {
    "objectID": "09_web_scraping.html#recall-the-four-steps-to-scraping-data-with-functions-in-the-rvest-library",
    "href": "09_web_scraping.html#recall-the-four-steps-to-scraping-data-with-functions-in-the-rvest-library",
    "title": "Web Scraping in R",
    "section": "Recall the four steps to scraping data with functions in the rvest library:",
    "text": "Recall the four steps to scraping data with functions in the rvest library:\n\nrobotstxt::paths_allowed() Check if the website allows scraping, and then make sure we scrape “politely”\nread_html(). Input the URL containing the data and turn the html code into an XML file (another markup format that’s easier to work with).\nhtml_nodes(). Extract specific nodes from the XML file by using the CSS path that leads to the content of interest. (use css=“table” for tables.)\nhtml_text(). Extract content of interest from nodes. Might also use html_table() etc.",
    "crumbs": [
      "Web Scraping in R"
    ]
  },
  {
    "objectID": "09_web_scraping.html#more-scraping-ethics",
    "href": "09_web_scraping.html#more-scraping-ethics",
    "title": "Web Scraping in R",
    "section": "More scraping ethics",
    "text": "More scraping ethics\n\nrobots.txt\nrobots.txt is a file that some websites will publish to clarify what can and cannot be scraped and other constraints about scraping. When a website publishes this file, this we need to comply with the information in it for moral and legal reasons.\nWe will look through the information in this tutorial and apply this to the NIH robots.txt file.\nFrom our investigation of the NIH robots.txt, we learn:\n\nUser-agent: *: Anyone is allowed to scrape\nCrawl-delay: 2: Need to wait 2 seconds between each page scraped\nNo Visit-time entry: no restrictions on time of day that scraping is allowed\nNo Request-rate entry: no restrictions on simultaneous requests\nNo mention of ?page=, news-events, news-releases, or https://science.education.nih.gov/ in the Disallow sections. (This is what we want to scrape today.)\n\n\n\nrobotstxt package\nWe can also use functions from the robotstxt package, which was built to download and parse robots.txt files (more info). Specifically, the paths_allowed() function can check if a bot has permission to access certain pages.",
    "crumbs": [
      "Web Scraping in R"
    ]
  },
  {
    "objectID": "09_web_scraping.html#a-timeout-to-preview-some-technical-ideas",
    "href": "09_web_scraping.html#a-timeout-to-preview-some-technical-ideas",
    "title": "Web Scraping in R",
    "section": "A timeout to preview some technical ideas",
    "text": "A timeout to preview some technical ideas\n\nHTML structure\nHTML (hypertext markup language) is the formatting language used to create webpages. We can see the core parts of HTML from the rvest vignette.\n\n\nFinding CSS Selectors\nIn order to gather information from a webpage, we must learn the language used to identify patterns of specific information. For example, on the NIH News Releases page, we can see that the data is represented in a consistent pattern of image + title + abstract.\nWe will identify data in a web page using a pattern matching language called CSS Selectors that can refer to specific patterns in HTML, the language used to write web pages.\nFor example:\n\nSelecting by tag:\n\n\"a\" selects all hyperlinks in a webpage (“a” represents “anchor” links in HTML)\n\"p\" selects all paragraph elements\n\nSelecting by ID and class:\n\n\".description\" selects all elements with class equal to “description”\n\nThe . at the beginning is what signifies class selection.\nThis is one of the most common CSS selectors for scraping because in HTML, the class attribute is extremely commonly used to format webpage elements. (Any number of HTML elements can have the same class, which is not true for the id attribute.)\n\n\"#mainTitle\" selects the SINGLE element with id equal to “mainTitle”\n\nThe # at the beginning is what signifies id selection.\n\n\n\n&lt;p class=\"title\"&gt;Title of resource 1&lt;/p&gt;\n&lt;p class=\"description\"&gt;Description of resource 1&lt;/p&gt;\n\n&lt;p class=\"title\"&gt;Title of resource 2&lt;/p&gt;\n&lt;p class=\"description\"&gt;Description of resource 2&lt;/p&gt;\nWarning: Websites change often! So if you are going to scrape a lot of data, it is probably worthwhile to save and date a copy of the website. Otherwise, you may return after some time and your scraping code will include all of the wrong CSS selectors.\n\n\nSelectorGadget\nAlthough you can learn how to use CSS Selectors by hand, we will use a shortcut by installing the Selector Gadget tool.\n\nThere is a version available for Chrome–add it to Chrome via the Chome Web Store.\n\nMake sure to pin the extension to the menu bar. (Click the 3 dots &gt; Extensions &gt; Manage extensions. Click the “Details” button under SelectorGadget and toggle the “Pin to toolbar” option.)\n\nThere is also a version that can be saved as a bookmark in the browser–see here.\n\nYou might watch the Selector Gadget tutorial video.",
    "crumbs": [
      "Web Scraping in R"
    ]
  },
  {
    "objectID": "09_web_scraping.html#case-study-nih-news-releases",
    "href": "09_web_scraping.html#case-study-nih-news-releases",
    "title": "Web Scraping in R",
    "section": "Case Study: NIH News Releases",
    "text": "Case Study: NIH News Releases\nOur goal is to build a data frame with the article title, publication date, and abstract text for the 50 most recent NIH news releases.\nHead over to the NIH News Releases page. Click the Selector Gadget extension icon or bookmark button. As you mouse over the webpage, different parts will be highlighted in orange. Click on the title (but not the live link portion!) of the first news release. You’ll notice that the Selector Gadget information in the lower right describes what you clicked on. (If SelectorGadget ever highlights too much in green, you can click on portions that you do not want to turn them red.)\nScroll through the page to verify that only the information you intend (the description paragraph) is selected. The selector panel shows the CSS selector (.teaser-title) and the number of matches for that CSS selector (10). (You may have to be careful with your clicking–there are two overlapping boxes, and clicking on the link of the title can lead to the CSS selector of “a”.)\n[Pause to Ponder:] Repeat the process above to find the correct selectors for the following fields. Make sure that each matches 10 results:\n\nThe publication date\n\n\n.date-display-single\n\n\nThe article abstract paragraph (which will also include the publication date)\n\n\n.teaser-description\n\n\nRetrieving Data Using rvest and CSS Selectors\nNow that we have identified CSS selectors for the information we need, let’s fetch the data using the rvest package similarly to our approach in 08_table_scraping.qmd.\n\n# check that scraping is allowed (Step 0)\nrobotstxt::paths_allowed(\"https://www.nih.gov/news-events/news-releases\")\n\n\n www.nih.gov                      \n\n\n[1] TRUE\n\n# Step 1: Download the HTML and turn it into an XML file with read_html()\nnih &lt;- read_html(\"https://www.nih.gov/news-events/news-releases\")\n\nFinding the exact node (e.g. “.teaser-title”) is the tricky part. Among all the html code used to produce a webpage, where do you go to grab the content of interest? This is where SelectorGadget comes to the rescue!\n\n# Step 2: Extract specific nodes with html_nodes()\ntitle_temp &lt;- html_nodes(nih, \".teaser-title\")\ntitle_temp\n\n{xml_nodeset (10)}\n [1] &lt;h4 class=\"teaser-title\"&gt;&lt;a href=\"/news-events/news-releases/nih-central ...\n [2] &lt;h4 class=\"teaser-title\"&gt;&lt;a href=\"/news-events/news-releases/nih-funded- ...\n [3] &lt;h4 class=\"teaser-title\"&gt;&lt;a href=\"/news-events/news-releases/longer-brea ...\n [4] &lt;h4 class=\"teaser-title\"&gt;&lt;a href=\"/news-events/news-releases/omalizumab- ...\n [5] &lt;h4 class=\"teaser-title\"&gt;&lt;a href=\"/news-events/news-releases/new-4d-brai ...\n [6] &lt;h4 class=\"teaser-title\"&gt;&lt;a href=\"/news-events/news-releases/nih-funded- ...\n [7] &lt;h4 class=\"teaser-title\"&gt;&lt;a href=\"/news-events/news-releases/single-dose ...\n [8] &lt;h4 class=\"teaser-title\"&gt;&lt;a href=\"/news-events/news-releases/nih-study-f ...\n [9] &lt;h4 class=\"teaser-title\"&gt;&lt;a href=\"/news-events/news-releases/influenza-v ...\n[10] &lt;h4 class=\"teaser-title\"&gt;&lt;a href=\"/news-events/news-releases/therapy-hel ...\n\n# Step 3: Extract content from nodes with html_text(), html_name(), \n#    html_attrs(), html_children(), html_table(), etc.\n# Usually will still need to do some stringr adjustments\ntitle_vec &lt;- html_text(title_temp)\ntitle_vec\n\n [1] \"NIH centralizes peer review to improve efficiency and strengthen integrity \"                   \n [2] \"NIH-funded research team engineers new drug targeting pain sensation pathway\"                  \n [3] \"Longer breastfeeding linked to blood-pressure lowering effects of certain infant gut bacteria \"\n [4] \"Omalizumab treats multi-food allergy better than oral immunotherapy\"                           \n [5] \"New 4D Brain Map reveals potential early warning signs of multiple sclerosis \"                 \n [6] \"NIH-funded clinical trial will evaluate new dengue therapeutic\"                                \n [7] \"Single dose of broadly neutralizing antibody protects macaques from H5N1 influenza\"            \n [8] \"NIH study finds infection-related hospitalizations linked to increased risk of heart failure\"  \n [9] \"Influenza A viruses adapt shape in response to environmental pressures\"                        \n[10] \"Therapy helps peanut-allergic kids tolerate tablespoons of peanut butter\"                      \n\n\nYou can also write this altogether with a pipe:\n\nrobotstxt::paths_allowed(\"https://www.nih.gov/news-events/news-releases\")\n\n\n www.nih.gov                      \n\n\n[1] TRUE\n\nread_html(\"https://www.nih.gov/news-events/news-releases\") |&gt;\n  html_nodes(\".teaser-title\") |&gt;\n  html_text()\n\n [1] \"NIH centralizes peer review to improve efficiency and strengthen integrity \"                   \n [2] \"NIH-funded research team engineers new drug targeting pain sensation pathway\"                  \n [3] \"Longer breastfeeding linked to blood-pressure lowering effects of certain infant gut bacteria \"\n [4] \"Omalizumab treats multi-food allergy better than oral immunotherapy\"                           \n [5] \"New 4D Brain Map reveals potential early warning signs of multiple sclerosis \"                 \n [6] \"NIH-funded clinical trial will evaluate new dengue therapeutic\"                                \n [7] \"Single dose of broadly neutralizing antibody protects macaques from H5N1 influenza\"            \n [8] \"NIH study finds infection-related hospitalizations linked to increased risk of heart failure\"  \n [9] \"Influenza A viruses adapt shape in response to environmental pressures\"                        \n[10] \"Therapy helps peanut-allergic kids tolerate tablespoons of peanut butter\"                      \n\n\nAnd finally we wrap the 4 steps above into the bow and scrape functions from the polite package:\n\nsession &lt;- bow(\"https://www.nih.gov/news-events/news-releases\", force = TRUE)\n\nnih_title &lt;- scrape(session) |&gt;\n  html_nodes(\".teaser-title\") |&gt;\n  html_text()\nnih_title\n\n [1] \"NIH centralizes peer review to improve efficiency and strengthen integrity \"                   \n [2] \"NIH-funded research team engineers new drug targeting pain sensation pathway\"                  \n [3] \"Longer breastfeeding linked to blood-pressure lowering effects of certain infant gut bacteria \"\n [4] \"Omalizumab treats multi-food allergy better than oral immunotherapy\"                           \n [5] \"New 4D Brain Map reveals potential early warning signs of multiple sclerosis \"                 \n [6] \"NIH-funded clinical trial will evaluate new dengue therapeutic\"                                \n [7] \"Single dose of broadly neutralizing antibody protects macaques from H5N1 influenza\"            \n [8] \"NIH study finds infection-related hospitalizations linked to increased risk of heart failure\"  \n [9] \"Influenza A viruses adapt shape in response to environmental pressures\"                        \n[10] \"Therapy helps peanut-allergic kids tolerate tablespoons of peanut butter\"                      \n\n\n\n\nPutting multiple columns of data together.\nNow repeat the process above to extract the publication date and the abstract.\n\nnih_pubdate &lt;- scrape(session) |&gt;\n  html_nodes(\".date-display-single\") |&gt;\n  html_text()\nnih_pubdate\n\n [1] \"March 6, 2025\"     \"March 5, 2025\"     \"March 4, 2025\"    \n [4] \"March 3, 2025\"     \"February 27, 2025\" \"February 11, 2025\"\n [7] \"February 11, 2025\" \"February 11, 2025\" \"February 10, 2025\"\n[10] \"February 10, 2025\"\n\nnih_description &lt;- scrape(session) |&gt;\n  html_nodes(\".teaser-description\") |&gt;\n  html_text()\nnih_description\n\n [1] \"March 6, 2025 —     \\n          The proposed approach is expected to save more than $65 million annually. \"                                                  \n [2] \"March 5, 2025 —     \\n          Study of CB1 receptor has implications for chronic pain treatment. \"                                                         \n [3] \"March 4, 2025 —     \\n          Nursing for at least six months may spur beneficial gut bacteria connected to better heart health years later. \"             \n [4] \"March 3, 2025 —     \\n          High rate of oral immunotherapy side effects in NIH trial explains superiority of omalizumab. \"                              \n [5] \"February 27, 2025 —     \\n          NIH study reveals key players underlying disease onset and repair. \"                                                     \n [6] \"February 11, 2025 —     \\n          Dengue virus sickens as many as 400 million people each year, primarily in tropical and subtropical parts of the world. \"\n [7] \"February 11, 2025 —     \\n          NIH science lays groundwork for future studies in people. \"                                                              \n [8] \"February 11, 2025 —     \\n          Findings highlight the importance of infection prevention measures and personalized heart failure care. \"                \n [9] \"February 10, 2025 —     \\n          NIH study identifies previously unknown adaptation. \"                                                                    \n[10] \"February 10, 2025 —     \\n          NIH trial informs potential treatment strategy for kids who already tolerate half a peanut or more. \"                    \n\n\nCombine these extracted variables into a single tibble. Make sure the variables are formatted correctly - e.g. pubdate has date type, description does not contain the pubdate, etc.\n\n# use tibble() to put multiple columns together into a tibble\nnih_top10 &lt;- tibble(title = nih_title, \n                    pubdate = nih_pubdate, \n                    description = nih_description)\nnih_top10\n\n# A tibble: 10 × 3\n   title                                                     pubdate description\n   &lt;chr&gt;                                                     &lt;chr&gt;   &lt;chr&gt;      \n 1 \"NIH centralizes peer review to improve efficiency and s… March … \"March 6, …\n 2 \"NIH-funded research team engineers new drug targeting p… March … \"March 5, …\n 3 \"Longer breastfeeding linked to blood-pressure lowering … March … \"March 4, …\n 4 \"Omalizumab treats multi-food allergy better than oral i… March … \"March 3, …\n 5 \"New 4D Brain Map reveals potential early warning signs … Februa… \"February …\n 6 \"NIH-funded clinical trial will evaluate new dengue ther… Februa… \"February …\n 7 \"Single dose of broadly neutralizing antibody protects m… Februa… \"February …\n 8 \"NIH study finds infection-related hospitalizations link… Februa… \"February …\n 9 \"Influenza A viruses adapt shape in response to environm… Februa… \"February …\n10 \"Therapy helps peanut-allergic kids tolerate tablespoons… Februa… \"February …\n\n# now clean the data\nnih_top10 &lt;- nih_top10 |&gt;\n  mutate(pubdate = mdy(pubdate),\n         description = str_trim(str_replace(description, \".*\\\\n\", \"\")))\nnih_top10\n\n# A tibble: 10 × 3\n   title                                                  pubdate    description\n   &lt;chr&gt;                                                  &lt;date&gt;     &lt;chr&gt;      \n 1 \"NIH centralizes peer review to improve efficiency an… 2025-03-06 The propos…\n 2 \"NIH-funded research team engineers new drug targetin… 2025-03-05 Study of C…\n 3 \"Longer breastfeeding linked to blood-pressure loweri… 2025-03-04 Nursing fo…\n 4 \"Omalizumab treats multi-food allergy better than ora… 2025-03-03 High rate …\n 5 \"New 4D Brain Map reveals potential early warning sig… 2025-02-27 NIH study …\n 6 \"NIH-funded clinical trial will evaluate new dengue t… 2025-02-11 Dengue vir…\n 7 \"Single dose of broadly neutralizing antibody protect… 2025-02-11 NIH scienc…\n 8 \"NIH study finds infection-related hospitalizations l… 2025-02-11 Findings h…\n 9 \"Influenza A viruses adapt shape in response to envir… 2025-02-10 NIH study …\n10 \"Therapy helps peanut-allergic kids tolerate tablespo… 2025-02-10 NIH trial …\n\n\nNOW - continue this process to build a tibble with the most recent 50 NIH news releases, which will require that you iterate over 5 webpages! You should write at least one function, and you will need iteration–use both a for loop and appropriate map_() functions from purrr. Some additional hints:\n\nMouse over the page buttons at the very bottom of the news home page to see what the URLs look like.\nInclude Sys.sleep(2) in your function to respect the Crawl-delay: 2 in the NIH robots.txt file.\nRecall that bind_rows() from dplyr takes a list of data frames and stacks them on top of each other.\n\n[Pause to Ponder:] Create a function to scrape a single NIH press release page by filling missing pieces labeled ???:\n\n# Helper function to reduce html_nodes() |&gt; html_text() code duplication\nget_text_from_page &lt;- function(page, css_selector) {\n  ???\n}\n\n# Main function to scrape and tidy desired attributes\nscrape_page &lt;- function(url) {\n    Sys.sleep(2)\n    page &lt;- read_html(url)\n    article_titles &lt;- get_text_from_page(???)\n    article_dates &lt;- get_text_from_page(???)\n    article_dates &lt;- mdy(article_dates)\n    article_description &lt;- get_text_from_page(???)\n    article_description &lt;- str_trim(str_replace(article_description, \n                                                \".*\\\\n\", \n                                                \"\")\n                                    )\n    \n    tibble(\n      ???\n    )\n}\n\n[Pause to Ponder:] Use a for loop over the first 5 pages:\n\npages &lt;- vector(\"list\", length = 5)\n\nfor (i in 0:4) {\n  url &lt;- str_c(???)\n  pages[[i + 1]] &lt;- ???\n}\n\ndf_articles &lt;- bind_rows(pages)\nhead(df_articles)\n\n[Pause to Ponder:] Use map functions in the purrr package:\n\n# Create a character vector of URLs for the first 5 pages\nbase_url &lt;- \"???\"\nurls_all_pages &lt;- c(base_url, str_c(???))\n\npages2 &lt;- purrr::map(???)\ndf_articles2 &lt;- bind_rows(pages2)\nhead(df_articles2)",
    "crumbs": [
      "Web Scraping in R"
    ]
  },
  {
    "objectID": "09_web_scraping.html#on-your-own",
    "href": "09_web_scraping.html#on-your-own",
    "title": "Web Scraping in R",
    "section": "On Your Own",
    "text": "On Your Own\n\nGo to https://www.bestplaces.net and search for Minneapolis, Minnesota. This is a site some people use when comparing cities they might consider working in and/or moving to. Using SelectorGadget, extract the following pieces of information from the Minneapolis page:\n\n\nproperty crime (on a scale from 0 to 100)\nminimum income required for a single person to live comfortably\naverage monthly rent for a 2-bedroom apartment\nthe “about” paragraph (the very first paragraph above “Location Details”)\n\n\nWrite a function called scrape_bestplaces() with arguments for state and city. When you run, for example, scrape_bestplaces(\"minnesota\", \"minneapolis\"), the output should be a 1 x 6 tibble with columns for state, city, crime, min_income_single, rent_2br, and about.\nCreate a 5 x 6 tibble by running scrape_bestplaces() 5 times with 5 cities you are interested in. You might have to combine tibbles using bind_rows(). Be sure you look at the URL at bestplaces.net for the various cities to make sure it works as you expect. For bonus points, create the same 5 x 6 tibble for the same 5 cities using purrr:map2!",
    "crumbs": [
      "Web Scraping in R"
    ]
  }
]