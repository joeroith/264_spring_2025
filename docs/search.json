[
  {
    "objectID": "01_review164.html",
    "href": "01_review164.html",
    "title": "Review of Data Science 1",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\n\nDeterminants of COVID vaccination rates\nFirst, a little detour to describe several alternatives for reading in data:\nIf you navigate to my Github account, and find the 264_spring_2025 repo, there is a Data folder inside. You can then click on vacc_Mar21.csv to see the data we want to download. This link should also get you there, but it’s good to be able to navigate there yourself.\n\n# Approach 1\n1vaccine_data &lt;- read_csv(\"Data/vaccinations_2021.csv\")\n\n# Approach 2\n2vaccine_data &lt;- read_csv(\"~/264_spring_2025/Data/vaccinations_2021.csv\")\n\n# Approach 3\n3vaccine_data &lt;- read_csv(\"https://joeroith.github.io/264_spring_2025/Data/vaccinations_2021.csv\")\n\n# Approach 4\n4vaccine_data &lt;- read_csv(\"https://raw.githubusercontent.com/joeroith/264_spring_2025/refs/heads/main/Data/vaccinations_2021.csv\")\n\n\n1\n\nApproach 1: create a Data folder in the same location where this .qmd file resides, and then store vaccinations_2021.csv in that Data folder\n\n2\n\nApproach 2: give R the complete path to the location of vaccinations_2021.csv, starting with Home (~)\n\n3\n\nApproach 3: link to our course webpage, and then know we have a Data folder containing all our csvs\n\n4\n\nApproach 4: navigate to the data in GitHub, hit the Raw button, and copy that link\n\n\n\n\nA recent Stat 272 project examined determinants of covid vaccination rates at the county level. Our data set contains 3053 rows (1 for each county in the US) and 14 columns; here is a quick description of the variables we’ll be using:\n\nstate = state the county is located in\ncounty = name of the county\nregion = region the state is located in\nmetro_status = Is the county considered “Metro” or “Non-metro”?\nrural_urban_code = from 1 (most urban) to 9 (most rural)\nperc_complete_vac = percent of county completely vaccinated as of 11/9/21\ntot_pop = total population in the county\nvotes_Trump = number of votes for Trump in the county in 2020\nvotes_Biden = number of votes for Biden in the county in 2020\nperc_Biden = percent of votes for Biden in the county in 2020\ned_somecol_perc = percent with some education beyond high school (but not a Bachelor’s degree)\ned_bachormore_perc = percent with a Bachelor’s degree or more\nunemployment_rate_2020 = county unemployment rate in 2020\nmedian_HHincome_2019 = county’s median household income in 2019\n\n\nConsider only Minnesota and its surrounding states (Iowa, Wisconsin, North Dakota, and South Dakota). We want to examine the relationship between the percentage who voted for Biden and the percentage of complete vaccinations by state. Generate two plots to examine this relationship:\n\n\nA scatterplot with points and smoothers colored by state. Make sure the legend is ordered in a meaningful way, and include good labels on your axes and your legend. Also leave off the error bars from your smoothers.\nOne plot per state containing a scatterplot and a smoother.\n\nDescribe which plot you prefer and why. What can you learn from your preferred plot?\n\nWe wish to compare the proportions of counties in each region with median household income above the national median ($69,560).\n\n\nFill in the blanks below to produce a segmented bar plot with regions ordered from highest proportion above the median to lowest.\nCreate a table of proportions by region to illustrate that your bar plot in (a) is in the correct order (you should find two regions that are really close when you just try to eyeball differences).\nExplain why we can replace fct_relevel(region, FILL IN CODE) with\n\nmutate(region_sort = fct_reorder(region, median_HHincome_2019 &lt; 69560, .fun = mean))\nbut not\nmutate(region_sort = fct_reorder(region, median_HHincome_2019 &lt; 69560))\n\nvaccine_data |&gt;\n  mutate(HHincome_vs_national = ifelse(median_HHincome_2019 &lt; 69560, FILL IN CODE)) |&gt;\n  mutate(region_sort = fct_relevel(region, FILL IN CODE)) |&gt;\n  ggplot(mapping = aes(x = region_sort, fill = HHincome_vs_national)) +\n    geom_bar(position = \"fill\")\n\n\nWe want to examine the distribution of total county populations and then see how it’s related to vaccination rates.\n\n\nCarefully and thoroughly explain why the two histograms below provide different plots.\n\n\nvaccine_data |&gt;\n  mutate(tot_pop_millions = tot_pop / 1000000) |&gt;\n  ggplot(mapping = aes(x = tot_pop_millions)) +\n    geom_histogram(bins = 40) +\n    labs(x = \"Total population in millions\")\n\n\n\n\n\n\n\nvaccine_data |&gt;\n  mutate(tot_pop_millions = tot_pop %/% 1000000) |&gt;\n  ggplot(mapping = aes(x = tot_pop_millions)) +\n    geom_histogram(bins = 40) +\n    labs(x = \"Total population in millions\")\n\n\n\n\n\n\n\n\n\nFind the top 5 counties in terms of total population.\nPlot a histogram of logged population and describe this distribution.\nPlot the relationship between log population and percent vaccinated using separate colors for Metro and Non-metro counties (be sure there’s no 3rd color used for NAs). Reduce the size and transparency of each point to make the plot more readable. Describe what you can learn from this plot.\n\n\nProduce 3 different plots for illustrating the relationship between the rural_urban_code and percent vaccinated. Hint: you can sometimes turn numeric variables into categorical variables for plotting purposes (e.g. as.factor(), ifelse()).\n\nState your favorite plot, why you like it better than the other two, and what you can learn from your favorite plot. Create an alt text description of your favorite plot, using the Four Ingredient Model. See this link for reminders and references about alt text.\n\nBEFORE running the code below, sketch the plot that will be produced by R. AFTER running the code, describe what conclusion(s) can we draw from this plot?\n\n\nvaccine_data |&gt;\n  filter(!is.na(perc_Biden)) |&gt;\n  mutate(big_states = fct_lump(state, n = 10)) |&gt;\n  group_by(big_states) |&gt;\n  summarize(IQR_Biden = IQR(perc_Biden)) |&gt;\n  mutate(big_states = fct_reorder(big_states, IQR_Biden)) |&gt;\n  ggplot() + \n    geom_point(aes(x = IQR_Biden, y = big_states))\n\n\nIn this question we will focus only on the 12 states in the Midwest (i.e. where region == “Midwest”).\n\n\nCreate a tibble with the following information for each state. Order states from least to greatest state population.\n\n\nnumber of different rural_urban_codes represented among the state’s counties (there are 9 possible)\ntotal state population\nproportion of Metro counties\nmedian unemployment rate\n\n\nUse your tibble in (a) to produce a plot of the relationship between proportion of Metro counties and median unemployment rate. Points should be colored by the number of different rural_urban_codes in a state, but a single linear trend should be fit to all points. What can you conclude from the plot?\n\n\nGenerate an appropriate plot to compare vaccination rates between two subregions of the US: New England (which contains the states Maine, Vermont, New Hampshire, Massachusetts, Connecticut, Rhode Island) and the Upper Midwest (which, according to the USGS, contains the states Minnesota, Wisconsin, Michigan, Illinois, Indiana, and Iowa). What can you conclude from your plot?\n\nIn this next section, we consider a few variables that could have been included in our data set, but were NOT. Thus, you won’t be able to write and test code, but you nevertheless should be able to use your knowledge of the tidyverse to answer these questions.\nHere are the hypothetical variables:\n\nHR_party = party of that county’s US Representative (Republican, Democrat, Independent, Green, or Libertarian)\npeople_per_MD = number of residents per doctor (higher values = fewer doctors)\nperc_over_65 = percent of residents over 65 years old\nperc_white = percent of residents who identify as white\n\n\nHypothetical R chunk #1:\n\n\n# Hypothetical R chunk 1\ntemp &lt;- vaccine_data |&gt;\n  mutate(new_perc_vac = ifelse(perc_complete_vac &gt; 95, NA, perc_complete_vac),\n         MD_group = cut_number(people_per_MD, 3)) |&gt;\n  group_by(MD_group) |&gt;\n  summarise(n = n(),\n            mean_perc_vac = mean(new_perc_vac, na.rm = TRUE),\n            mean_white = mean(perc_white, na.rm = TRUE))\n\n\nDescribe the tibble temp created above. What would be the dimensions? What do rows and columns represent?\nWhat would happen if we replaced new_perc_vac = ifelse(perc_complete_vac &gt; 95, NA, perc_complete_vac) with new_perc_vac = ifelse(perc_complete_vac &gt; 95, perc_complete_vac, NA)?\nWhat would happen if we replaced mean_white = mean(perc_white, na.rm = TRUE) with mean_white = mean(perc_white)?\nWhat would happen if we removed group_by(MD_group)?\n\n\nHypothetical R chunk #2:\n\n\n# Hypothetical R chunk 2\nggplot(data = vaccine_data) +\n  geom_point(mapping = aes(x = perc_over_65, y = perc_complete_vac, \n                           color = HR_party)) +\n  geom_smooth()\n\ntemp &lt;- vaccine_data |&gt;\n  group_by(HR_party) |&gt;\n  summarise(var1 = n()) |&gt;\n  arrange(desc(var1)) |&gt;\n  slice_head(n = 3)\n\nvaccine_data |&gt;\n  ggplot(mapping = aes(x = fct_reorder(HR_party, perc_over_65, .fun = median), \n                       y = perc_over_65)) +\n    geom_boxplot()\n\n\nWhy would the first plot produce an error?\nDescribe the tibble temp created above. What would be the dimensions? What do rows and columns represent?\nWhat would happen if we replaced fct_reorder(HR_party, perc_over_65, .fun = median) with HR_party?\n\n\nHypothetical R chunk #3:\n\n\n# Hypothetical R chunk 3\nvaccine_data |&gt;\n  filter(!is.na(people_per_MD)) |&gt;\n  mutate(state_lump = fct_lump(state, n = 4)) |&gt;\n  group_by(state_lump, rural_urban_code) |&gt;\n  summarise(mean_people_per_MD = mean(people_per_MD)) |&gt;\n  ggplot(mapping = aes(x = rural_urban_code, y = mean_people_per_MD, \n      colour = fct_reorder2(state_lump, rural_urban_code, mean_people_per_MD))) +\n    geom_line()\n\n\nDescribe the tibble piped into the ggplot above. What would be the dimensions? What do rows and columns represent?\nCarefully describe the plot created above.\nWhat would happen if we removed filter(!is.na(people_per_MD))?\nWhat would happen if we replaced fct_reorder2(state_lump, rural_urban_code, mean_people_per_MD) with state_lump?",
    "crumbs": [
      "Review of Data Science 1"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SDS 264: Data Science 2 (Spring 2025)",
    "section": "",
    "text": "Quarto\n\n\n\n\n\n\n\ntidyverse\n\n\n\n\n\n\nKey links for SDS 264\n\nCourse syllabus\nRStudio server\nmoodle\nGitHub source code for this website"
  },
  {
    "objectID": "miniproject1.html",
    "href": "miniproject1.html",
    "title": "Mini-Project 1: Maps",
    "section": "",
    "text": "Overview\nYou will produce choropleth maps illustrating two different characteristics – one numeric and one categorical – that have been measured for each US state (you can choose to exclude Alaska and Hawaii), like we did in the “Creating Informative Maps” activity. Just as we found state-level data from both a vaccine data set and the poliscidata package, you should find your own state-level data that is interesting to you.\nA few additional details for this mini-project:\n\nYou should create two versions of each plot – one that is static and one that is interactive.\nBe sure to include a note on your plots with your data source.\nYou should be able to merge your state-level data with the state mapping data sets we used in class.\nBe sure you label your plot well and provide a description of what insights can be gained from each static plot. For one of your static plots, this should be in the form of alt-text, using the “Four Ingredient Model” in the article from class.\nCheck out this rubric for Mini-Project 1.\n\n\n\nSubmission and Timeline\nMini-Project 1 must be submitted on Moodle by 11:00 PM on Fri Feb 21. You should submit your two static plots, along with descriptions and alt-text, in a pdf document. For your interactive plots, just submit a GitHub link where I can see your code that would produce a nice html document. Then, in Mini-Project 2, I will ask you to build a webpage where you will link to these interactive html pages.",
    "crumbs": [
      "Mini-Project 1: Maps"
    ]
  },
  {
    "objectID": "rtipoftheday.html",
    "href": "rtipoftheday.html",
    "title": "R Tip of the Day",
    "section": "",
    "text": "Signup Sheet\nCool presentations using Quarto\nMy example of an RTD presentation with revealjs in quarto\nRTD rubric"
  },
  {
    "objectID": "02_maps.html",
    "href": "02_maps.html",
    "title": "Creating informative maps",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\n\n# Initial packages required (we'll be adding more)\nlibrary(tidyverse)\nlibrary(mdsr)      # package associated with our MDSR book\n\n\nOpening example\nHere is a simple choropleth map example from Section 3.2.3 of MDSR. Note how we use an underlying map with strategic shading to convey a story about a variable that’s been measured on each country.\n\n# CIACountries is a 236 x 8 data set with information on each country\n#   taken from the CIA factbook - gdp, education, internet use, etc.\nhead(CIACountries)\nCIACountries |&gt;\n  select(country, oil_prod) |&gt;\n  mutate(oil_prod_disc = cut(oil_prod, \n    breaks = c(0, 1e3, 1e5, 1e6, 1e7, 1e8), \n    labels = c(\"&gt;1000\", \"&gt;10,000\", \"&gt;100,000\", \"&gt;1 million\", \n               \"&gt;10 million\"))) |&gt;\n1  mosaic::mWorldMap(key = \"country\") +\n  geom_polygon(aes(fill = oil_prod_disc)) + \n  scale_fill_brewer(\"Oil Prod. (bbl/day)\", na.value = \"white\") +\n  theme(legend.position = \"top\")\n\n\n1\n\nWe won’t use mWorldMap often, but it’s a good quick illustration\n\n\n\n\n\n\n\n\n\n\n\n         country      pop    area oil_prod   gdp educ   roadways net_users\n1    Afghanistan 32564342  652230        0  1900   NA 0.06462444       &gt;5%\n2        Albania  3029278   28748    20510 11900  3.3 0.62613051      &gt;35%\n3        Algeria 39542166 2381741  1420000 14500  4.3 0.04771929      &gt;15%\n4 American Samoa    54343     199        0 13000   NA 1.21105528      &lt;NA&gt;\n5        Andorra    85580     468       NA 37200   NA 0.68376068      &gt;60%\n6         Angola 19625353 1246700  1742000  7300  3.5 0.04125211      &gt;15%\n\n\n\n\nChoropleth Maps\nWhen you have specific regions (e.g. countries, states, counties, census tracts,…) and a value associated with each region.\nA choropleth map will color the entire region according to the value. For example, let’s consider state vaccination data from March 2021.\n\nvaccines &lt;- read_csv(\"https://joeroith.github.io/264_spring_2025/Data/vacc_Mar21.csv\") \n\nvacc_mar13 &lt;- vaccines |&gt;\n  filter(Date ==\"2021-03-13\") |&gt;\n  select(State, Date, people_vaccinated_per100, share_doses_used, Governor)\n\nvacc_mar13\n\n# A tibble: 50 × 5\n   State       Date       people_vaccinated_per100 share_doses_used Governor\n   &lt;chr&gt;       &lt;date&gt;                        &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;   \n 1 Alabama     2021-03-13                     17.2            0.671 R       \n 2 Alaska      2021-03-13                     27.0            0.686 R       \n 3 Arizona     2021-03-13                     21.5            0.821 R       \n 4 Arkansas    2021-03-13                     19.2            0.705 R       \n 5 California  2021-03-13                     20.3            0.726 D       \n 6 Colorado    2021-03-13                     20.8            0.801 D       \n 7 Connecticut 2021-03-13                     26.2            0.851 D       \n 8 Delaware    2021-03-13                     20.2            0.753 D       \n 9 Florida     2021-03-13                     20.1            0.766 R       \n10 Georgia     2021-03-13                     15.2            0.674 R       \n# ℹ 40 more rows\n\n\nThe tricky part of choropleth maps is getting the shapes (polygons) that make up the regions. This is really a pretty complex set of lines for R to draw!\nLuckily, some maps are already created in R in the maps package.\n\nlibrary(maps)\nus_states &lt;- map_data(\"state\")\nhead(us_states)\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\nus_states |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(fill = \"white\", color = \"black\")\n\n\n\n\n\n\n\n\n[Pause to ponder:] What might the group and order columns represent?\nOther maps provided by the maps package include US counties, France, Italy, New Zealand, and two different views of the world. If you want maps of other countries or regions, you can often find them online.\nWhere the really cool stuff happens is when we join our data to the us_states dataframe. Notice that the state name appears in the “region” column of us_states, and that the state name is in all small letters. In vacc_mar13, the state name appears in the State column and is in lower case. Thus, we have to be very careful when we join the state vaccine info to the state geography data.\nRun this line by line to see what it does:\n\nvacc_mar13 &lt;- vacc_mar13 |&gt;\n  mutate(State = str_to_lower(State))\n\nvacc_mar13 |&gt;\n  right_join(us_states, by = c(\"State\" = \"region\")) |&gt;\n  rename(region = State) |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(aes(fill = people_vaccinated_per100), color = \"black\")\n\n\n\n\n\n\n\n\noops, New York appears to be a problem.\n\nvacc_mar13 |&gt;\n  anti_join(us_states, by = c(\"State\" = \"region\"))\n\n# A tibble: 3 × 5\n  State          Date       people_vaccinated_per100 share_doses_used Governor\n  &lt;chr&gt;          &lt;date&gt;                        &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;   \n1 alaska         2021-03-13                     27.0            0.686 R       \n2 hawaii         2021-03-13                     22.8            0.759 D       \n3 new york state 2021-03-13                     21.7            0.764 D       \n\nus_states |&gt;\n  anti_join(vacc_mar13, by = c(\"region\" = \"State\")) |&gt;\n  count(region)\n\n                region   n\n1 district of columbia  10\n2             new york 495\n\n\n[Pause to ponder:] What did we learn by running anti_join() above?\nNotice that the us_states map also includes only the contiguous 48 states. This gives an example of creating really beautiful map insets for Alaska and Hawaii.\n\nvacc_mar13 &lt;- vacc_mar13 |&gt;\n  mutate(State = str_replace(State, \" state\", \"\"))\n\nvacc_mar13 |&gt;\n  anti_join(us_states, by = c(\"State\" = \"region\"))\n\n# A tibble: 2 × 5\n  State  Date       people_vaccinated_per100 share_doses_used Governor\n  &lt;chr&gt;  &lt;date&gt;                        &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;   \n1 alaska 2021-03-13                     27.0            0.686 R       \n2 hawaii 2021-03-13                     22.8            0.759 D       \n\nus_states |&gt;\n  anti_join(vacc_mar13, by = c(\"region\" = \"State\")) %&gt;%\n  count(region)\n\n                region  n\n1 district of columbia 10\n\n\nBetter.\n\nlibrary(viridis) # for color schemes\nvacc_mar13 |&gt;\n  right_join(us_states, by = c(\"State\" = \"region\")) |&gt;\n  rename(region = State) |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(aes(fill = people_vaccinated_per100), color = \"black\") + \n  labs(fill = \"People Vaccinated\\nper 100 pop.\") +\n1  coord_map() +\n2  theme_void() +\n3  scale_fill_viridis()\n\n\n1\n\nThis scales the longitude and latitude so that the shapes look correct. coord_quickmap() can also work here - it’s less exact but faster.\n\n2\n\nThis theme can give you a really clean look\n\n3\n\nYou can change the fill scale for different color schemes.\n\n\n\n\n\n\n\n\n\n\n\nYou can also use a categorical variable to color regions:\n\nvacc_mar13 |&gt;\n  right_join(us_states, by = c(\"State\" = \"region\")) |&gt;\n  rename(region = State) |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(aes(fill = Governor), color = \"darkgrey\", linewidth = 0.2) + \n  labs(fill = \"Governor\") +\n  coord_map() + \n  theme_void() +  \n1  scale_fill_manual(values = c(\"blue\", \"red\"))\n\n\n1\n\nYou can change the fill scale for different color schemes.\n\n\n\n\n\n\n\n\n\n\n\nNote: Map projections are actually pretty complicated, especially if you’re looking at large areas (e.g. world maps) or drilling down to very small regions where a few feet can make a difference (e.g. tracking a car on a map of roads). It’s impossible to preserve both shape and area when projecting an (imperfect) sphere onto a flat surface, so that’s why you sometimes see such different maps of the world. This is why packages like maps which connect latitude-longitude points are being phased out in favor of packages like sf with more GIS functionality. We won’t get too deep into GIS in this class, but to learn more, take Spatial Data Analysis!!\n\n\nMultiple maps!\nYou can still use data viz tools from Data Science 1 (like faceting) to create things like time trends in maps:\n\nlibrary(lubridate)\nweekly_vacc &lt;- vaccines |&gt;\n  mutate(State = str_to_lower(State)) |&gt;\n  mutate(State = str_replace(State, \" state\", \"\"),\n         week = week(Date)) |&gt;\n  group_by(week, State) |&gt;\n  summarize(date = first(Date),\n            mean_daily_vacc = mean(daily_vaccinated/est_population*1000)) |&gt;\n  right_join(us_states, by =c(\"State\" = \"region\")) |&gt;\n  rename(region = State)\n\nweekly_vacc |&gt;\n  filter(week &gt; 2, week &lt; 11) |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(aes(fill = mean_daily_vacc), color = \"darkgrey\", \n               linewidth = 0.1) + \n  labs(fill = \"Weekly Average Daily Vaccinations per 1000\") +\n  coord_map() + \n  theme_void() + \n  scale_fill_viridis() + \n  facet_wrap(~date) + \n  theme(legend.position = \"bottom\") \n\n\n\n\n\n\n\n\n[Pause to ponder:] are we bothered by the warning about many-to-many when you run the code above?\n\n\nOther cool state maps\n\nstatebin (square representation of states)\n\nlibrary(statebins) # may need to install\n\nvacc_mar13 |&gt;\n  mutate(State = str_to_title(State)) |&gt;\n  statebins(state_col = \"State\",\n            value_col = \"people_vaccinated_per100\") + \n1  theme_statebins() +\n  labs(fill = \"People Vaccinated per 100\")\n\n\n1\n\nOne nice layout. You can customize with usual ggplot themes.\n\n\n\n\n\n\n\n\n\n\n\n[Pause to ponder:] Why might one use a map like above instead of our previous choropleth maps?\nI used this example to create the code above. The original graph is located here.\n\n\n\nInteractive point maps with leaflet\nTo add even more power and value to your plots, we can add interactivity. For now, we will use the leaflet package, but later in the course we will learn even more powerful and flexible approaches for creating interactive plots and webpages.\nFor instance, here is a really simple plot with a pop-up window:\n\nlibrary(leaflet)\n\nleaflet() |&gt; \n1  addTiles() |&gt;\n2  setView(-93.1832, 44.4597, zoom = 17) |&gt;\n3  addPopups(-93.1832, 44.4597, 'Here is the &lt;b&gt;Regents Hall of Mathematical Sciences&lt;/b&gt;, home of the Statistics and Data Science program at St. Olaf College')\n\n\n1\n\naddTiles() uses OpenStreetMap, an awesome open-source mapping resource, as the default tile layer (background map)\n\n2\n\nsetView() centers the map at a specific latitude and longitude, then zoom controls how much of the surrounding area is shown\n\n3\n\nadd a popup message (with html formatting) that can be clicked on or off\n\n\n\n\n\n\n\n\nLeaflet is not part of the tidyverse, but the structure of its code is pretty similar and it also plays well with piping.\nLet’s try pop-up messages with a data set containing Airbnb listings in the Boston area:\n\nleaflet() |&gt;\n    addTiles() |&gt;\n    setView(lng = mean(airbnb.df$Long), lat = mean(airbnb.df$Lat), \n            zoom = 13) |&gt; \n    addCircleMarkers(data = airbnb.df,\n        lat = ~ Lat, \n        lng = ~ Long, \n        popup = ~ AboutListing, \n        radius = ~ S_Accomodates,  \n        # These last options describe how the circles look\n        weight = 2,\n        color = \"red\", \n        fillColor = \"yellow\")\n\n\n\n\n\n[Pause to ponder:] List similarities and differences between leaflet plots and ggplots.\n\n\nInteractive choropleth maps with leaflet\nOK. Now let’s see if we can put things together and duplicate the interactive choropleth map found here showing population density by state in the US.\n\nA preview to shapefiles and the sf package\n\n1library(sf)\n2states &lt;- read_sf(\"https://rstudio.github.io/leaflet/json/us-states.geojson\")\n3class(states)\nstates\n\n\n1\n\nsf stands for “simple features”\n\n2\n\nFrom https://leafletjs.com/examples/choropleth/us-states.js\n\n3\n\nNote that states has class sf in addition to the usual tbl and df\n\n\n\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\nSimple feature collection with 52 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -188.9049 ymin: 17.92956 xmax: -65.6268 ymax: 71.35163\nGeodetic CRS:  WGS 84\n# A tibble: 52 × 4\n   id    name                  density                                  geometry\n   &lt;chr&gt; &lt;chr&gt;                   &lt;dbl&gt;                        &lt;MULTIPOLYGON [°]&gt;\n 1 01    Alabama                 94.6  (((-87.3593 35.00118, -85.60667 34.98475…\n 2 02    Alaska                   1.26 (((-131.602 55.11798, -131.5692 55.28229…\n 3 04    Arizona                 57.0  (((-109.0425 37.00026, -109.048 31.33163…\n 4 05    Arkansas                56.4  (((-94.47384 36.50186, -90.15254 36.4963…\n 5 06    California             242.   (((-123.2333 42.00619, -122.3789 42.0116…\n 6 08    Colorado                49.3  (((-107.9197 41.00391, -105.729 40.99843…\n 7 09    Connecticut            739.   (((-73.05353 42.03905, -71.79931 42.0226…\n 8 10    Delaware               464.   (((-75.41409 39.80446, -75.5072 39.68396…\n 9 11    District of Columbia 10065    (((-77.03526 38.99387, -76.90929 38.8952…\n10 12    Florida                353.   (((-85.49714 30.99754, -85.00421 31.0030…\n# ℹ 42 more rows\n\n\nFor maps in leaflet that show boundaries and not just points, we need to input a shapefile rather than a series of latitude-longitude combinations like we did for the maps package. In the example we’re emulating, they use the read_sf() function from the sf package to read in data. While our us_states data frame from the maps package contained 15537 rows, our simple features object states contains only 52 rows - one per state. Importantly, states contains a column called geometry, which is a “multipolygon” with all the information necessary to draw a specific state. Also, while states can be treated as a tibble or data frame, it is also an sf class object with a specific “geodetic coordinate reference system”. Again, take Spatial Data Analysis for more on shapefiles and simple features!\nNote also that the authors of this example have already merged state population densities with state geometries, but if we wanted to merge in other state characteristics using the name column as a key, we could definitely do this!\nFirst we’ll start with a static plot using a simple features object and geom_sf():\n\n# Create density bins as on the webpage\nstate_plotting_sf &lt;- states |&gt;\n  mutate(density_intervals = cut(density, n = 8,\n          breaks = c(0, 10, 20, 50, 100, 200, 500, 1000, Inf))) |&gt;\n  filter(!(name %in% c(\"Alaska\", \"Hawaii\", \"Puerto Rico\")))\n\nggplot(data = state_plotting_sf) + \n  geom_sf(aes(fill = density_intervals), colour = \"white\", linetype = 2) + \n#  geom_sf_label(aes(label = density)) +   # labels too busy here\n  theme_void() +  \n  scale_fill_brewer(palette = \"YlOrRd\") \n\n\n\n\n\n\n\n\nNow let’s use leaflet to create an interactive plot!\n\n# Create our own category bins for population densities\n#   and assign the yellow-orange-red color palette\nbins &lt;- c(0, 10, 20, 50, 100, 200, 500, 1000, Inf)\npal &lt;- colorBin(\"YlOrRd\", domain = states$density, bins = bins)\n\n# Create labels that pop up when we hover over a state.  The labels must\n#   be part of a list where each entry is tagged as HTML code.\nlibrary(htmltools)\nlibrary(glue)\n\nstates &lt;- states |&gt;\n  mutate(labels = str_c(name, \": \", density, \" people / sq mile\"))\n\n# If want more HTML formatting, use these lines instead of those above:\n#states &lt;- states |&gt;\n#  mutate(labels = glue(\"&lt;strong&gt;{name}&lt;/strong&gt;&lt;br/&gt;{density} people / #mi&lt;sup&gt;2&lt;/sup&gt;\"))\n\nlabels &lt;- lapply(states$labels, HTML)\n\nleaflet(states) |&gt;\n  setView(-96, 37.8, 4) |&gt;\n  addTiles() |&gt;\n  addPolygons(\n    fillColor = ~pal(density),\n    weight = 2,\n    opacity = 1,\n    color = \"white\",\n    dashArray = \"3\",\n    fillOpacity = 0.7,\n    highlightOptions = highlightOptions(\n      weight = 5,\n      color = \"#666\",\n      dashArray = \"\",\n      fillOpacity = 0.7,\n      bringToFront = TRUE),\n    label = labels,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n      textsize = \"15px\",\n      direction = \"auto\")) |&gt;\n  addLegend(pal = pal, values = ~density, opacity = 0.7, title = NULL,\n    position = \"bottomright\")\n\n\n\n\n\n[Pause to ponder:] Pick several formatting options in the code above, determine what they do, and then change them to create a customized look.\n\n\n\nOn Your Own\nThe states dataset in the poliscidata package contains 135 variables on each of the 50 US states. See here for more detail.\nYour task is to create a two meaningful choropleth plots, one using a numeric variable and one using a categorical variable from poliscidata::states. You should make two versions of each plot: a static plot using the maps package and ggplot(), and an interactive plot using the sf package and leaflet(). Write a sentence or two describing what you can learn from each plot.\nHere’s some R code and hints to get you going:\n\n# Get info to draw US states for geom_polygon (connect the lat-long points)\nlibrary(maps)\nstates_polygon &lt;- as_tibble(map_data(\"state\")) |&gt;\n  select(region, group, order, lat, long)\n\n# See what the state (region) levels look like in states_polygon\nunique(states_polygon$region)\n\n [1] \"alabama\"              \"arizona\"              \"arkansas\"            \n [4] \"california\"           \"colorado\"             \"connecticut\"         \n [7] \"delaware\"             \"district of columbia\" \"florida\"             \n[10] \"georgia\"              \"idaho\"                \"illinois\"            \n[13] \"indiana\"              \"iowa\"                 \"kansas\"              \n[16] \"kentucky\"             \"louisiana\"            \"maine\"               \n[19] \"maryland\"             \"massachusetts\"        \"michigan\"            \n[22] \"minnesota\"            \"mississippi\"          \"missouri\"            \n[25] \"montana\"              \"nebraska\"             \"nevada\"              \n[28] \"new hampshire\"        \"new jersey\"           \"new mexico\"          \n[31] \"new york\"             \"north carolina\"       \"north dakota\"        \n[34] \"ohio\"                 \"oklahoma\"             \"oregon\"              \n[37] \"pennsylvania\"         \"rhode island\"         \"south carolina\"      \n[40] \"south dakota\"         \"tennessee\"            \"texas\"               \n[43] \"utah\"                 \"vermont\"              \"virginia\"            \n[46] \"washington\"           \"west virginia\"        \"wisconsin\"           \n[49] \"wyoming\"             \n\n# Get info to draw US states for geom_sf and leaflet (simple features object \n#   with multipolygon geometry column)\nlibrary(sf)\nstates_sf &lt;- read_sf(\"https://rstudio.github.io/leaflet/json/us-states.geojson\") |&gt;\n  select(name, geometry)\n\n# See what the state (name) levels look like in states_sf\nunique(states_sf$name)\n\n [1] \"Alabama\"              \"Alaska\"               \"Arizona\"             \n [4] \"Arkansas\"             \"California\"           \"Colorado\"            \n [7] \"Connecticut\"          \"Delaware\"             \"District of Columbia\"\n[10] \"Florida\"              \"Georgia\"              \"Hawaii\"              \n[13] \"Idaho\"                \"Illinois\"             \"Indiana\"             \n[16] \"Iowa\"                 \"Kansas\"               \"Kentucky\"            \n[19] \"Louisiana\"            \"Maine\"                \"Maryland\"            \n[22] \"Massachusetts\"        \"Michigan\"             \"Minnesota\"           \n[25] \"Mississippi\"          \"Missouri\"             \"Montana\"             \n[28] \"Nebraska\"             \"Nevada\"               \"New Hampshire\"       \n[31] \"New Jersey\"           \"New Mexico\"           \"New York\"            \n[34] \"North Carolina\"       \"North Dakota\"         \"Ohio\"                \n[37] \"Oklahoma\"             \"Oregon\"               \"Pennsylvania\"        \n[40] \"Rhode Island\"         \"South Carolina\"       \"South Dakota\"        \n[43] \"Tennessee\"            \"Texas\"                \"Utah\"                \n[46] \"Vermont\"              \"Virginia\"             \"Washington\"          \n[49] \"West Virginia\"        \"Wisconsin\"            \"Wyoming\"             \n[52] \"Puerto Rico\"         \n\n# Load in state-wise data for filling our choropleth maps\n#   (Note that I selected my two variables of interest to simplify)\nlibrary(poliscidata)   # may have to install first\npolisci_data &lt;- as_tibble(poliscidata::states) |&gt;\n  select(state, carfatal07, cook_index3)\n\n# See what the state (state) levels look like in polisci_data\nunique(polisci_data$state)   # can't see trailing spaces but can see\n\n [1] Alaska                                    \n [2] Alabama                                   \n [3] Arkansas                                  \n [4] Arizona                                   \n [5] California                                \n [6] Colorado                                  \n [7] Connecticut                               \n [8] Delaware                                  \n [9] Florida                                   \n[10] Georgia                                   \n[11] Hawaii                                    \n[12] Iowa                                      \n[13] Idaho                                     \n[14] Illinois                                  \n[15] Indiana                                   \n[16] Kansas                                    \n[17] Kentucky                                  \n[18] Louisiana                                 \n[19] Massachusetts                             \n[20] Maryland                                  \n[21] Maine                                     \n[22] Michigan                                  \n[23] Minnesota                                 \n[24] Missouri                                  \n[25] Mississippi                               \n[26] Montana                                   \n[27] NorthCarolina                             \n[28] NorthDakota                               \n[29] Nebraska                                  \n[30] NewHampshire                              \n[31] NewJersey                                 \n[32] NewMexico                                 \n[33] Nevada                                    \n[34] NewYork                                   \n[35] Ohio                                      \n[36] Oklahoma                                  \n[37] Oregon                                    \n[38] Pennsylvania                              \n[39] RhodeIsland                               \n[40] SouthCarolina                             \n[41] SouthDakota                               \n[42] Tennessee                                 \n[43] Texas                                     \n[44] Utah                                      \n[45] Virginia                                  \n[46] Vermont                                   \n[47] Washington                                \n[48] Wisconsin                                 \n[49] WestVirginia                              \n[50] Wyoming                                   \n50 Levels: Alabama                                    ...\n\n                             #   lack of internal spaces\nprint(polisci_data)   # can see trailing spaces\n\n# A tibble: 50 × 3\n   state                                        carfatal07 cook_index3\n   &lt;fct&gt;                                             &lt;dbl&gt; &lt;fct&gt;      \n 1 \"Alaska                                    \"       15.2 More Rep   \n 2 \"Alabama                                   \"       25.9 More Rep   \n 3 \"Arkansas                                  \"       23.7 More Rep   \n 4 \"Arizona                                   \"       17.6 Even       \n 5 \"California                                \"       11.7 More Dem   \n 6 \"Colorado                                  \"       12.3 Even       \n 7 \"Connecticut                               \"        8.7 More Dem   \n 8 \"Delaware                                  \"       13.6 More Dem   \n 9 \"Florida                                   \"       18.1 Even       \n10 \"Georgia                                   \"       18.5 Even       \n# ℹ 40 more rows\n\n\nR code hints:\n\nstringr functions like str_squish and str_to_lower and str_replace_all (be sure to carefully look at your keys!)\n*_join functions (make sure they preserve classes)\nfilter so that you only have 48 contiguous states (and maybe DC)\nfor help with colors: https://rstudio.github.io/leaflet/reference/colorNumeric.html\nbe sure labels pop up when scrolling with leaflet\n\n\n# Make sure all keys have the same format before joining:\n#   all lower case, no internal or external spaces\n\n\n# Now we can merge data sets together for the static and the interactive plots\n\n\n# Merge with states_polygon (static)\n\n# Check that merge worked for 48 contiguous states\n\n\n# Merge with states_sf (static or interactive)\n\n# Check that merge worked for 48 contiguous states\n\nNumeric variable (static plot):\nNumeric variable (interactive plot):\n\n# it's okay to skip a legend here\n\nCategorical variable (static plot):\n\n# be really careful with matching color order to factor level order\n\nCategorical variable (interactive plot):\n\n# may use colorFactor() here",
    "crumbs": [
      "Creating informative maps"
    ]
  },
  {
    "objectID": "06_data_types.html",
    "href": "06_data_types.html",
    "title": "Data types",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\nThis leans on parts of R4DS Chapter 27: A field guide to base R, in addition to parts of the first edition of R4DS.\n# Initial packages required\nlibrary(tidyverse)"
  },
  {
    "objectID": "06_data_types.html#what-is-a-vector",
    "href": "06_data_types.html#what-is-a-vector",
    "title": "Data types",
    "section": "What is a vector?",
    "text": "What is a vector?\nWe’ve seen them:\n\n1:5 \n\n[1] 1 2 3 4 5\n\nc(3, 6, 1, 7)\n\n[1] 3 6 1 7\n\nc(\"a\", \"b\", \"c\")\n\n[1] \"a\" \"b\" \"c\"\n\nx &lt;- c(0:3, NA)\nis.na(x)\n\n[1] FALSE FALSE FALSE FALSE  TRUE\n\nsqrt(x)\n\n[1] 0.000000 1.000000 1.414214 1.732051       NA\n\n\nThis doesn’t really fit the mathematical definition of a vector (direction and magnitude)… its really just some numbers (or letters, or TRUE’s…) strung together."
  },
  {
    "objectID": "06_data_types.html#types-of-vectors",
    "href": "06_data_types.html#types-of-vectors",
    "title": "Data types",
    "section": "Types of vectors",
    "text": "Types of vectors\nAtomic vectors are homogeneous… they can contain only one “type”. Types include logical, integer, double, and character (Also complex and raw, but we will ignore those).\nLists can be heterogeneous…. they can be made up of vectors of different types, or even of other lists!\nNULL denotes the absence of a vector (whereas NA denotes absence of a value in a vector).\nLet’s check out some vector types:\n\nx &lt;- c(0:3, NA)\ntypeof(x)\n\n[1] \"integer\"\n\nsqrt(x)\n\n[1] 0.000000 1.000000 1.414214 1.732051       NA\n\ntypeof(sqrt(x))\n\n[1] \"double\"\n\n\n[Pause to Ponder:] State the types of the following vectors, then use typeof() to check:\n\nis.na(x)\n\n[1] FALSE FALSE FALSE FALSE  TRUE\n\nx &gt; 2\n\n[1] FALSE FALSE FALSE  TRUE    NA\n\nc(\"apple\", \"banana\", \"pear\")\n\n[1] \"apple\"  \"banana\" \"pear\"  \n\n\nA logical vector can be implicitly coerced to numeric - T to 1 and F to 0\n\nx &lt;- sample(1:20, 100, replace = TRUE)\ny &lt;- x &gt; 10\nis_logical(y)\n\n[1] TRUE\n\nas.numeric(y)\n\n  [1] 1 1 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 0\n [38] 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 0 1\n [75] 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 1\n\nsum(y)  # how many are greater than 10?\n\n[1] 60\n\nmean(y) # what proportion are greater than 10?\n\n[1] 0.6\n\n\nIf there are multiple data types in a vector, then the most complex type wins, because you cannot mix types in a vector (although you can in a list)\n\ntypeof(c(TRUE, 1L))\n\n[1] \"integer\"\n\ntypeof(c(1L, 1.5))\n\n[1] \"double\"\n\ntypeof(c(1.5, \"a\"))\n\n[1] \"character\"\n\n\nIntegers are whole numbers. “double” refers to “Double-precision” representation of fractional values… don’t worry about the details here (Google it if you care), but just recognize that computers have to round at some point. “Double-precision” tries to store numbers precisely and efficiently.\nBut weird stuff can happen:\n\ny &lt;- sqrt(2) ^2\ny\n\n[1] 2\n\ny == 2\n\n[1] FALSE\n\n\nthe function near is better here:\n\nnear(y, 2)\n\n[1] TRUE\n\n\nAnd doubles have a couple extra possible values: Inf, -Inf, and NaN, in addition to NA:\n\n1/0\n\n[1] Inf\n\n-1/0\n\n[1] -Inf\n\n0/0\n\n[1] NaN\n\nInf*0\n\n[1] NaN\n\nInf/Inf\n\n[1] NaN\n\nInf/NA\n\n[1] NA\n\nInf*NA\n\n[1] NA\n\n\nIt’s not a good idea to check for special values (NA, NaN, Inf, -Inf) with ==. Use these instead:\n\nis.finite(Inf)\n\n[1] FALSE\n\nis.infinite(Inf)\n\n[1] TRUE\n\nis.finite(NA)\n\n[1] FALSE\n\nis.finite(NaN)\n\n[1] FALSE\n\nis.infinite(NA)\n\n[1] FALSE\n\nis.infinite(NaN)\n\n[1] FALSE\n\nis.na(NA)\n\n[1] TRUE\n\nis.na(NaN)\n\n[1] TRUE\n\nis.nan(NA)\n\n[1] FALSE\n\nis.nan(NaN)\n\n[1] TRUE\n\nis.na(Inf)\n\n[1] FALSE\n\nis.nan(Inf)\n\n[1] FALSE\n\n\nWhy not use == ?\n\n# Sometimes it works how you think it would:\n1/0\n\n[1] Inf\n\n1/0 == Inf\n\n[1] TRUE\n\n# Sometimes it doesn't (Because NA is contagious!)\n0/0\n\n[1] NaN\n\n0/0 == NaN    \n\n[1] NA\n\nNA == NA \n\n[1] NA\n\nx &lt;- c(0, 1, 1/0, 0/0)\n# Doesn't work well\nx == NA\n\n[1] NA NA NA NA\n\nx == Inf\n\n[1] FALSE FALSE  TRUE    NA\n\n# Works better\nis.na(x)\n\n[1] FALSE FALSE FALSE  TRUE\n\nis.infinite(x)\n\n[1] FALSE FALSE  TRUE FALSE\n\n\nAnother note: technically, each type of vector has its own type of NA… this usually doesn’t matter, but is good to know in case one day you get very very strange errors."
  },
  {
    "objectID": "06_data_types.html#augmented-vectors",
    "href": "06_data_types.html#augmented-vectors",
    "title": "Data types",
    "section": "Augmented vectors",
    "text": "Augmented vectors\nVectors may carry additional metadata in the form of attributes which create augmented vectors.\n\nFactors are built on top of integer vectors\nDates and date-times are built on top of numeric (either integer or double) vectors\nData frames and tibbles are built on top of lists"
  },
  {
    "objectID": "06_data_types.html#naming-items-in-vectors",
    "href": "06_data_types.html#naming-items-in-vectors",
    "title": "Data types",
    "section": "Naming items in vectors",
    "text": "Naming items in vectors\nEach element of a vector can be named, either when it is created or with setnames from package purrr.\n\nx &lt;- c(a = 1, b = 2, c = 3)\nx\n\na b c \n1 2 3 \n\n\nThis is more commonly used when you’re dealing with lists or tibbles (which are just a special kind of list!)\n\ntibble(x = 1:4, y = 5:8)\n\n# A tibble: 4 × 2\n      x     y\n  &lt;int&gt; &lt;int&gt;\n1     1     5\n2     2     6\n3     3     7\n4     4     8"
  },
  {
    "objectID": "06_data_types.html#subsetting-vectors",
    "href": "06_data_types.html#subsetting-vectors",
    "title": "Data types",
    "section": "Subsetting vectors",
    "text": "Subsetting vectors\nSo many ways to do this.\nI. Subset with numbers.\nUse positive integers to keep elements at those positions:\n\nx &lt;- c(\"one\", \"two\", \"three\", \"four\", \"five\")\nx[1]\n\n[1] \"one\"\n\nx[4]\n\n[1] \"four\"\n\nx[1:2]\n\n[1] \"one\" \"two\"\n\n\n[Pause to Ponder:] How would you extract values 1 and 3?\nYou can also repeat values:\n\nx[c(1, 1, 3, 3, 5, 5, 2, 2, 4, 4, 4)]\n\n [1] \"one\"   \"one\"   \"three\" \"three\" \"five\"  \"five\"  \"two\"   \"two\"   \"four\" \n[10] \"four\"  \"four\" \n\n\nUse negative integers to drop elements:\n\nx[-3]\n\n[1] \"one\"  \"two\"  \"four\" \"five\"\n\n\n[Pause to Ponder:] How would you drop values 2 and 4?\nWhat happens if you mix positive and negative values?\n\nx[c(1, -1)]\n\nError in x[c(1, -1)]: only 0's may be mixed with negative subscripts\n\n\nYou can just subset with 0… this isn’t usually helpful, except perhaps for testing weird cases when you write functions:\n\nx[0]\n\ncharacter(0)\n\n\n\nSubset with a logical vector (“Logical subsetting”).\n\n\nx == \"one\"\n\n[1]  TRUE FALSE FALSE FALSE FALSE\n\nx[x == \"one\"]\n\n[1] \"one\"\n\ny &lt;- c(10, 3, NA, 5, 8, 1, NA)\nis.na(y)\n\n[1] FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE\n\ny[!is.na(y)]\n\n[1] 10  3  5  8  1\n\n\n[Pause to Ponder:] Extract values of y that are less than or equal to 5 (what happens to NAs?). Then extract all non-missing values of y that are less than or equal to 5\n\nIf named, subset with a character vector.\n\n\nz &lt;- c(abc = 1, def = 2, xyz = 3)\nz[\"abc\"]\n\nabc \n  1 \n\n# A slightly more useful example:\nsummary(y)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n    1.0     3.0     5.0     5.4     8.0    10.0       2 \n\nsummary(y)[\"Min.\"]\n\nMin. \n   1 \n\n\n[Pause to Ponder:] Extract abc and xyz from the vector z, and then extract the mean from summary(y)\nNote: Using $ is just for lists (and tibbles, since tibbles are lists)! Not atomic vectors!\n\nz$abc\n\nError in z$abc: $ operator is invalid for atomic vectors\n\n\n\nBlank space. (Don’t subset).\n\n\nx\n\n[1] \"one\"   \"two\"   \"three\" \"four\"  \"five\" \n\nx[]\n\n[1] \"one\"   \"two\"   \"three\" \"four\"  \"five\" \n\n\nThis seems kind of silly. But blank is useful for higher-dimensional objects… like a matrix, or data frame. But our book doesn’t use matrices, so this may be the last one you see this semester:\n\nz &lt;- matrix(1:8, nrow= 2)\nz\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    3    5    7\n[2,]    2    4    6    8\n\nz[1, ]\n\n[1] 1 3 5 7\n\nz[, 1]\n\n[1] 1 2\n\nz[, -3]\n\n     [,1] [,2] [,3]\n[1,]    1    3    7\n[2,]    2    4    8\n\n\nWe could use this with tibbles too, but it is generally better to use the column names (more readable, and less likely to get the wrong columns by accident), and you should probably use select, filter, or slice:\n\nmpg[, 1:2]\n\n# A tibble: 234 × 2\n   manufacturer model     \n   &lt;chr&gt;        &lt;chr&gt;     \n 1 audi         a4        \n 2 audi         a4        \n 3 audi         a4        \n 4 audi         a4        \n 5 audi         a4        \n 6 audi         a4        \n 7 audi         a4        \n 8 audi         a4 quattro\n 9 audi         a4 quattro\n10 audi         a4 quattro\n# ℹ 224 more rows\n\nmpg[1:3, ]\n\n# A tibble: 3 × 11\n  manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class \n  &lt;chr&gt;        &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; \n1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa…\n2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa…\n3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa…"
  },
  {
    "objectID": "06_data_types.html#recycling",
    "href": "06_data_types.html#recycling",
    "title": "Data types",
    "section": "Recycling",
    "text": "Recycling\nWhat does R do with vectors:\n\n1:5 + 1:5\n\n[1]  2  4  6  8 10\n\n1:5 * 1:5\n\n[1]  1  4  9 16 25\n\n1:5 + 2\n\n[1] 3 4 5 6 7\n\n1:5 * 2\n\n[1]  2  4  6  8 10\n\n\nThis last two lines makes sense… but R is doing something important here, called recycling. In other words, it is really doing this:\n\n1:5 * c(2, 2, 2, 2, 2)\n\n[1]  2  4  6  8 10\n\n\nYou never need to do this explicit iteration! (This is different from some other more general purpose computing languages…. R was built for analyzing data, so this type of behavior is really desirable!)\nR can recycle longer vectors too, and only warns you if lengths are not multiples of each other:\n\n1:10 + 1:2\n\n [1]  2  4  4  6  6  8  8 10 10 12\n\n1:10 + 1:3\n\nWarning in 1:10 + 1:3: longer object length is not a multiple of shorter object\nlength\n\n\n [1]  2  4  6  5  7  9  8 10 12 11\n\n\nHowever, functions within the tidyverse will not allow you to recycle anything other than scalars (math word for single number… in R, a vector of length 1).\n\n#OK:\ntibble(x = 1:4, y = 1)\n\n# A tibble: 4 × 2\n      x     y\n  &lt;int&gt; &lt;dbl&gt;\n1     1     1\n2     2     1\n3     3     1\n4     4     1\n\n#not OK:\ntibble(x = 1:4, y = 1:2)\n\nError in `tibble()`:\n! Tibble columns must have compatible sizes.\n• Size 4: Existing data.\n• Size 2: Column `y`.\nℹ Only values of size one are recycled.\n\n\nTo intentionally recycle, use rep:\n\nrep(1:3, times = 2)\n\n[1] 1 2 3 1 2 3\n\nrep(1:3, each = 2)\n\n[1] 1 1 2 2 3 3"
  },
  {
    "objectID": "06_data_types.html#lists",
    "href": "06_data_types.html#lists",
    "title": "Data types",
    "section": "Lists",
    "text": "Lists\nLists can contain a mix of objects, even other lists.\nAs noted previously, tibbles are an augmented list. Augmented lists have additional attributes. For example, the names of the columns in a tibble.\nAnother list you may have encountered in a stats class is output from lm, linear regression:\n\nmpg_model &lt;- lm(hwy ~ cty, data = mpg)\n\nmpg_model\n\n\nCall:\nlm(formula = hwy ~ cty, data = mpg)\n\nCoefficients:\n(Intercept)          cty  \n      0.892        1.337  \n\ntypeof(mpg_model)\n\n[1] \"list\"\n\nstr(mpg_model)\n\nList of 12\n $ coefficients : Named num [1:2] 0.892 1.337\n  ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"cty\"\n $ residuals    : Named num [1:234] 4.0338 0.0214 3.3588 1.0214 3.7087 ...\n  ..- attr(*, \"names\")= chr [1:234] \"1\" \"2\" \"3\" \"4\" ...\n $ effects      : Named num [1:234] -358.566 -86.887 3.121 0.787 3.458 ...\n  ..- attr(*, \"names\")= chr [1:234] \"(Intercept)\" \"cty\" \"\" \"\" ...\n $ rank         : int 2\n $ fitted.values: Named num [1:234] 25 29 27.6 29 22.3 ...\n  ..- attr(*, \"names\")= chr [1:234] \"1\" \"2\" \"3\" \"4\" ...\n $ assign       : int [1:2] 0 1\n $ qr           :List of 5\n  ..$ qr   : num [1:234, 1:2] -15.2971 0.0654 0.0654 0.0654 0.0654 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:234] \"1\" \"2\" \"3\" \"4\" ...\n  .. .. ..$ : chr [1:2] \"(Intercept)\" \"cty\"\n  .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  ..$ qraux: num [1:2] 1.07 1.06\n  ..$ pivot: int [1:2] 1 2\n  ..$ tol  : num 1e-07\n  ..$ rank : int 2\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 232\n $ xlevels      : Named list()\n $ call         : language lm(formula = hwy ~ cty, data = mpg)\n $ terms        :Classes 'terms', 'formula'  language hwy ~ cty\n  .. ..- attr(*, \"variables\")= language list(hwy, cty)\n  .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:2] \"hwy\" \"cty\"\n  .. .. .. ..$ : chr \"cty\"\n  .. ..- attr(*, \"term.labels\")= chr \"cty\"\n  .. ..- attr(*, \"order\")= int 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. ..- attr(*, \"predvars\")= language list(hwy, cty)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. ..- attr(*, \"names\")= chr [1:2] \"hwy\" \"cty\"\n $ model        :'data.frame':  234 obs. of  2 variables:\n  ..$ hwy: int [1:234] 29 29 31 30 26 26 27 26 25 28 ...\n  ..$ cty: int [1:234] 18 21 20 21 16 18 18 18 16 20 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language hwy ~ cty\n  .. .. ..- attr(*, \"variables\")= language list(hwy, cty)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"hwy\" \"cty\"\n  .. .. .. .. ..$ : chr \"cty\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"cty\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(hwy, cty)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"hwy\" \"cty\"\n - attr(*, \"class\")= chr \"lm\"\n\n\nThere are three ways to extract from a list. Check out the pepper shaker analogy in Section 27.3.3 (note: shaker = list)\n\n[] returns new, smaller list (fewer pepper packs in shaker)\n[[]] drills down one level (individual pepper packs not in shaker)\n\nI. [ to extract a sub-list. The result is a list.\n\nmpg_model[1]\n\n$coefficients\n(Intercept)         cty \n  0.8920411   1.3374556 \n\ntypeof(mpg_model[1])\n\n[1] \"list\"\n\n\nyou can also do this by name, rather than number:\n\nmpg_model[\"coefficients\"]\n\n$coefficients\n(Intercept)         cty \n  0.8920411   1.3374556 \n\n\n\n[[ extracts a single component from the list… It removes a level of hierarchy\n\n\nmpg_model[[1]]\n\n(Intercept)         cty \n  0.8920411   1.3374556 \n\ntypeof(mpg_model[[1]])\n\n[1] \"double\"\n\n\nAgain, it can be done by name instead:\n\nmpg_model[[\"coefficients\"]]\n\n(Intercept)         cty \n  0.8920411   1.3374556 \n\n\n\n$ is a shorthand way of extracting elements by name… it is similar to [[ in that it removes a level of hierarchy. You don’t need quotes. (We’ve seen this with tibbles before too!)\n\n\nmpg_model$coefficients\n\n(Intercept)         cty \n  0.8920411   1.3374556"
  },
  {
    "objectID": "06_data_types.html#str",
    "href": "06_data_types.html#str",
    "title": "Data types",
    "section": "str",
    "text": "str\nThe str function allows us to see the structure of a list, as well as any attributes.\n\nmpg\n\n# A tibble: 234 × 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n 2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n 3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n 4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n 5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n 6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n 7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n 8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n 9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n# ℹ 224 more rows\n\nstr(mpg)\n\ntibble [234 × 11] (S3: tbl_df/tbl/data.frame)\n $ manufacturer: chr [1:234] \"audi\" \"audi\" \"audi\" \"audi\" ...\n $ model       : chr [1:234] \"a4\" \"a4\" \"a4\" \"a4\" ...\n $ displ       : num [1:234] 1.8 1.8 2 2 2.8 2.8 3.1 1.8 1.8 2 ...\n $ year        : int [1:234] 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 ...\n $ cyl         : int [1:234] 4 4 4 4 6 6 6 4 4 4 ...\n $ trans       : chr [1:234] \"auto(l5)\" \"manual(m5)\" \"manual(m6)\" \"auto(av)\" ...\n $ drv         : chr [1:234] \"f\" \"f\" \"f\" \"f\" ...\n $ cty         : int [1:234] 18 21 20 21 16 18 18 18 16 20 ...\n $ hwy         : int [1:234] 29 29 31 30 26 26 27 26 25 28 ...\n $ fl          : chr [1:234] \"p\" \"p\" \"p\" \"p\" ...\n $ class       : chr [1:234] \"compact\" \"compact\" \"compact\" \"compact\" ...\n\nmpg_model\n\n\nCall:\nlm(formula = hwy ~ cty, data = mpg)\n\nCoefficients:\n(Intercept)          cty  \n      0.892        1.337  \n\nstr(mpg_model)\n\nList of 12\n $ coefficients : Named num [1:2] 0.892 1.337\n  ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"cty\"\n $ residuals    : Named num [1:234] 4.0338 0.0214 3.3588 1.0214 3.7087 ...\n  ..- attr(*, \"names\")= chr [1:234] \"1\" \"2\" \"3\" \"4\" ...\n $ effects      : Named num [1:234] -358.566 -86.887 3.121 0.787 3.458 ...\n  ..- attr(*, \"names\")= chr [1:234] \"(Intercept)\" \"cty\" \"\" \"\" ...\n $ rank         : int 2\n $ fitted.values: Named num [1:234] 25 29 27.6 29 22.3 ...\n  ..- attr(*, \"names\")= chr [1:234] \"1\" \"2\" \"3\" \"4\" ...\n $ assign       : int [1:2] 0 1\n $ qr           :List of 5\n  ..$ qr   : num [1:234, 1:2] -15.2971 0.0654 0.0654 0.0654 0.0654 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:234] \"1\" \"2\" \"3\" \"4\" ...\n  .. .. ..$ : chr [1:2] \"(Intercept)\" \"cty\"\n  .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  ..$ qraux: num [1:2] 1.07 1.06\n  ..$ pivot: int [1:2] 1 2\n  ..$ tol  : num 1e-07\n  ..$ rank : int 2\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 232\n $ xlevels      : Named list()\n $ call         : language lm(formula = hwy ~ cty, data = mpg)\n $ terms        :Classes 'terms', 'formula'  language hwy ~ cty\n  .. ..- attr(*, \"variables\")= language list(hwy, cty)\n  .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:2] \"hwy\" \"cty\"\n  .. .. .. ..$ : chr \"cty\"\n  .. ..- attr(*, \"term.labels\")= chr \"cty\"\n  .. ..- attr(*, \"order\")= int 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. ..- attr(*, \"predvars\")= language list(hwy, cty)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. ..- attr(*, \"names\")= chr [1:2] \"hwy\" \"cty\"\n $ model        :'data.frame':  234 obs. of  2 variables:\n  ..$ hwy: int [1:234] 29 29 31 30 26 26 27 26 25 28 ...\n  ..$ cty: int [1:234] 18 21 20 21 16 18 18 18 16 20 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language hwy ~ cty\n  .. .. ..- attr(*, \"variables\")= language list(hwy, cty)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"hwy\" \"cty\"\n  .. .. .. .. ..$ : chr \"cty\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"cty\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(hwy, cty)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"hwy\" \"cty\"\n - attr(*, \"class\")= chr \"lm\"\n\n\nAs you can see, the mpg_model is a very complicated list with lots of attributes. The elements of the list can be all different types.\nThe last attribute is the object class, which it lists as lm.\n\nclass(mpg_model)\n\n[1] \"lm\"\n\n\nNow let’s see how extracting from a list works with a tibble (since a tibble is built on top of a list).\n\nugly_data &lt;- tibble(\n  truefalse = c(\"TRUE\", \"FALSE\", \"NA\"),\n  numbers = c(\"1\", \"2\", \"3\"),\n  dates = c(\"2010-01-01\", \"1979-10-14\", \"2013-08-17\"),\n  more_numbers = c(\"1\", \"231\", \".\")\n)\nugly_data\n\n# A tibble: 3 × 4\n  truefalse numbers dates      more_numbers\n  &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;       \n1 TRUE      1       2010-01-01 1           \n2 FALSE     2       1979-10-14 231         \n3 NA        3       2013-08-17 .           \n\nstr(ugly_data)   # we've seen str before... stands for \"structure\"\n\ntibble [3 × 4] (S3: tbl_df/tbl/data.frame)\n $ truefalse   : chr [1:3] \"TRUE\" \"FALSE\" \"NA\"\n $ numbers     : chr [1:3] \"1\" \"2\" \"3\"\n $ dates       : chr [1:3] \"2010-01-01\" \"1979-10-14\" \"2013-08-17\"\n $ more_numbers: chr [1:3] \"1\" \"231\" \".\"\n\npretty_data &lt;- ugly_data %&gt;% \n  mutate(truefalse = parse_logical(truefalse),\n         numbers = parse_number(numbers),\n         dates = parse_date(dates),\n         more_numbers = parse_number(more_numbers))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `more_numbers = parse_number(more_numbers)`.\nCaused by warning:\n! 1 parsing failure.\nrow col expected actual\n  3  -- a number      .\n\npretty_data\n\n# A tibble: 3 × 4\n  truefalse numbers dates      more_numbers\n  &lt;lgl&gt;       &lt;dbl&gt; &lt;date&gt;            &lt;dbl&gt;\n1 TRUE            1 2010-01-01            1\n2 FALSE           2 1979-10-14          231\n3 NA              3 2013-08-17           NA\n\nstr(pretty_data)\n\ntibble [3 × 4] (S3: tbl_df/tbl/data.frame)\n $ truefalse   : logi [1:3] TRUE FALSE NA\n $ numbers     : num [1:3] 1 2 3\n $ dates       : Date[1:3], format: \"2010-01-01\" \"1979-10-14\" ...\n $ more_numbers: num [1:3] 1 231 NA\n  ..- attr(*, \"problems\")= tibble [1 × 4] (S3: tbl_df/tbl/data.frame)\n  .. ..$ row     : int 3\n  .. ..$ col     : int NA\n  .. ..$ expected: chr \"a number\"\n  .. ..$ actual  : chr \".\"\n\n# Get a smaller tibble\npretty_data[1]\n\n# A tibble: 3 × 1\n  truefalse\n  &lt;lgl&gt;    \n1 TRUE     \n2 FALSE    \n3 NA       \n\nclass(pretty_data[1])\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\ntypeof(pretty_data[1])\n\n[1] \"list\"\n\npretty_data[2:3]\n\n# A tibble: 3 × 2\n  numbers dates     \n    &lt;dbl&gt; &lt;date&gt;    \n1       1 2010-01-01\n2       2 1979-10-14\n3       3 2013-08-17\n\npretty_data[1, 3:4]\n\n# A tibble: 1 × 2\n  dates      more_numbers\n  &lt;date&gt;            &lt;dbl&gt;\n1 2010-01-01            1\n\npretty_data[\"dates\"]\n\n# A tibble: 3 × 1\n  dates     \n  &lt;date&gt;    \n1 2010-01-01\n2 1979-10-14\n3 2013-08-17\n\npretty_data[c(\"dates\", \"more_numbers\")]\n\n# A tibble: 3 × 2\n  dates      more_numbers\n  &lt;date&gt;            &lt;dbl&gt;\n1 2010-01-01            1\n2 1979-10-14          231\n3 2013-08-17           NA\n\npretty_data %&gt;% select(dates, more_numbers) \n\n# A tibble: 3 × 2\n  dates      more_numbers\n  &lt;date&gt;            &lt;dbl&gt;\n1 2010-01-01            1\n2 1979-10-14          231\n3 2013-08-17           NA\n\npretty_data %&gt;% select(dates, more_numbers) %&gt;% slice(1:2) \n\n# A tibble: 2 × 2\n  dates      more_numbers\n  &lt;date&gt;            &lt;dbl&gt;\n1 2010-01-01            1\n2 1979-10-14          231\n\n# Remove a level of hierarchy - drill down one level to get a new object\npretty_data$dates\n\n[1] \"2010-01-01\" \"1979-10-14\" \"2013-08-17\"\n\nclass(pretty_data$dates)\n\n[1] \"Date\"\n\ntypeof(pretty_data$dates)\n\n[1] \"double\"\n\npretty_data[[1]]\n\n[1]  TRUE FALSE    NA\n\nclass(pretty_data[[1]])\n\n[1] \"logical\"\n\ntypeof(pretty_data[[1]])\n\n[1] \"logical\"\n\n\n[Pause to Ponder:] Predict what these lines will produce BEFORE running them:\n\npretty_data[[c(\"dates\", \"more_numbers\")]]\n\nError in `pretty_data[[c(\"dates\", \"more_numbers\")]]`:\n! Can't extract column with `c(\"dates\", \"more_numbers\")`.\n✖ Subscript `c(\"dates\", \"more_numbers\")` must be size 1, not 2.\n\npretty_data[[2]][[3]]\n\n[1] 3\n\npretty_data[[2]][3]\n\n[1] 3\n\npretty_data[[2]][c(TRUE, FALSE, TRUE)]\n\n[1] 1 3\n\npretty_data[[1]][c(1, 2, 1, 2)]\n\n[1]  TRUE FALSE  TRUE FALSE"
  },
  {
    "objectID": "06_data_types.html#generic-functions",
    "href": "06_data_types.html#generic-functions",
    "title": "Data types",
    "section": "Generic functions",
    "text": "Generic functions\nAnother important feature of R is generic functions. Some functions, like plot and summary for example, behave very differently depending on the class of their input.\n\nclass(mpg)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nsummary(mpg)\n\n manufacturer          model               displ            year     \n Length:234         Length:234         Min.   :1.600   Min.   :1999  \n Class :character   Class :character   1st Qu.:2.400   1st Qu.:1999  \n Mode  :character   Mode  :character   Median :3.300   Median :2004  \n                                       Mean   :3.472   Mean   :2004  \n                                       3rd Qu.:4.600   3rd Qu.:2008  \n                                       Max.   :7.000   Max.   :2008  \n      cyl           trans               drv                 cty       \n Min.   :4.000   Length:234         Length:234         Min.   : 9.00  \n 1st Qu.:4.000   Class :character   Class :character   1st Qu.:14.00  \n Median :6.000   Mode  :character   Mode  :character   Median :17.00  \n Mean   :5.889                                         Mean   :16.86  \n 3rd Qu.:8.000                                         3rd Qu.:19.00  \n Max.   :8.000                                         Max.   :35.00  \n      hwy             fl               class          \n Min.   :12.00   Length:234         Length:234        \n 1st Qu.:18.00   Class :character   Class :character  \n Median :24.00   Mode  :character   Mode  :character  \n Mean   :23.44                                        \n 3rd Qu.:27.00                                        \n Max.   :44.00                                        \n\nclass(mpg_model)\n\n[1] \"lm\"\n\nsummary(mpg_model)\n\n\nCall:\nlm(formula = hwy ~ cty, data = mpg)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.3408 -1.2790  0.0214  1.0338  4.0461 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.89204    0.46895   1.902   0.0584 .  \ncty          1.33746    0.02697  49.585   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.752 on 232 degrees of freedom\nMultiple R-squared:  0.9138,    Adjusted R-squared:  0.9134 \nF-statistic:  2459 on 1 and 232 DF,  p-value: &lt; 2.2e-16\n\n\nAs a simpler case, consider the mean function.\n\nmean\n\nfunction (x, ...) \nUseMethod(\"mean\")\n&lt;bytecode: 0x0000018c8ad034c8&gt;\n&lt;environment: namespace:base&gt;\n\n\nAs a generic function, we can see what methods are available:\n\nmethods(mean)\n\n[1] mean.Date        mean.default     mean.difftime    mean.POSIXct    \n[5] mean.POSIXlt     mean.quosure*    mean.vctrs_vctr*\nsee '?methods' for accessing help and source code\n\n\n\nmean(c(20, 21, 23))\n\n[1] 21.33333\n\nlibrary(lubridate)\ndate_test &lt;- ymd(c(\"2020-03-20\", \"2020-03-21\", \"2020-03-23\"))\nmean(date_test)\n\n[1] \"2020-03-21\""
  },
  {
    "objectID": "06_data_types.html#what-makes-tibbles-special",
    "href": "06_data_types.html#what-makes-tibbles-special",
    "title": "Data types",
    "section": "What makes Tibbles special?",
    "text": "What makes Tibbles special?\nTibbles are lists that: - have names attributes (column/variable names) as well as row.names attributes. - have elements that are all vectors of the same length\n\nattributes(mpg)\n\n$class\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n$row.names\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n[109] 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n[127] 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n[145] 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n[163] 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n[181] 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n[199] 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216\n[217] 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234\n\n$names\n [1] \"manufacturer\" \"model\"        \"displ\"        \"year\"         \"cyl\"         \n [6] \"trans\"        \"drv\"          \"cty\"          \"hwy\"          \"fl\"          \n[11] \"class\""
  },
  {
    "objectID": "06_data_types.html#on-your-own",
    "href": "06_data_types.html#on-your-own",
    "title": "Data types",
    "section": "On Your Own",
    "text": "On Your Own\n\nThe dataset roster includes 24 names (the first 24 alphabetically on this list of names). Let’s suppose this is our class, and you want to divide students into 6 groups. Modify the code below using the rep function to create groups in two different ways.\n\n\nbabynames &lt;- read_csv(\"https://proback.github.io/264_fall_2024/Data/babynames_2000.csv\")\n\nroster &lt;- babynames %&gt;%\n  sample_n(size = 24) %&gt;%\n  select(name) \n\nroster %&gt;%\n  mutate(group_method1 = , \n         group_method2 = )\n\n\nHere’s is a really crazy list that tells you some stuff about data science.\n\n\ndata_sci &lt;- list(first = c(\"first it must work\", \"then it can be\" , \"pretty\"),\n                 DRY = c(\"Do not\", \"Repeat\", \"Yourself\"),\n                 dont_forget = c(\"garbage\", \"in\", \"out\"),\n                 our_first_tibble = mpg,\n                 integers = 1:25,\n                 doubles = sqrt(1:25),\n                 tidyverse = c(pack1 = \"ggplot2\", pack2 = \"dplyr\", \n                               pack3 = \"lubridate\", etc = \"and more!\"),\n                 opinion = list(\"MSCS 264 is\",  \"awesome!\", \"amazing!\", \"rainbows!\")\n                  )\n\nUse str to learn about data_sci.\nNow, figure out how to get exactly the following outputs. Bonus points if you can do it more than one way!\n[1] “first it must work” “then it can be” “pretty”\n$DRY [1] “Do not” “Repeat” “Yourself”\n[1] 3 1 4 1 5 9 3\n  pack1         etc \n“ggplot2” “and more!”\n[1] “rainbows!”\n[1] “garbage” “in” “garbage” “out”"
  },
  {
    "objectID": "03_functions.html",
    "href": "03_functions.html",
    "title": "Functions and tidy evaluation",
    "section": "",
    "text": "Based on Chapter 25 from R for Data Science\nYou can download this .qmd file from here. Just hit the Download Raw File button.",
    "crumbs": [
      "Functions and tidy evaluation"
    ]
  },
  {
    "objectID": "03_functions.html#vector-functions",
    "href": "03_functions.html#vector-functions",
    "title": "Functions and tidy evaluation",
    "section": "Vector functions",
    "text": "Vector functions\n\nExample 1: Rescale variables from 0 to 1.\nThis code creates a 10 x 4 tibble filled with random values taken from a normal distribution with mean 0 and SD 1\n\ndf &lt;- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\ndf\n\n# A tibble: 10 × 4\n         a       b      c       d\n     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1  1.04   -0.427   0.667  0.526 \n 2 -1.86    2.11   -0.229 -0.467 \n 3  0.259   0.0781 -1.71  -0.961 \n 4  2.01   -1.17   -0.844 -1.02  \n 5  1.27    1.02   -1.59  -0.480 \n 6  0.423   0.405  -1.96  -1.18  \n 7  0.0940  0.669   0.191 -0.0379\n 8 -1.33    0.0651 -1.87  -1.33  \n 9 -0.592   1.06   -1.14  -1.14  \n10 -0.451  -1.32    0.362  1.15  \n\n\nThis code below for rescaling variables from 0 to 1 is ripe for functions… we did it four times!\nIt’s easiest to start with working code and turn it into a function.\n\ndf$a &lt;- (df$a - min(df$a)) / (max(df$a) - min(df$a))\ndf$b &lt;- (df$b - min(df$b)) / (max(df$b) - min(df$b))\ndf$c &lt;- (df$c - min(df$c)) / (max(df$c) - min(df$c))\ndf$d &lt;- (df$d - min(df$d)) / (max(df$d) - min(df$d))\ndf\n\n# A tibble: 10 × 4\n       a      b      c      d\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 0.749 0.261  1      0.748 \n 2 0     1      0.658  0.348 \n 3 0.548 0.408  0.0920 0.150 \n 4 1     0.0439 0.424  0.127 \n 5 0.809 0.682  0.141  0.344 \n 6 0.590 0.503  0      0.0621\n 7 0.506 0.580  0.819  0.521 \n 8 0.139 0.404  0.0323 0     \n 9 0.328 0.694  0.310  0.0793\n10 0.365 0      0.884  1     \n\n\nNotice first what changes and what stays the same in each line. Then, if we look at the first line above, we see we have one value we’re using over and over: df$a. So our function will have one input. We’ll start with our code from that line, then replace the input (df$a) with x. We should give our function a name that explains what it does. The name should be a verb.\n\n# I'm going to show you how to write the function in class! \n# I have it in the code already below, but don't look yet!\n# Let's try to write it together first!\n\n. . . . . . . . .\n\n# Our function (first draft!)\nrescale01 &lt;- function(x) {\n  (x - min(x)) / (max(x) - min(x))\n}\n\nNote the general form of a function:\n\nname &lt;- function(arguments) {\n  body\n}\n\nEvery function contains 3 essential components:\n\nA name. The name should clearly evoke what the function does; hence, it is often a verb (action). Here we’ll use rescale01 because this function rescales a vector to lie between 0 and 1. snake_case is good; CamelCase is just okay.\nThe arguments. The arguments are things that vary across calls and they are usually nouns - first the data, then other details. Our analysis above tells us that we have just one; we’ll call it x because this is the conventional name for a numeric vector, but you can use any word.\nThe body. The body is the code that’s repeated across all the calls. By default a function will return the last statement; use return() to specify a return value\n\n\nSummary: Functions should be written for both humans and computers!\n\nOnce we have written a function we like, then we need to test it with different inputs!\n\ntemp &lt;- c(4, 6, 8, 9)\nrescale01(temp)\n\n[1] 0.0 0.4 0.8 1.0\n\ntemp0 &lt;- c(4, 6, 8, 9, NA)\nrescale01(temp0)\n\n[1] NA NA NA NA NA\n\n\nOK, so NA’s don’t work the way we want them to.\n\nrescale01 &lt;- function(x) {\n  (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))\n}\nrescale01(temp)\n\n[1] 0.0 0.4 0.8 1.0\n\nrescale01(temp0)\n\n[1] 0.0 0.4 0.8 1.0  NA\n\n\nWe can continue to improve our function. Here is another method, which uses the existing range function within R to avoid 3 max/min executions:\n\nrescale01 &lt;- function(x) {\n  rng &lt;- range(x, na.rm = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\nrescale01(temp)\n\n[1] 0.0 0.4 0.8 1.0\n\nrescale01(c(0, 5, 10))\n\n[1] 0.0 0.5 1.0\n\nrescale01(c(-10, 0, 10))\n\n[1] 0.0 0.5 1.0\n\nrescale01(c(1, 2, 3, NA, 5))\n\n[1] 0.00 0.25 0.50   NA 1.00\n\n\nWe should continue testing unusual inputs. Think carefully about how you want this function to behave… the current behavior is to include the Inf (infinity) value when calculating the range. You get strange output everywhere, but it’s pretty clear that there is a problem right away when you use the function. In the example below (rescale1), you ignore the infinity value when calculating the range. The function returns Inf for one value, and sensible stuff for the rest. In many cases this may be useful, but it could also hide a problem until you get deeper into an analysis.\n\nx &lt;- c(1:10, Inf)\nrescale01(x)\n\n [1]   0   0   0   0   0   0   0   0   0   0 NaN\n\nrescale1 &lt;- function(x) {\n  rng &lt;- range(x, na.rm = TRUE, finite = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\nrescale1(x)\n\n [1] 0.0000000 0.1111111 0.2222222 0.3333333 0.4444444 0.5555556 0.6666667\n [8] 0.7777778 0.8888889 1.0000000       Inf\n\n\nNow we’ve used functions to simplify original example. We will learn to simplify further in iterations (Ch 26)\n\ndf &lt;- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n# add a little noise\ndf$a[5] = NA\ndf$b[6] = Inf\ndf\n\n# A tibble: 10 × 4\n        a        b       c       d\n    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 -1.11    0.910  -0.900   1.55  \n 2  0.451  -0.707  -0.0438  1.26  \n 3 -1.71   -0.894  -0.884   0.397 \n 4  0.970  -0.0546 -1.45    0.0452\n 5 NA       0.0957 -1.33   -0.673 \n 6  0.243 Inf      -0.853   2.17  \n 7  0.746   0.457   1.00    0.845 \n 8  0.292   0.446  -0.406   1.31  \n 9  0.461  -1.53   -1.18    1.46  \n10 -0.537  -0.814   0.563   0.741 \n\ndf$a_new &lt;- rescale1(df$a)\ndf$b_new &lt;- rescale1(df$b)\ndf$c_new &lt;- rescale1(df$c)\ndf$d_new &lt;- rescale1(df$d)\ndf\n\n# A tibble: 10 × 8\n        a        b       c       d  a_new   b_new  c_new d_new\n    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 -1.11    0.910  -0.900   1.55    0.226   1     0.223  0.782\n 2  0.451  -0.707  -0.0438  1.26    0.807   0.338 0.573  0.681\n 3 -1.71   -0.894  -0.884   0.397   0       0.261 0.230  0.377\n 4  0.970  -0.0546 -1.45    0.0452  1       0.605 0      0.253\n 5 NA       0.0957 -1.33   -0.673  NA       0.667 0.0465 0    \n 6  0.243 Inf      -0.853   2.17    0.729 Inf     0.242  1    \n 7  0.746   0.457   1.00    0.845   0.917   0.815 1      0.535\n 8  0.292   0.446  -0.406   1.31    0.747   0.810 0.425  0.699\n 9  0.461  -1.53   -1.18    1.46    0.810   0     0.109  0.751\n10 -0.537  -0.814   0.563   0.741   0.439   0.294 0.820  0.498\n\ndf |&gt;\n  select(1:4) |&gt;\n  mutate(a_new = rescale1(a),\n         b_new = rescale1(b),\n         c_new = rescale1(c),\n         d_new = rescale1(d))\n\n# A tibble: 10 × 8\n        a        b       c       d  a_new   b_new  c_new d_new\n    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 -1.11    0.910  -0.900   1.55    0.226   1     0.223  0.782\n 2  0.451  -0.707  -0.0438  1.26    0.807   0.338 0.573  0.681\n 3 -1.71   -0.894  -0.884   0.397   0       0.261 0.230  0.377\n 4  0.970  -0.0546 -1.45    0.0452  1       0.605 0      0.253\n 5 NA       0.0957 -1.33   -0.673  NA       0.667 0.0465 0    \n 6  0.243 Inf      -0.853   2.17    0.729 Inf     0.242  1    \n 7  0.746   0.457   1.00    0.845   0.917   0.815 1      0.535\n 8  0.292   0.446  -0.406   1.31    0.747   0.810 0.425  0.699\n 9  0.461  -1.53   -1.18    1.46    0.810   0     0.109  0.751\n10 -0.537  -0.814   0.563   0.741   0.439   0.294 0.820  0.498\n\n# Even better - from Chapter 26\ndf |&gt; \n  select(1:4) |&gt;\n  mutate(across(a:d, rescale1, .names = \"{.col}_new\"))\n\n# A tibble: 10 × 8\n        a        b       c       d  a_new   b_new  c_new d_new\n    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 -1.11    0.910  -0.900   1.55    0.226   1     0.223  0.782\n 2  0.451  -0.707  -0.0438  1.26    0.807   0.338 0.573  0.681\n 3 -1.71   -0.894  -0.884   0.397   0       0.261 0.230  0.377\n 4  0.970  -0.0546 -1.45    0.0452  1       0.605 0      0.253\n 5 NA       0.0957 -1.33   -0.673  NA       0.667 0.0465 0    \n 6  0.243 Inf      -0.853   2.17    0.729 Inf     0.242  1    \n 7  0.746   0.457   1.00    0.845   0.917   0.815 1      0.535\n 8  0.292   0.446  -0.406   1.31    0.747   0.810 0.425  0.699\n 9  0.461  -1.53   -1.18    1.46    0.810   0     0.109  0.751\n10 -0.537  -0.814   0.563   0.741   0.439   0.294 0.820  0.498\n\n\n\n\nOptions for handling NAs in functions\nBefore we try some practice problems, let’s consider various options for handling NAs in functions. We used the na.rm option within functions like min, max, and range in order to take care of missing values. But there are alternative approaches:\n\nfilter/remove the NA values before rescaling\ncreate an if statement to check if there are NAs; return an error if NAs exist\ncreate a removeNAs option in the function we are creating\n\nLet’s take a look at each alternative approach in turn:\n\nFilter/remove the NA values before rescaling\n\ndf &lt;- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\ndf$a[5] = NA\ndf\n\n# A tibble: 10 × 4\n        a        b       c       d\n    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 -1.34  -1.79     0.944   0.492 \n 2  0.933 -1.18    -0.267  -1.29  \n 3 -0.698  1.06     0.822   0.450 \n 4  0.707 -0.267   -0.0492  0.0603\n 5 NA      0.778    1.23   -1.09  \n 6  0.196  0.00335  0.0660 -1.16  \n 7 -0.906  0.773    0.772  -1.34  \n 8  0.177  0.920   -0.559   0.626 \n 9  1.69   0.317    1.77    2.15  \n10  1.01  -0.319   -0.861  -0.533 \n\nrescale_basic &lt;- function(x) {\n  (x - min(x)) / (max(x) - min(x))\n}\n\ndf |&gt;\n  filter(!is.na(a)) |&gt;\n  mutate(new_a = rescale_basic(a))\n\n# A tibble: 9 × 5\n       a        b       c       d new_a\n   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 -1.34  -1.79     0.944   0.492  0    \n2  0.933 -1.18    -0.267  -1.29   0.750\n3 -0.698  1.06     0.822   0.450  0.213\n4  0.707 -0.267   -0.0492  0.0603 0.676\n5  0.196  0.00335  0.0660 -1.16   0.508\n6 -0.906  0.773    0.772  -1.34   0.144\n7  0.177  0.920   -0.559   0.626  0.501\n8  1.69   0.317    1.77    2.15   1    \n9  1.01  -0.319   -0.861  -0.533  0.775\n\n\n[Pause to Ponder:] Do you notice anything in the output above that gives you pause?\n\n\nCreate an if statement to check if there are NAs; return an error if NAs exist\nFirst, here’s an example involving weighted means:\n\n# Create function to calculate weighted mean\nwt_mean &lt;- function(x, w) {\n  sum(x * w) / sum(w)\n}\nwt_mean(c(1, 10), c(1/3, 2/3))\n\n[1] 7\n\nwt_mean(1:6, 1:3)\n\n[1] 7.666667\n\n\n[Pause to Ponder:] Why is the answer to the last call above 7.67? Aren’t we taking a weighted mean of 1-6, all of which are below 7?\n\n# update function to handle cases where data and weights of unequal length\nwt_mean &lt;- function(x, w) {\n  if (length(x) != length(w)) {\n    stop(\"`x` and `w` must be the same length\", call. = FALSE)\n  } else {\n  sum(w * x) / sum(w)\n  }  \n}\nwt_mean(1:6, 1:3) \n\nError: `x` and `w` must be the same length\n\n# should produce an error now if weights and data different lengths\n#  - nice example of if and else\n\n[Pause to Ponder:] What does the call. option do?\nNow let’s apply this to our rescaling function\n\nrescale_w_error &lt;- function(x) {\n  if (is.na(sum(x))) {\n    stop(\"`x` cannot have NAs\", call. = FALSE)\n  } else {\n    (x - min(x)) / (max(x) - min(x))\n  }  \n}\n\ntemp &lt;- c(4, 6, 8, 9)\nrescale_w_error(temp)\n\n[1] 0.0 0.4 0.8 1.0\n\ntemp &lt;- c(4, 6, 8, 9, NA)\nrescale_w_error(temp)\n\nError: `x` cannot have NAs\n\n\n[Pause to Ponder:] Why can’t we just use if (is.na(x)) instead of is.na(sum(x))?\n\n\nCreate a removeNAs option in the function we are creating\n\nrescale_NAoption &lt;- function(x, removeNAs = FALSE) {\n  (x - min(x, na.rm = removeNAs)) / \n    (max(x, na.rm = removeNAs) - min(x, na.rm = removeNAs))\n} \n\ntemp &lt;- c(4, 6, 8, 9)\nrescale_NAoption(temp)\n\n[1] 0.0 0.4 0.8 1.0\n\ntemp &lt;- c(4, 6, 8, 9, NA)\nrescale_NAoption(temp, removeNAs = TRUE)\n\n[1] 0.0 0.4 0.8 1.0  NA\n\n\nOK, but all the other summary stats functions use na.rm as the input, so to be consistent, it’s probably better to do something slightly awkward like this:\n\nrescale_NAoption &lt;- function(x, na.rm = FALSE) {\n  (x - min(x, na.rm = na.rm)) / \n    (max(x, na.rm = na.rm) - min(x, na.rm = na.rm))\n} \n\ntemp &lt;- c(4, 6, 8, 9, NA)\nrescale_NAoption(temp, na.rm = TRUE)\n\n[1] 0.0 0.4 0.8 1.0  NA\n\n\nwt_mean() is an example of a “summary function (single value output) instead of a”mutate function” (vector output) like rescale01(). Here’s another summary function to produce the mean absolute percentage error:\n\nmape &lt;- function(actual, predicted) {\n  sum(abs((actual - predicted) / actual)) / length(actual)\n}\n\ny &lt;- c(2,6,3,8,5)\nyhat &lt;- c(2.5, 5.1, 4.4, 7.8, 6.1)\nmape(actual = y, predicted = yhat)\n\n[1] 0.2223333",
    "crumbs": [
      "Functions and tidy evaluation"
    ]
  },
  {
    "objectID": "03_functions.html#data-frame-functions",
    "href": "03_functions.html#data-frame-functions",
    "title": "Functions and tidy evaluation",
    "section": "Data frame functions",
    "text": "Data frame functions\nThese work like dplyr verbs, taking a data frame as the first argument, and then returning a data frame or a vector.\n\nDemonstration of tidy evaluation in functions\n\n# Start with working code then functionize\nggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +\n  geom_point(size = 0.75) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\nmake_plot &lt;- function(dataset, xvar, yvar, pt_size = 0.75)  {\n  ggplot(data = dataset, mapping = aes(x = xvar, y = yvar)) +\n    geom_point(size = pt_size) +\n    geom_smooth()\n}\n\nmake_plot(dataset = mpg, xvar = cty, yvar = hwy)  # Error!\n\nError in `geom_point()`:\n! Problem while computing aesthetics.\nℹ Error occurred in the 1st layer.\nCaused by error:\n! object 'cty' not found\n\n\nThe problem is tidy evaluation, which makes most common coding easier, but makes some less common things harder. Key terms to understand tidy evaluation:\n\nenv-variables = live in the environment (mpg)\ndata-variables = live in data frame or tibble (cty)\ndata masking = tidyverse use data-variables as if they are env-variables. That is, you don’t always need mpg$cty to access cty in tidyverse\n\nThe key idea behind data masking is that it blurs the line between the two different meanings of the word “variable”:\n\nenv-variables are “programming” variables that live in an environment. They are usually created with &lt;-.\ndata-variables are “statistical” variables that live in a data frame. They usually come from data files (e.g. .csv, .xls), or are created manipulating existing variables.\n\nThe solution is to embrace {{ }} data-variables which are user inputs into functions. One way to remember what’s happening, as suggested by our book authors, is to think of {{ }} as looking down a tunnel — {{ var }} will make a dplyr function look inside of var rather than looking for a variable called var. Thus, embracing a variable tells dplyr to use the value stored inside the argument, not the argument as the literal variable name.\nSee Section 25.3 of R4DS for more details (and there are plenty!).\n\n# This will work to make our plot!\nmake_plot &lt;- function(dataset, xvar, yvar, pt_size = 0.75)  {\n  ggplot(data = dataset, mapping = aes(x = {{ xvar }}, y = {{ yvar }})) +\n    geom_point(size = pt_size) +\n    geom_smooth()\n}\n\nmake_plot(dataset = mpg, xvar = cty, yvar = hwy)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nI often wish it were easier to get my own custom summary statistics for numeric variables in EDA rather than using mosaic::favstats(). Using group_by() and summarise() from the tidyverse reads clearly but takes so many lines, but if I only had to write the code once…\n\nsummary6 &lt;- function(data, var) {\n  data |&gt; summarize(\n    mean = mean({{ var }}, na.rm = TRUE),\n    median = median({{ var }}, na.rm = TRUE),\n    sd = sd({{ var }}, na.rm = TRUE),\n    IQR = IQR({{ var }}, na.rm = TRUE),\n    n = n(),\n    n_miss = sum(is.na({{ var }})),\n    .groups = \"drop\"    # to leave the data in an ungrouped state\n  )\n}\n\nmpg |&gt; summary6(hwy)\n\n# A tibble: 1 × 6\n   mean median    sd   IQR     n n_miss\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;\n1  23.4     24  5.95     9   234      0\n\n\nEven cooler, I can use my new function with group_by()!\n\nmpg |&gt; \n  group_by(drv) |&gt;\n  summary6(hwy)\n\n# A tibble: 3 × 7\n  drv    mean median    sd   IQR     n n_miss\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;\n1 4      19.2     18  4.08     5   103      0\n2 f      28.2     28  4.21     3   106      0\n3 r      21       21  3.66     7    25      0\n\n\nYou can even pass conditions into a function using the embrace:\n[Pause to Ponder:] Predict what the code below will do, and (only) then run it to check. Think about: why do we have sort = sort? why not embrace df? why didn’t we need n in the arguments?\n\nnew_function &lt;- function(df, var, condition, sort = TRUE) {\n  df |&gt;\n    filter({{ condition }}) |&gt;\n    count({{ var }}, sort = sort) |&gt;\n    mutate(prop = n / sum(n))\n}\n\nmpg |&gt; new_function(var = manufacturer, \n                    condition = manufacturer %in% c(\"audi\", \n                                                    \"honda\", \n                                                    \"hyundai\", \n                                                    \"nissan\", \n                                                    \"subaru\", \n                                                    \"toyota\", \n                                                    \"volkswagen\")\n                    )\n\n\n\nData-masking vs. tidy-selection (Section 25.3.4)\nWhy doesn’t the following code work?\n\ncount_missing &lt;- function(df, group_vars, x_var) {\n  df |&gt; \n    group_by({{ group_vars }}) |&gt; \n    summarize(\n      n_miss = sum(is.na({{ x_var }})),\n      .groups = \"drop\"\n    )\n}\n\nflights |&gt; \n  count_missing(c(year, month, day), dep_time)\n\nError in `group_by()`:\nℹ In argument: `c(year, month, day)`.\nCaused by error:\n! `c(year, month, day)` must be size 336776 or 1, not 1010328.\n\n\nThe problem is that group_by() uses data-masking rather than tidy-selection; it is selecting certain variables rather than evaluating values of those variables. These are the two most common subtypes of tidy evaluation:\n\nData-masking is used in functions like arrange(), filter(), mutate(), and summarize() that compute with variables. Data masking is an R feature that blends programming variables that live inside environments (env-variables) with statistical variables stored in data frames (data-variables).\n\nTidy-selection is used for functions like select(), relocate(), and rename() that select variables. Tidy selection provides a concise dialect of R for selecting variables based on their names or properties.\n\nMore detail can be found here.\nThe error above can be solved by using the pick() function, which uses tidy selection inside of data masking:\n\ncount_missing &lt;- function(df, group_vars, x_var) {\n  df |&gt; \n    group_by(pick({{ group_vars }})) |&gt; \n    summarize(\n      n_miss = sum(is.na({{ x_var }})),\n      .groups = \"drop\"\n  )\n}\n\nflights |&gt; \n  count_missing(c(year, month, day), dep_time)\n\n# A tibble: 365 × 4\n    year month   day n_miss\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1  2013     1     1      4\n 2  2013     1     2      8\n 3  2013     1     3     10\n 4  2013     1     4      6\n 5  2013     1     5      3\n 6  2013     1     6      1\n 7  2013     1     7      3\n 8  2013     1     8      4\n 9  2013     1     9      5\n10  2013     1    10      3\n# ℹ 355 more rows\n\n\n[Pause to Ponder:] Here’s another nice use of pick(). Predict what the function will do, then run the code to see if you are correct.\n\n# Source: https://twitter.com/pollicipes/status/1571606508944719876\nnew_function &lt;- function(data, rows, cols) {\n  data |&gt; \n    count(pick(c({{ rows }}, {{ cols }}))) |&gt; \n    pivot_wider(\n      names_from = {{ cols }}, \n      values_from = n,\n      names_sort = TRUE,\n      values_fill = 0\n    )\n}\n\nmpg |&gt; new_function(c(manufacturer, model), cyl)",
    "crumbs": [
      "Functions and tidy evaluation"
    ]
  },
  {
    "objectID": "03_functions.html#plot-functions",
    "href": "03_functions.html#plot-functions",
    "title": "Functions and tidy evaluation",
    "section": "Plot functions",
    "text": "Plot functions\nLet’s say you find yourself making a lot of histograms:\n\nflights |&gt; \n  ggplot(aes(x = dep_time)) +\n  geom_histogram(bins = 25)\n\n\n\n\n\n\n\nflights |&gt; \n  ggplot(aes(x = air_time)) +\n  geom_histogram(bins = 35)\n\n\n\n\n\n\n\n\nJust use embrace to create a histogram-making function\n\nhistogram &lt;- function(df, var, bins = NULL) {\n  df |&gt; \n    ggplot(aes(x = {{ var }})) + \n    geom_histogram(bins = bins)\n}\n\nflights |&gt; histogram(air_time, 35)\n\n\n\n\n\n\n\n\nSince histogram() returns a ggplot, you can add any layers you want\n\nflights |&gt; \n  histogram(air_time, 35) +\n  labs(x = \"Flight time (minutes)\", y = \"Number of flights\")\n\n\n\n\n\n\n\n\nYou can also combine data wrangling with plotting. Note that we need the “walrus operator” (:=) since the variable name on the left is being generated with user-supplied data.\n\n# sort counts with highest values at top and counts on x-axis\nsorted_bars &lt;- function(df, var) {\n  df |&gt; \n    mutate({{ var }} := fct_rev(fct_infreq({{ var }})))  |&gt;\n    ggplot(aes(y = {{ var }})) +\n    geom_bar()\n}\n\nflights |&gt; sorted_bars(carrier)\n\n\n\n\n\n\n\n\nFinally, it would be really helpful to label plots based on user inputs. This is a bit more complicated, but still do-able!\nFor this, we’ll need the rlang package. rlang is a low-level package that’s used by just about every other package in the tidyverse because it implements tidy evaluation (as well as many other useful tools).\nCheck out the following update of our histogram() function which uses the englue() function from the rlang package:\n\nhistogram &lt;- function(df, var, bins) {\n  label &lt;- rlang::englue(\"A histogram of {{var}} with binwidth {bins}\")\n  \n  df |&gt; \n    ggplot(aes(x = {{ var }})) + \n    geom_histogram(bins = bins) + \n    labs(title = label)\n}\n\nflights |&gt; histogram(air_time, 35)\n\n\n\n\n\n\n\n\n\nOn Your Own\n\nRewrite this code snippet as a function: x / sum(x, na.rm = TRUE). This code creates weights which sum to 1, where NA values are ignored. Test it for at least two different vectors. (Make sure at least one has NAs!)\nCreate a function to calculate the standard error of a variable, where SE = square root of the variance divided by the sample size. Hint: start with a vector like x &lt;- 0:5 or x &lt;- gss_cat$age and write code to find the SE of x, then turn it into a function to handle any vector x. Note: var is the function to find variance in R and sqrt does square root. length may also be handy. Test your function on two vectors that do not include NAs (i.e. do not worry about removing NAs at this point).\nUse your se function within summarize to get a table of the mean and s.e. of hwy and cty by class in the mpg dataset.\nUse your se function within summarize to get a table of the mean and s.e. of arr_delay and dep_delay by carrier in the flights dataset. Why does the output look like this?\nMake your se function handle NAs with an na.rm option. Test your new function (you can call it se again) on a vector that doesn’t include NA and on the same vector with an added NA. Be sure to check that it gives the expected output with na.rm = TRUE and na.rm = FALSE. Make na.rm = FALSE the default value. Repeat #4. (Hint: be sure when you divide by sample size you don’t count any NAs)\nCreate both_na(), a function that takes two vectors of the same length and returns how many positions have an NA in both vectors. Hint: create two vectors like test_x &lt;- c(1, 2, 3, NA, NA) and test_y &lt;- c(NA, 1, 2, 3, NA) and write code that works for test_x and test_y, then turn it into a function that can handle any x and y. (In this case, the answer would be 1, since both vectors have NA in the 5th position.) Test it for at least one more combination of x and y.\nRun your code from (6) with the following two vectors: test_x &lt;- c(1, 2, 3, NA, NA, NA) and test_y &lt;- c(NA, 1, 2, 3, NA). Did you get the output you wanted or expected? Modify your function using if, else, and stop to print an error if x and y are not the same length. Then test again with test_x, test_y and the sets of vectors you used in (6).\nHere is a way to get not_cancelled flights in the flights dataset:\n\n\nnot_cancelled &lt;- flights |&gt; \n  filter(!is.na(dep_delay), !is.na(arr_delay))\n\nIs it necessary to check is.na for both departure and arrival? Using summarize, find the number of flights missing departure delay, arrival delay, and both. (Use your new function!)\n\nRead the code for each of the following three functions, puzzle out what they do, and then brainstorm better names.\n\n\nf1 &lt;- function(time1, time2) {\n  hour1 &lt;- time1 %/% 100\n  min1 &lt;- time1 %% 100\n  hour2 &lt;- time2 %/% 100\n  min2 &lt;- time2 %% 100\n  \n  (hour2 - hour1)*60 + (min2 - min1)\n}\n\n\nf2 &lt;- function(lengthcm, widthcm) {\n  (lengthcm / 2.54) * (widthcm / 2.54)\n}\n\n\nf3 &lt;- function(x) {\n  fct_collapse(x, \"non answer\" = c(\"No answer\", \"Refused\", \n                                   \"Don't know\", \"Not applicable\"))\n}\n\n\nExplain what the following function does and demonstrate by running foo1(x) with a few appropriately chosen vectors x. (Hint: set x and run the “guts” of the function piece by piece.)\n\n\nfoo1 &lt;- function(x) {\n  diff &lt;- x[-1] - x[1:(length(x) - 1)]\n  sum(diff &lt; 0)\n}\n\n\nThe foo1() function doesn’t perform well if a vector has missing values. Amend foo1() so that it produces a helpful error message and stops if there are any missing values in the input vector. Show that it works with appropriately chosen vectors x. Be sure you add error = TRUE to your R chunk, or else knitting will fail!\nWrite a function called greet using if, else if, and else to print out “good morning” if it’s before 12 PM, “good afternoon” if it’s between 12 PM and 5 PM, and “good evening” if it’s after 5 PM. Your function should work if you input a time like: greet(time = \"2018-05-03 17:38:01 CDT\") or if you input the current time with greet(time = Sys.time()). [Hint: check out the hour function in the lubridate package]\nModify the summary6() function from earlier to add an argument that gives the user an option to remove missing values, if any exist. Show that your function works for (a) the hwy variable in mpg_tbl &lt;- as_tibble(mpg), and (b) the age variable in gss_cat.\nAdd an argument to (13) to produce summary statistics by group for a second variable (you should now have 4 possible inputs to your function). Show that your function works for (a) the hwy variable in mpg_tbl &lt;- as_tibble(mpg) grouped by drv, and (b) the age variable in gss_cat grouped by partyid.\nCreate a function that has a vector as the input and returns the last value. (Note: Be sure to use a name that does not write over an existing function!)\nSave your final table from (14) and write a function to draw a scatterplot of a measure of center (mean or median - user can choose) vs. a measure of spread (sd or IQR - user can choose), with points sized by sample size, to see if there is constant variance. Each point should be labeled with partyid, and the plot title should reflect the variables chosen by the user.\n\nHint: start with a ggplot with no user input, and then functionize:\n\nlibrary(ggrepel)\nparty_age |&gt;\n  ggplot(aes(x = mean, y = sd)) + \n    geom_point(aes(size = n)) +\n    geom_smooth(method = lm) +\n    geom_label_repel(aes(label = partyid)) +\n    labs(title = \"Mean vs SD\")",
    "crumbs": [
      "Functions and tidy evaluation"
    ]
  },
  {
    "objectID": "github_intro.html",
    "href": "github_intro.html",
    "title": "Intro to GitHub",
    "section": "",
    "text": "Version Control (GitHub)\nIn order to collaborate on an R project (or any coding project in general), data scientists typically use a version control system like GitHub. With GitHub, you never have to save files as Final.docx, Final2.docx, Newfinal.docx, RealFinal.docx, nothisisreallyit.docx, etc. You update code like .qmd and .Rmd and data files, recording descriptions of any changes. Then if you ever want to go back to an earlier version, GitHub can facilitate that. Or if you want to make your work public, others can see it and even suggest changes, but you are ultimately in control of any changes that get made.\nAnd, you can have multiple collaborators with access to the same set of files. While it can be dicey if multiple people have the file open and make changes at the same time; if you do this with GitHub, it is at least POSSIBLE to get it straightened out, and the second person who tries to save will get warned. If you are both just using a common folder on RStudio, you can easily write over and erase each other’s work. (If you use a common folder, be sure that one person is editing at a time to prevent this).\nIn order to begin to get familiar with GitHub, we will use it to create a course folder for you.\n\n\nGetting started on GitHub and connecting to RStudio\n\nCreate a GitHub account at github.com. It’s usually okay to hit “Skip Personalization” at the bottom of the screen after entering an email, username, and password (you might have to enable 2-factor authentication as well). There are a few bonuses you can get as a student that you might consider.\n\nYou may choose to use a non-St. Olaf email address to ensure you’ll have access to your GitHub account after you graduate.\n\nObtain a personal access token (PAT) in GitHub using the following steps:\n\nClick your profile picture/symbol in the upper right of your GitHub page. Then select Settings &gt; Developer settings &gt; Personal access tokens &gt; Tokens (classic).\nClick “Generate new token (classic)” and give a descriptive name like “My PAT for RStudio”. Note that the default expiration is 30 days; I did a Custom setting through the end of the semester.\nSelect scopes and permissions; I often select: repo, workflow, gist, and user.\nClick “Generate token”. Copy your personal access token and store it somewhere.\n\nStore your credentials in RStudio using the following steps:\n\nIn the console, type library(credentials). You might have to install the credentials package first.\nThen type set_github_pat() and hit Return. You can sign in with the browser, or you can choose the Token option, where you can copy in your personal access token\n\n\nAlert! If the steps in (3) don’t work, you may have to install Git on your computer first. This chapter in “Happy Git with R” provides nice guidance for installing Git. Installing Git for Windows seems to work well on most Windows machines, using this site seems to work well for macOS, and a command like sudo apt-get install git can often work nicely in Linux. Once Git is installed, restart RStudio, and it will usually magically find Git. If not, there’s some good advice in this chapter of “Happy Git with R”. If you ever get frustrated with Git, remember that No one is giving out Git Nerd merit badges! Just muddle through until you figure out something that works for you!\n\n\nCreating an R project (local) that’s connected to GitHub (cloud)\n\nIn your GitHub account, click the \\(+ \\nabla\\) (+down arrow) button near the top right and select New Repository (repo). Put something like “SDS264_F24” for your repository (repo) name; use simple but descriptive names that avoid spaces. Check Private for now; you can turn a repository Public if you want later. Check Add a ReadMe File. Finally hit Create Repository and copy the URL once your repo has been created; the URL should be something like github.com/username/SDS264_F24.\nGo into your RStudio and select File &gt; New Project &gt; Version Control &gt; Git. For the repository URL paste in the URL for the repository you just created. A project directory named “SDS264_F24” will be created in your specified folder (which you can navigate to).\n\nNotice that you are now starting with a blank slate! Nothing in the environment or history. Also note where it says your project name in the top right corner.\nAt this point your should have a GitHub repo called “SDS264_F24” connected to an R project named “SDS264_F24”. The basic framework is set!\nHere is an illustration (source) of the process of using GitHub to manage version control and collaboration, followed by greater detail about each step:\n\n\n\nCreating new files in RStudio (local)\n\nYou can download our first file with in-class exercises here. Just hit the Download Raw File button and note where the file is saved on your computer. Use File &gt; Open File in RStudio to open up 01_review164.qmd. Then, use File &gt; Save As to navigate to the SDS264_F24 folder on your computer and save a copy there. You can even add your name or a few answers and re-save your file.\n\n\n\nMaking sure the connection to GitHub is ready\n\n[If necessary] You may need one of these two steps when using GitHub for the first name in a new R Project, even though you likely did them while installing GitHub.\n\nIn the console, type library(credentials). Then type set_github_pat(), hit Return, and copy in your personal access token.\nIn the Terminal window (this should be the tab next to Console), type the following two lines (be precise with the dashes and spaces!!):\n\n\n\ngit config --global user.name \"YOUR USER NAME\"\n\n\ngit config --global user.email \"YOUR EMAIL ASSOCIATED WITH GITHUB\"\n\n\n\nPushing your work to GitHub (cloud)\n\nWe now want to “push” the changes made in 01_review164.qmd to your GitHub repo (the changes have only been made in your local RStudio for now).\n\nunder the Git tab in the Environment panel, check the boxes in front of all modified files to “stage” your changes. To select a large number of files, check the top box, scroll down to the bottom of the list, and then shift-click the final box\nclick the Commit tab to “commit” your changes (like saving a file locally) along with a message describing the changes you made. GitHub guides you by by showing your old code (red) and new code (green), to make sure you approve of the changes. This first time, all code should be green since we’ve only added new things rather than modifying previously saved files.\n“push” your changes to GitHub (an external website) by clicking the green Up arrow. Refresh your GitHub account to see new files in the user_name.github.io repo!\n\n\n\n\nModifying files that have already been pushed to GitHub\n\nMake and save a change (anything) to your file 01_review164.qmd in RStudio. Now go back under the Git tab and “push” these new changes to GitHub. You’ll have to go through the same process of Stage, Commit, and Push, although this time you’ll see only your newest changes in green when you Commit. Confirm that your changes appear in GitHub.\n\n\n\nPulling work from GitHub\nBefore you start a new session of working on a project in RStudio, you should always Pull changes from GitHub first. Most of the time there will be nothing new, but if a collaborator made changes since the last time you worked on a file, you want to make sure you’re working with the latest and greatest version. If not, you’ll end up trying to Push changes made to an old version, and GitHub will balk and produce Merge Conflict messages. We’ll see how to handle Merge Conflicts later, but it’s a bit of a pain and best avoided!\n\nGo into 01_review164.qmd on GitHub and hit the Edit icon. Add a line anywhere, and then scroll down to hit Commit Changes. (This is not recommended and for illustrative purposes only! You will likely never edit directly in GitHub, but we’re emulating what might happen if a collaborator makes changes since the last time you worked on a document.) Now go back to RStudio and, under the Git tab, “Pull” the changes from GitHub into your R project folder. (Use the blue Down arrow). Confirm that your changes now appear in RStudio. Before you start working on the R server, you should always Pull any changes that might have been made on GitHub (especially if you’re working on a team!), since things can get dicey if you try to merge new changes from RStudio with new changes on GitHub.\n\n\n\nA bit more about R projects\n\nTo see the power of projects, select File &gt; Close Project and Don’t Save the workspace image. Then, select File &gt; Recent Projects &gt; SDS264_F24; you will get a clean Environment and Console once again, but History shows the commands you ran by hand, active Rmd and qmd files appear in the Source panel, and Files contains the Rmd, qmd, html, and csv files produced by your last session. And you can stage, commit and push the changes and the new file to GitHub!",
    "crumbs": [
      "Intro to GitHub"
    ]
  },
  {
    "objectID": "tech_setup.html",
    "href": "tech_setup.html",
    "title": "Tech Setup",
    "section": "",
    "text": "Ideally before class on Thurs Feb 6, and definitely before class on Tues Feb 11, you should follow these instructions to set up the software that we’ll be using throughout the semester. Even if you’ve already downloaded both R and RStudio, you’ll want to re-download to make sure that you have the most current versions.\n\nRequired: Download R and RStudio\n\nFIRST: Download R here.\n\nIn the top section, you will see three links “Download R for …”\nChoose the link that corresponds to your computer.\nAs of the start of this semester, the latest version of R is 4.4.2 (“Pile of Leaves”).\n\nSECOND: Download RStudio here.\n\nClick the button under step 2 to install the version of RStudio recommended for your computer.\nAs of the start of this semester, the latest version of RStudio is 2024.12.0 (Build 467).\n\nTHIRD: Check that when you go to File &gt; New Project &gt; New Directory, you see “Quarto Website” as an option.\n\n\nSuggested: Watch this video from Lisa Lendway at Macalester describing key configuration options for RStudio.\n\nSuggested: Change the default file download location for your internet browser.\n\nGenerally by default, internet browsers automatically save all files to the Downloads folder on your computer. In that case, you have to grab files from Downloads and move them to a more appropriate storage spot. You can change this option so that your browser asks you where to save each file before downloading it.\nThis page has information on how to do this for the most common browsers.\n\n\nRequired: Install required packages.\n\nAn R package is an extra bit of functionality that will help us in our data analysis efforts in a variety of ways. Many contributors create open source packages that can be added to base R to perform certain tasks in new and better ways.\nFor now, we’ll just make sure the tidyverse package is installed. Open RStudio and click on the Packages tab in the bottom right pane. Click the Install button and type “tidyverse” (without quotes) in the pop-up box. Click the Install button at the bottom of the pop-up box.\nYou will see a lot of text from status messages appearing in the Console as the packages are being installed. Wait until you see the &gt; again.\nEnter the command library(tidyverse) in the Console and hit Enter.\n\nQuit RStudio. You’re done setting up!\n\n\nOptional: For a refresher on RStudio features, watch this video. It also shows you how to customize the layout and color scheme of RStudio.",
    "crumbs": [
      "Tech Setup"
    ]
  },
  {
    "objectID": "04_code_quality.html",
    "href": "04_code_quality.html",
    "title": "Code quality",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.",
    "crumbs": [
      "Code quality"
    ]
  },
  {
    "objectID": "04_code_quality.html#code-style",
    "href": "04_code_quality.html#code-style",
    "title": "Code quality",
    "section": "Code style",
    "text": "Code style\nWe are going to take a timeout at this point to focus a little on code quality. Chapter 4 in R4DS provides a nice introduction to code style and why it’s important. As they do in that chapter, we will follow the tidyverse style guide in this class.\nBased on those resources, can you improve this poorly styled code chunk using the data from our DS1 Review activity?\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nVACCINE.DATA &lt;- read_csv(\"https://joeroith.github.io/264_spring_2025/Data/vaccinations_2021.csv\")\n\nRows: 3053 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): state, county, region, metro_status\ndbl (10): rural_urban_code, perc_complete_vac, tot_pop, votes_Trump, votes_B...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nVACCINE.DATA |&gt; filter(state %in% c(\"Minnesota\",\"Iowa\",\"Wisconsin\",\"North Dakota\",\"South Dakota\")) |&gt;\n  mutate(state_ordered=fct_reorder2(state,perc_Biden,perc_complete_vac),prop_Biden=perc_Biden/100,prop_complete_vac=perc_complete_vac/100) |&gt;\nggplot(mapping = aes(x = prop_Biden, y = prop_complete_vac, \n                       color = state_ordered)) +\ngeom_point() + geom_smooth(se = FALSE) +\nlabs(color = \"State\", x = \"Proportion of Biden votes\",\n     y = \"Proportion completely vaccinated\", title = \"The positive relationship between Biden votes and \\n vaccination rates by county differs by state\") +     \ntheme(axis.title = element_text(size=10), plot.title = element_text(size=12))  \n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'",
    "crumbs": [
      "Code quality"
    ]
  },
  {
    "objectID": "04_code_quality.html#code-comments",
    "href": "04_code_quality.html#code-comments",
    "title": "Code quality",
    "section": "Code comments",
    "text": "Code comments\nPlease read Fostering Better Coding Practices for Data Scientists, which lays out a nice case for the importance of teaching good coding practices. In particular, their Top 10 List can help achieve the four Cs (correctness, clarity, containment, and consistency) that typify high-quality code:\n\nChoose good names.\nFollow a style guide consistently.\nCreate documents using tools that support reproducible workflows.\nSelect a coherent, minimal, yet powerful tool kit.\nDon’t Repeat Yourself (DRY).\nTake advantage of a functional programming style.\nEmploy consistency checks.\nLearn how to debug and to ask for help.\nGet (version) control of the situation.\nBe multilingual.\n\nPlease also read the Stack Overflow blog on Best Practices for Writing Code Comments with their set of 9 rules:\n\nRule 1: Comments should not duplicate the code.\nRule 2: Good comments do not excuse unclear code.\nRule 3: If you can’t write a clear comment, there may be a problem with the code.\nRule 4: Comments should dispel confusion, not cause it.\nRule 5: Explain unidiomatic code in comments.\nRule 6: Provide links to the original source of copied code.\nRule 7: Include links to external references where they will be most helpful.\nRule 8: Add comments when fixing bugs.\nRule 9: Use comments to mark incomplete implementations.\n\nIn your projects and homework for this course, we will look for good style and good commenting to optimize your abilities as a collaborating data scientist!",
    "crumbs": [
      "Code quality"
    ]
  }
]