[
  {
    "objectID": "why_quarto.html",
    "href": "why_quarto.html",
    "title": "Why Quarto?",
    "section": "",
    "text": "As described in the quarto documentation: Quarto is a multi-language, next generation version of R Markdown from RStudio, with many new features and capabilities. Like R Markdown, Quarto uses Knitr to execute R code, and is therefore able to render most existing Rmd files without modification.\nData scientists are pretty excited about the introduction of Quarto, and since it represents the future of R Markdown, we will conduct MSCS 264 using Quarto. Intriguing Quarto features that have been cited include:\nHere’s a cool example from the Quarto documentation, showing features like cross-referencing of figures, chunk options using the hash-pipe format, collapsed code, and easy figure legends:"
  },
  {
    "objectID": "why_quarto.html#air-quality",
    "href": "why_quarto.html#air-quality",
    "title": "Why Quarto?",
    "section": "Air Quality",
    "text": "Air Quality\nFigure 1 further explores the impact of temperature on ozone level.\n\n\nCode\nlibrary(ggplot2)\n\nggplot(airquality, aes(Temp, Ozone)) + \n  geom_point() + \n  geom_smooth(method = \"loess\")\n\n\n\n\n\nFigure 1: Temperature and ozone level."
  },
  {
    "objectID": "rtipoftheday.html",
    "href": "rtipoftheday.html",
    "title": "R Tip of the Day",
    "section": "",
    "text": "Signup Sheet"
  },
  {
    "objectID": "miniproject3.html",
    "href": "miniproject3.html",
    "title": "Mini-Project 3: Strings and Regular Expressions",
    "section": "",
    "text": "You will find a data set containing string data. This could be newspaper articles, tweets, songs, plays, movie reviews, or anything else you can imagine. Then you will answer questions of interest and tell a story about your data using string and regular expression skills you have developed.\nYour story must contain the following elements:\n\nat least 3 str_ functions\nat least 3 regular expressions\nat least 2 illustrative, well-labeled plots or tables\na description of what insights can be gained from your plots and tables"
  },
  {
    "objectID": "miniproject3.html#overview",
    "href": "miniproject3.html#overview",
    "title": "Mini-Project 3: Strings and Regular Expressions",
    "section": "",
    "text": "You will find a data set containing string data. This could be newspaper articles, tweets, songs, plays, movie reviews, or anything else you can imagine. Then you will answer questions of interest and tell a story about your data using string and regular expression skills you have developed.\nYour story must contain the following elements:\n\nat least 3 str_ functions\nat least 3 regular expressions\nat least 2 illustrative, well-labeled plots or tables\na description of what insights can be gained from your plots and tables"
  },
  {
    "objectID": "miniproject3.html#timeline",
    "href": "miniproject3.html#timeline",
    "title": "Mini-Project 3: Strings and Regular Expressions",
    "section": "Timeline",
    "text": "Timeline\nMini-Project 3 must be submitted on moodle by 11:00 PM on Mon Apr 15. Ideally, you’d add a tab to your quarto webpage with Mini-Project 3, then you can just submit your URL (as long as your code is linked to the page). You can alternatively submit a knitted pdf showing your code and output."
  },
  {
    "objectID": "miniproject3.html#topic-ideas",
    "href": "miniproject3.html#topic-ideas",
    "title": "Mini-Project 3: Strings and Regular Expressions",
    "section": "Topic Ideas",
    "text": "Topic Ideas\n\nObama tweets\n\n#barack &lt;- read_csv(\"Data/tweets_potus.csv\") \nbarack &lt;- read_csv(\"https://proback.github.io/264_fall_2024/Data/tweets_potus.csv\")\n#michelle &lt;- read_csv(\"Data/tweets_flotus.csv\") \nmichelle &lt;- read_csv(\"https://proback.github.io/264_fall_2024/Data/tweets_flotus.csv\")\n\ntweets &lt;- bind_rows(barack %&gt;% \n                      mutate(person = \"Barack\"),\n                    michelle %&gt;% \n                      mutate(person = \"Michelle\")) %&gt;%\n  mutate(timestamp = ymd_hms(timestamp))\n\nPresident Barack Obama became the first US President with an official twitter account, when @POTUS went live on May 18, 2015. (Yes, there was a time before Twitter.) First Lady Michelle Obama got in on Twitter much earlier, though her first tweet was not from @FLOTUS. All of the tweets from @POTUS and @FLOTUS are now archived on Twitter as @POTUS44 and @FLOTUS44, and they are available as a csv download from the National Archive. Read more here: https://obamawhitehouse.archives.gov/blog/2017/01/05/new-lenses-first-social-media-presidency\nPotential things to investigate:\n\nuse of specific terms\nuse of @, #, RT (retweet), or -mo (personal tweet from Michelle Obama)\ntimestamp for date and time trends\nsentiment analysis (after merging in a sentiment library like in Data Science 1)\nanything else that seems interesting!\n\n\n\nDear Abby advice column\nRead in the “Dear Abby” data underlying The Pudding’s 30 Years of American Anxieties article.\n\nposts &lt;- read_csv(\"https://raw.githubusercontent.com/the-pudding/data/master/dearabby/raw_da_qs.csv\")\n\nTake a couple minutes to scroll throgh the 30 Years of American Anxieties article to get ideas for themes that you might want to search for using regular expressions.\n\n\nOther sources for string data\n\nOther articles from The Pudding\nNY Times headlines from the RTextTools package (see below)\nfurther analysis with the bigspotify data from class\nthe options are endless – be resourceful and creative!\n\n\nlibrary(RTextTools)  # may have to install first\n\nWarning: package 'RTextTools' was built under R version 4.3.3\n\ndata(NYTimes)\nas_tibble(NYTimes)\n\n# A tibble: 3,104 × 5\n   Article_ID Date      Title                                 Subject Topic.Code\n        &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;                                 &lt;fct&gt;        &lt;int&gt;\n 1      41246 1-Jan-96  Nation's Smaller Jails Struggle To C… Jails …         12\n 2      41257 2-Jan-96  FEDERAL IMPASSE SADDLING STATES WITH… Federa…         20\n 3      41268 3-Jan-96  Long, Costly Prelude Does Little To … Conten…         20\n 4      41279 4-Jan-96  Top Leader of the Bosnian Serbs Now … Bosnia…         19\n 5      41290 5-Jan-96  BATTLE OVER THE BUDGET: THE OVERVIEW… Battle…          1\n 6      41302 7-Jan-96  South African Democracy Stumbles on … politi…         19\n 7      41314 8-Jan-96  Among Economists, Little Fear on Def… econom…          1\n 8      41333 10-Jan-96 BATTLE OVER THE BUDGET: THE OVERVIEW… budget…          1\n 9      41344 11-Jan-96 High Court Is Cool To Census Change   census…         20\n10      41355 12-Jan-96 TURMOIL AT BARNEYS: THE DIFFICULTIES… barney…         15\n# ℹ 3,094 more rows"
  },
  {
    "objectID": "miniproject1.html",
    "href": "miniproject1.html",
    "title": "Mini-Project 1: Maps",
    "section": "",
    "text": "Overview\nYou will produce a personal website using Quarto that contains, at a minimum, a Home/About page with a picture or image, some basic information about you, and a link to the GitHub repo that contains the R code for your website.\nThen, in the Menu bar, you should have a selection called “Maps” with two sub-selections:\n\n“US States”. On this page, you should produce a choropleth map of the U.S. states (you can choose to exclude Alaska and Hawaii) like we did in the “Creating Informative Maps” activity. Just as we found state-level data from both a vaccine data set and the poliscidata package, you should find your own state-level data. Merge your data with the us_states dataframe, and then produce a choropleth map. Your variable of interest can be either numeric or categorical. Be sure you label your plot well and provide a description of what insights can be gained from your plot.\n“Wisconsin Districts”. On this page, you will emulate our analysis of potential gerrymandering in North Carolina by generating a similar plot of the state of Wisconsin. For background about controversial congressional districts and gerrymandering in Wisconsin, see this article.\n\nA few detailed instructions and hints for Part 2:\n\nuse voting data from 2016 from the fec16 package. Make sure that data from fec16 is being used rather than similarly named data from fec12, for instance by first running detach(\"package:fec12\").\nyour final plot should have shaded reds and blues representing relative levels of support for Republican and Democrat candidates in a district. You may use ggplot2 or leaflet.\nif you’ve been doing previous geospatial modeling in RStudio, you may have to clear your environment and start a new R session to have adequate space available.\n\nthe UCLA site with district geographies only goes to 2012, so you can’t grab 115th Congress in 2016 from there. You could try to find that updated geospatial information, potentially at a site like this:, but it’s fine to just use the 2012 geospatial data like we did with North Carolina.\n\nAgain, be sure you label your plot well and provide a description of what insights can be gained from your plot.\n\n\nTimeline\nMini-Project 1 must be submitted on moodle by 11:00 PM on Wed Feb 28. All you need to submit is a URL for your website!"
  },
  {
    "objectID": "github_website.html",
    "href": "github_website.html",
    "title": "Using GitHub",
    "section": "",
    "text": "Version Control (GitHub)\nIn order to collaborate on an R project (or any coding project in general), data scientists typically use a version control system like GitHub. With GitHub, you never have to save files as Final.docx, Final2.docx, Newfinal.docx, RealFinal.docx, nothisisreallyit.docx, etc. You update code like .qmd and .Rmd and data files, recording descriptions of any changes. Then if you ever want to go back to an earlier version, GitHub can facilitate that. Or if you want to make your work public, others can see it and even suggest changes, but you are ultimately in control of any changes that get made.\nAnd, you can have multiple collaborators with access to the same set of files. While it can be dicey if multiple people have the file open and make changes at the same time, if you do this with GitHub, it is at least POSSIBLE to get it straightened out, and the second person who tries to save will get warned. If you are both just using a common folder on RStudio, you can easily write over and erase each other’s work. (If you use a common folder, be sure that one person is editing at a time to prevent this).\nIn order to begin to get familiar with GitHub, we will use it to create a personal website.\n\n\n\n\n\n\nAn alternative approach\n\n\n\n\n\nYou can also follow instructions at the excellent website Creating Your Personal Website using Quarto by Sam Csik (the underlying quarto code for creating this website can be found here).\nProcess Overview\n\nThe general plan is to create a local version (using RStudio on your own computer) of a quarto webpage, and then connect the files you build to an upstream cloud-based hosting service (GitHub).\n\nAfter initializing a basic webpage, you will toggle between RStudio to improve the webpage and GitHub to save the results in a public repository.\nFinally, you will use GitHub Page to publish and host your webpage.\n\nClarifications\nWhile Sam Csik’s instructions and insights are awesome, there are a few places where you should tread carefully or take a slightly different path:\n\nBe sure to heed the “Before getting started” box in this section! In particular, follow the MEDS Installation Guide and make sure you have the latest versions of both R and RStudio, a GitHub account, and Git installed on your machine.\nIn that same section, be sure to choose the “Using the RStudio IDE” tab to access that set of instructions.\nBe careful with the first step in this section. I would recommend the second option, although you’ll actually need to select File &gt; New File &gt; Text file to open a blank text file, and then use Save As to save that blank file as “.nojekyll” (this file will be invisible in RStudio but not in GitHub). If you use the first option, her command is only for Macs; Windows users should use “copy NUL .nojekyll” in the Terminal.\nIn Step 3, you can also type “quarto_render()” in the Console. You might have to run this twice if you needed to delete your _site folder as in the green box.\nIn Step 5, you also have to return to the main page of your GitHub repository and hit the Settings button to the right of About. In the box that appears, you’ll enter a description of your project, and check “Use your GitHub Pages website”.\n\n\n\n\n\n\nGetting started on GitHub and connecting to RStudio\n\nCreate a GitHub account at github.com. It’s usually okay to hit “Skip Personalization” at the bottom of the screen after entering an email, username, and password. There are a few bonuses you can get as a student that you might consider.\nObtain a personal access token (PAT) in GitHub using the following steps:\n\nClick your profile picture/symbol in the upper right of your GitHub page. Then select Settings &gt; Developer settings &gt; Personal access tokens &gt; Tokens (classic).\nClick “Generate new token” and give a descriptive name like “My PAT for RStudio”. Note that the default expiration is 30 days; I did a Custom setting through the end of the semester.\nSelect scopes and permissions; I often select: repo, workflow, gist, and user.\nClick “Generate token”. Copy your personal access token and store it somewhere.\n\nStore your credentials in RStudio using the following steps:\n\nIn the console, type library(credentials). You might have to install the credentials package first.\nThen type set_github_pat(), hit Return, choose the Token option, and copy in your personal access token\n\n\nAlert! If the steps in (3) don’t work, you may have to install Git on your computer first. This chapter in “Happy Git with R” provides nice guidance for installing Git. Installing Git for Windows seems to work well on most Windows machines, using this site seems to work well for macOS, and a command like sudo apt-get install git can often work nicely in Linux. Once Git is installed, restart RStudio, and it will usually magically find Git. If not, there’s some good advice in this chapter of “Happy Git with R”. If you ever get frustrated with Git, remember that No one is giving out Git Nerd merit badges! Just muddle through until you figure out something that works for you!\n\n\nCreating an R project (local) that’s connected to GitHub (cloud)\n\nIn your GitHub account, click the \\(+ \\nabla\\) (+down arrow) button near the top right and select New Repository (repo). For a personal website, you might put something like “proback.github.io” for your repository (repo) name, where you should replace proback with your GitHub user name. For other repos, choose simple but descriptive names that should avoid spaces. Be sure Public is checked; sometimes I create Private repositories and turn them Public if I want later, but we know a personal webpage should be Public. Check Add a ReadMe File. Finally hit Create Repository and copy the URL once your repo has been created; the URL should be something like github.com/username/user_name.github.io.\nGo into your RStudio and select File &gt; New Project &gt; Version Control &gt; Git. For the repository URL paste in the URL for the repository you just created. A project directory named “username.github.io” will be created in your specified folder (which you can navigate to).\n\nNotice that you are now starting with a blank slate! Nothing in the environment or history. Also note where it says your project name in the top right corner.\nAt this point your should have a GitHub repo called “user_name.github.io” connected to an R project named “username.github.io”. The basic framework is set!\n\n\nCreating a personal website in RStudio (local)\n\nFollow the instructions here for constructing a personal website on your local machine. These instructions use materials from Brianna Heggeseth at Macalester, Joyce Robbins at Columbia, and quarto documentation.\n\n\n\nPushing your work to GitHub (cloud)\n\nWe now want to “push” the changes made in “user_name.github.io” to your GitHub repo (the changes have only been made in your local RStudio for now).\n\nIn the console, type library(credentials)\nThen type set_github_pat(), hit Return, and copy in your personal access token. We do this again because we are in a new project. (These first two steps might not be necessary, but it’s good to make sure.)\nunder the Git tab in the Environment panel, check the boxes in front of all modified files to “stage” your changes. To select a large number of files, check the top box, scroll down to the bottom of the list, and then shift-click the final box\nclick the Commit tab to “commit” your changes (like saving a file locally) along with a message describing the changes you made. GitHub guides you by by showing your old code (red) and new code (green), to make sure you approve of the changes. This first time, all code should be green since we’ve only added new things rather than modifying previously saved files.\n“push” your changes to GitHub (an external website) by clicking the green Up arrow. Refresh your GitHub account to see new files in the user_name.github.io repo!\n\n\n\n\nModifying files that have already been pushed to GitHub\n\nMake a change (anything) to one of your files in RStudio. Now go back under the Git tab and “push” these new changes to GitHub. You’ll have to go through the same process of Stage, Commit, and Push, although this time you’ll see only your newest changes in green when you Commit. Confirm that your changes appear in GitHub.\n\n\n\nPulling work from GitHub\nBefore you start a new session of working on a project in RStudio, you should always Pull changes from GitHub first. Most of the time there will be nothing new, but if a collaborator made changes since the last time you worked on a file, you want to make sure you’re working with the latest and greatest version. If not, you’ll end up trying to Push changes made to an old version, and GitHub will balk and produce Merge Conflict messages. We’ll see how to handle Merge Conflicts later, but it’s a bit of a pain and best avoided!\n\nGo into one of your files on GitHub and hit the Edit icon. Add a line anywhere, and then scroll down to hit Commit Changes. (This is not recommended and for illustrative purposes only! You typically won’t edit directly in GitHub, but we’re emulating what might happen if a collaborator makes changes since the last time you worked on a document.) Now go back to RStudio and, under the Git tab, “Pull” the changes from GitHub into your R project folder. (Use the blue Down arrow). Confirm that your changes now appear in RStudio. Before you start working on the R server, you should always Pull any changes that might have been made on GitHub (especially if you’re working on a team!), since things can get dicey if you try to merge new changes from RStudio with new changes on GitHub.\n\n\n\nA bit more about R projects\n\nTo see the power of projects, select File &gt; Close Project and Don’t Save the workspace image. Then, select File &gt; Recent Projects &gt; user_name.github.io; you will get a clean Environment and Console once again, but History shows the commands you ran by hand, active Rmd and qmd files appear in the Source panel, and Files contains the Rmd, qmd, html, and csv files produced by your last session. And you can stage, commit and push the changes and the new file to GitHub!"
  },
  {
    "objectID": "MDSR_Ch13_simulation.html",
    "href": "MDSR_Ch13_simulation.html",
    "title": "MDSR Ch 13: Simulation",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mdsr)"
  },
  {
    "objectID": "MDSR_Ch13_simulation.html#section-13.3-randomizing-functions",
    "href": "MDSR_Ch13_simulation.html#section-13.3-randomizing-functions",
    "title": "MDSR Ch 13: Simulation",
    "section": "Section 13.3: Randomizing functions",
    "text": "Section 13.3: Randomizing functions\n\n# uniform random number generator (between 0 and 1)\nrunif(5)\n\n[1] 0.9669975 0.1437981 0.7412448 0.3753798 0.3631877\n\n# select one value at random from a vector\nselect_one &lt;- function(vec) {\n  n &lt;- length(vec)\n  ind &lt;- which.max(runif(n))\n  vec[ind]\n}\nselect_one(letters) # letters are a, b, c, ..., z\n\n[1] \"f\"\n\nselect_one(letters)\n\n[1] \"m\"\n\n# can also sample from other distributions (e.g. rnorm(1))"
  },
  {
    "objectID": "MDSR_Ch13_simulation.html#section-13.4-simulating-variability",
    "href": "MDSR_Ch13_simulation.html#section-13.4-simulating-variability",
    "title": "MDSR Ch 13: Simulation",
    "section": "Section 13.4: Simulating variability",
    "text": "Section 13.4: Simulating variability\n\nSection 13.4.1: The partially-planned rendezvous\n\nn &lt;- 100000\nsim_meet &lt;- tibble(\n  sally = runif(n, min = 0, max = 60),\n  joan = runif(n, min = 0, max = 60),\n  result = ifelse(\n    abs(sally - joan) &lt;= 10, \"They meet\", \"They do not\"\n  )\n)\nmosaic::tally(~ result, format = \"percent\", data = sim_meet)\n\nresult\nThey do not   They meet \n     69.405      30.595 \n\nggplot(data = sim_meet, aes(x = joan, y = sally, color = result)) + \n  geom_point(alpha = 0.3) + \n  geom_abline(intercept = 10, slope = 1) + \n  geom_abline(intercept = -10, slope = 1) + \n  scale_color_brewer(palette = \"Set2\")\n\n\n\n\nIf Joan revised her strategy to arrive between 7:10 and 7:50, what would her new probability of meeting up with Sally be?\n\n\nSection 13.4.2: The jobs report (How not to be misled)\nBased on this article from the New York Times.\n\n# simulate a year's worth of jobs reports with no real patterns\njobs_true &lt;- 150\njobs_se &lt;- 65  # in thousands of jobs - this reflect statistical noise described by the Labor Department\n\ngen_samp &lt;- function(true_mean, true_sd, \n                     num_months = 12, delta = 0, id = 1) {\n  samp_year &lt;- rep(true_mean, num_months) + \n    rnorm(num_months, mean = delta * (1:num_months), sd = true_sd)\n  return(\n    tibble(\n      jobs_number = samp_year, \n      month = as.factor(1:num_months), \n      id = id\n    )\n  )\n}\n\n# Try our new function\ngen_samp(true_mean = 150, true_sd = 65, id = \"Sample 1\")\n\n# A tibble: 12 × 3\n   jobs_number month id      \n         &lt;dbl&gt; &lt;fct&gt; &lt;chr&gt;   \n 1      166.   1     Sample 1\n 2      295.   2     Sample 1\n 3       85.7  3     Sample 1\n 4      226.   4     Sample 1\n 5      193.   5     Sample 1\n 6       -1.67 6     Sample 1\n 7      133.   7     Sample 1\n 8      152.   8     Sample 1\n 9      173.   9     Sample 1\n10      -16.1  10    Sample 1\n11      188.   11    Sample 1\n12       91.8  12    Sample 1\n\n# params contains info about 3 simulations we want to run\nn_sims &lt;- 3\nparams &lt;- tibble(\n  sd = c(0, rep(jobs_se, n_sims)), \n  id = c(\"Truth\", paste(\"Sample\", 1:n_sims))\n)\nparams\n\n# A tibble: 4 × 2\n     sd id      \n  &lt;dbl&gt; &lt;chr&gt;   \n1     0 Truth   \n2    65 Sample 1\n3    65 Sample 2\n4    65 Sample 3\n\ndf &lt;- params %&gt;%\n  pmap_dfr(~gen_samp(true_mean = jobs_true, true_sd = ..1, id = ..2))\nprint(df, n = Inf)\n\n# A tibble: 48 × 3\n   jobs_number month id      \n         &lt;dbl&gt; &lt;fct&gt; &lt;chr&gt;   \n 1       150   1     Truth   \n 2       150   2     Truth   \n 3       150   3     Truth   \n 4       150   4     Truth   \n 5       150   5     Truth   \n 6       150   6     Truth   \n 7       150   7     Truth   \n 8       150   8     Truth   \n 9       150   9     Truth   \n10       150   10    Truth   \n11       150   11    Truth   \n12       150   12    Truth   \n13        66.1 1     Sample 1\n14       164.  2     Sample 1\n15       252.  3     Sample 1\n16       309.  4     Sample 1\n17       160.  5     Sample 1\n18        62.7 6     Sample 1\n19       108.  7     Sample 1\n20       153.  8     Sample 1\n21       210.  9     Sample 1\n22        65.7 10    Sample 1\n23       237.  11    Sample 1\n24        59.7 12    Sample 1\n25        71.9 1     Sample 2\n26       159.  2     Sample 2\n27       162.  3     Sample 2\n28       119.  4     Sample 2\n29       288.  5     Sample 2\n30       139.  6     Sample 2\n31       137.  7     Sample 2\n32       315.  8     Sample 2\n33       141.  9     Sample 2\n34        52.9 10    Sample 2\n35       178.  11    Sample 2\n36       189.  12    Sample 2\n37       182.  1     Sample 3\n38        99.5 2     Sample 3\n39        57.5 3     Sample 3\n40       101.  4     Sample 3\n41       176.  5     Sample 3\n42        96.8 6     Sample 3\n43       183.  7     Sample 3\n44       155.  8     Sample 3\n45        74.9 9     Sample 3\n46       114.  10    Sample 3\n47        92.5 11    Sample 3\n48       151.  12    Sample 3\n\nggplot(data = df, aes(x = month, y = jobs_number)) + \n  geom_hline(yintercept = jobs_true, linetype = 2) + \n  geom_col() + \n  facet_wrap(~ id) + \n  ylab(\"Number of new jobs (in thousands)\")\n\n\n\n\n\n\nSection 13.4.3: Restaurant health and sanitation grades\nHealth inspectors in New York City assign a score to restaurants based on an unannounced yearly inspection. Scores 0-13 receive an A, 14-27 receive a B, and those 28 or above receive a C. There seems to be some strange patterns near the threshold of 13…\n\nminval &lt;- 7\nmaxval &lt;- 19\nviolation_scores &lt;- Violations %&gt;%\n  filter(lubridate::year(inspection_date) == 2015) %&gt;%\n  filter(score &gt;= minval & score &lt;= maxval) %&gt;%\n  select(dba, score)\n\nggplot(data = violation_scores, aes(x = score)) + \n  geom_histogram(binwidth = 0.5) + \n  geom_vline(xintercept = 13, linetype = 2) + \n  scale_x_continuous(breaks = minval:maxval) + \n  annotate(\n    \"text\", x = 10, y = 15000, \n    label = \"'A' grade: score of 13 or less\"\n  )\n\n\n\n# a simple simulation where scores of 13 and 14 are equally likely\nscores &lt;- mosaic::tally(~score, data = violation_scores)\nscores\n\nscore\n    7     8     9    10    11    12    13    14    15    16    17    18    19 \n 5985  3026  8401  9007  8443 13907  9021  2155  2679  2973  4720  4119  4939 \n\nmean(scores[c(\"13\", \"14\")])\n\n[1] 5588\n\nrandom_flip &lt;- 1:1000 %&gt;%\n  map_dbl(~mosaic::nflip(scores[\"13\"] + scores[\"14\"])) %&gt;%\n  enframe(name = \"sim\", value = \"heads\")\nhead(random_flip, 3)\n\n# A tibble: 3 × 2\n    sim heads\n  &lt;int&gt; &lt;dbl&gt;\n1     1  5562\n2     2  5590\n3     3  5532\n\n# compare observed scores of 14 with expected number if 13 and 14 equally likely\nggplot(data = random_flip, aes(x = heads)) + \n  geom_histogram(binwidth = 10) + \n  geom_vline(xintercept = scores[\"14\"], col = \"red\") + \n  annotate(\n    \"text\", x = 2200, y = 75, \n    label = \"observed\", hjust = \"left\"\n  ) + \n  xlab(\"Number of restaurants with scores of 14 (if equal probability)\")"
  },
  {
    "objectID": "MDSR_Ch13_simulation.html#section-13.6-key-principles-of-simulation",
    "href": "MDSR_Ch13_simulation.html#section-13.6-key-principles-of-simulation",
    "title": "MDSR Ch 13: Simulation",
    "section": "Section 13.6: Key principles of simulation",
    "text": "Section 13.6: Key principles of simulation\n\nSection 13.6.3: Reproducibility and random number seeds\n\n# How many simulations should we do in random rendezvous problem?\ncampus_sim &lt;- function(sims = 1000, wait = 10) {\n  sally &lt;- runif(sims, min = 0, max = 60)\n  joan &lt;- runif(sims, min = 0, max = 60)\n  return(\n    tibble(\n      num_sims = sims, \n      meet = sum(abs(sally - joan) &lt;= wait),\n      meet_pct = meet / num_sims,\n    )\n  )\n}\n\nreps &lt;- 5000\nsim_results &lt;- 1:reps %&gt;%\n  map_dfr(~map_dfr(c(100, 400, 1600), campus_sim))\n\nsim_results %&gt;%\n  group_by(num_sims) %&gt;%\n  skim(meet_pct)\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvar\nnum_sims\nn\nna\nmean\nsd\np0\np25\np50\np75\np100\n\n\n\n\nmeet_pct\n100\n5000\n0\n0.31\n0.05\n0.16\n0.27\n0.30\n0.34\n0.47\n\n\nmeet_pct\n400\n5000\n0\n0.31\n0.02\n0.22\n0.29\n0.30\n0.32\n0.39\n\n\nmeet_pct\n1600\n5000\n0\n0.31\n0.01\n0.27\n0.30\n0.31\n0.31\n0.35\n\n\n\n\nsim_results %&gt;%\n  ggplot(aes(x = meet_pct, color = factor(num_sims))) + \n  geom_density(linewidth = 2) + \n  geom_vline(aes(xintercept = 11/36), linetype = 3) +\n  scale_x_continuous(\"Proportion of times that Sally and Joan meet\") + \n  scale_color_brewer(\"Number\\nof sims\", palette = \"Set2\")\n\n\n\n# One more important idea: seeds produce the same list of random values\n\nset.seed(1974)\ncampus_sim()\n\n# A tibble: 1 × 3\n  num_sims  meet meet_pct\n     &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt;\n1     1000   308    0.308\n\ncampus_sim()\n\n# A tibble: 1 × 3\n  num_sims  meet meet_pct\n     &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt;\n1     1000   331    0.331\n\nset.seed(1974)\ncampus_sim()\n\n# A tibble: 1 × 3\n  num_sims  meet meet_pct\n     &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt;\n1     1000   308    0.308\n\ncampus_sim()\n\n# A tibble: 1 × 3\n  num_sims  meet meet_pct\n     &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt;\n1     1000   331    0.331"
  },
  {
    "objectID": "11_SQL.html",
    "href": "11_SQL.html",
    "title": "MDSR Ch 15: Database querying using SQL",
    "section": "",
    "text": "We will begin by walking through many of the examples in MDSR Chapter 15, using a .qmd file that can be downloaded here. Just hit the Download Raw File button. That code and output will not be replicated below.\nIn the code and output below, we present a set of practice exercises in converting from the tidyverse to SQL. This code can be downloaded here.\nlibrary(tidyverse)\nlibrary(mdsr)\nlibrary(dbplyr)\nlibrary(DBI)\n# connect to the scidb server on Amazon Web Services - the airlines \n# database lives on a remote server\ndb &lt;- dbConnect_scidb(\"airlines\")\nflights &lt;- tbl(db, \"flights\")\nplanes &lt;- tbl(db, \"planes\")"
  },
  {
    "objectID": "11_SQL.html#on-your-own---extended-example-from-mdsr",
    "href": "11_SQL.html#on-your-own---extended-example-from-mdsr",
    "title": "MDSR Ch 15: Database querying using SQL",
    "section": "On Your Own - Extended Example from MDSR",
    "text": "On Your Own - Extended Example from MDSR\nRefer to Section 15.5 in MDSR, where they attempt to replicate FiveThirtyEight’s plot of slowest and fastest airports in the section below Figure 15.1. Instead of using target time, which has a complex definition, we will use arrival time, which oversimplifies the situation but gets us in the ballpark.\nThe MDSR authors provide a mix of SQL and R code to perform their analysis, but the code will not work if you simply cut-and-paste as-is into R. Your task is to convert the book code into something that actually runs. [Hint: use dbGetQuery()]"
  },
  {
    "objectID": "11_SQL.html#on-your-own---practice-with-sql",
    "href": "11_SQL.html#on-your-own---practice-with-sql",
    "title": "MDSR Ch 15: Database querying using SQL",
    "section": "On Your Own - Practice with SQL",
    "text": "On Your Own - Practice with SQL\nThese problems are based on class exercises from MSCS 164 in Fall 2023, so you’ve already solved them in R! Now we’re going to try to duplicate those solutions in SQL.\n\n# Read in 2013 NYC flights data\nlibrary(nycflights13)\nflights_nyc13 &lt;- nycflights13::flights\nplanes_nyc13 &lt;- nycflights13::planes\n\n\nSummarize carriers flying to MSP by number of flights and proportion that are cancelled (assuming that a missing arrival time indicates a cancelled flight). [This was #4 in 17_longer_pipelines.Rmd.]\n\n\n# Original solution from MSCS 164\nflights_nyc13 |&gt;\n  mutate(carrier = fct_collapse(carrier, \"Delta +\" = c(\"DL\", \"9E\"), \n                                      \"American +\"= c(\"AA\", \"MQ\"), \n                                     \"United +\" = c(\"EV\", \"OO\", \"UA\"))) |&gt;\n  filter(dest == \"MSP\") |&gt;   \n  group_by(origin, carrier) |&gt;\n  summarize(n_flights = n(), \n            num_cancelled = sum(is.na(arr_time)),\n            prop_cancelled = mean(is.na(arr_time)))\n\n# A tibble: 5 × 5\n# Groups:   origin [3]\n  origin carrier    n_flights num_cancelled prop_cancelled\n  &lt;chr&gt;  &lt;fct&gt;          &lt;int&gt;         &lt;int&gt;          &lt;dbl&gt;\n1 EWR    Delta +          598            10         0.0167\n2 EWR    United +        1779           105         0.0590\n3 JFK    Delta +         1095            41         0.0374\n4 LGA    Delta +         2420            25         0.0103\n5 LGA    American +      1293            62         0.0480\n\n\nFirst duplicate the output above, then check trends across all years and origins. Here are a few hints:\n\nuse flights instead of flights_nyc13\nremember that flights_nyc13 only contained 2013 and 3 NYC origin airports (EWR, JFK, LGA)\nis.na can be replaced with CASE WHEN arr_time = ‘NA’ THEN 1 ELSE 0 END\nCASE WHEN can also be used replace fct_collapse\n\n\nPlot number of flights vs. proportion cancelled for every origin-destination pair (assuming that a missing arrival time indicates a cancelled flight). [This was #7 in 17_longer_pipelines.Rmd.]\n\n\n# Original solution from MSCS 164\nflights_nyc13 |&gt;\n  group_by(origin, dest) |&gt;\n  summarize(n = n(),\n            prop_cancelled = mean(is.na(arr_time))) |&gt;\n  filter(prop_cancelled &lt; 1) |&gt;\n  ggplot(aes(n, prop_cancelled)) + \n  geom_point()\n\n\n\n\nFirst duplicate the plot above, then check trends across all years and origins. Do all of the data wrangling in SQL. Here are a few hints:\n\nuse flights instead of flights_nyc13\nremember that flights_nyc13 only contained 2013 and 3 NYC origin airports (EWR, JFK, LGA)\nuse an sql chunk and an r chunk\ninclude connection = and output.var = in your sql chunk header (this doesn’t seem to work with dbGetQuery()…)\n\n\nProduce a table of weighted plane age by carrier, where weights are based on number of flights per plane. [This was #6 in 26_more_joins.Rmd.]\n\n\n# Original solution from MSCS 164\nflights_nyc13 |&gt;\n  left_join(planes_nyc13, join_by(tailnum)) |&gt;\n  mutate(plane_age = 2013 - year.y) |&gt;\n  group_by(carrier) |&gt;\n  summarize(unique_planes = n_distinct(tailnum),\n            mean_weighted_age = mean(plane_age, na.rm =TRUE),\n            sd_weighted_age = sd(plane_age, na.rm =TRUE)) |&gt;\n  arrange(mean_weighted_age)\n\n# A tibble: 16 × 4\n   carrier unique_planes mean_weighted_age sd_weighted_age\n   &lt;chr&gt;           &lt;int&gt;             &lt;dbl&gt;           &lt;dbl&gt;\n 1 HA                 14              1.55            1.14\n 2 AS                 84              3.34            3.07\n 3 VX                 53              4.47            2.14\n 4 F9                 26              4.88            3.67\n 5 B6                193              6.69            3.29\n 6 OO                 28              6.84            2.41\n 7 9E                204              7.10            2.67\n 8 US                290              9.10            4.88\n 9 WN                583              9.15            4.63\n10 YV                 58              9.31            1.93\n11 EV                316             11.3             2.29\n12 FL                129             11.4             2.16\n13 UA                621             13.2             5.83\n14 DL                629             16.4             5.49\n15 AA                601             25.9             5.42\n16 MQ                238             35.3             3.13\n\n\nFirst duplicate the output above, then check trends across all years and origins. Do all of the data wrangling in SQL. Here are a few hints:\n\nuse flights instead of flights_nyc13\nremember that flights_nyc13 only contained 2013 and 3 NYC origin airports (EWR, JFK, LGA)\nyou’ll have to merge the flights dataset with the planes dataset\nyou can use DISTINCT inside a COUNT()"
  },
  {
    "objectID": "09_strings_part2.html",
    "href": "09_strings_part2.html",
    "title": "Strings: In-class Exercises",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\nThis uses parts of MDSR Ch 14: Strings and Ch 15: Regular Expressions (both the first and second editions)."
  },
  {
    "objectID": "09_strings_part2.html#manipulating-strings",
    "href": "09_strings_part2.html#manipulating-strings",
    "title": "Strings: In-class Exercises",
    "section": "Manipulating strings",
    "text": "Manipulating strings\nstr functions to know for manipulating strings:\n\nstr_length()\nstr_sub()\nstr_c()\nstr_to_lower()\nstr_to_upper()\nstr_to_title()\nstr_replace() not in video examples\n\n\nlibrary(tidyverse)\n\n#spotify &lt;- read_csv(\"Data/spotify.csv\") \nspotify &lt;- read_csv(\"https://proback.github.io/264_fall_2024/Data/spotify.csv\")\n\nspot_smaller &lt;- spotify |&gt;\n  select(title, artist, album_release_date, album_name, subgenre, playlist_name)\n\nspot_smaller &lt;- spot_smaller[c(5, 32, 49, 52,  83, 175, 219, 231,  246, 265), ]\nspot_smaller\n\n# A tibble: 10 × 6\n   title             artist album_release_date album_name subgenre playlist_name\n   &lt;chr&gt;             &lt;chr&gt;  &lt;chr&gt;              &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;        \n 1 Hear Me Now       Alok   2016-01-01         Hear Me N… indie p… \"Chillout & …\n 2 Run the World (G… Beyon… 2011-06-24         4          post-te… \"post-teen a…\n 3 Formation         Beyon… 2016-04-23         Lemonade   hip pop  \"Feeling Acc…\n 4 7/11              Beyon… 2014-11-24         BEYONCÉ [… hip pop  \"Feeling Acc…\n 5 My Oh My (feat. … Camil… 2019-12-06         Romance    latin p… \"2020 Hits &…\n 6 It's Automatic    Frees… 2013-11-28         It's Auto… latin h… \"80's Freest…\n 7 Poetic Justice    Kendr… 2012               good kid,… hip hop  \"Hip Hop Con…\n 8 A.D.H.D           Kendr… 2011-07-02         Section.80 souther… \"Hip-Hop 'n …\n 9 Ya Estuvo         Kid F… 1990-01-01         Hispanic … latin h… \"HIP-HOP: La…\n10 Runnin (with A$A… Mike … 2018-11-16         Creed II:… gangste… \"RAP Gangsta\""
  },
  {
    "objectID": "09_strings_part2.html#warm-up",
    "href": "09_strings_part2.html#warm-up",
    "title": "Strings: In-class Exercises",
    "section": "Warm-up",
    "text": "Warm-up\n\nDescribe what EACH of the str_ functions below does. Then, create a new variable “month” which is the two digit month from album_release_date\n\n\nspot_new &lt;- spot_smaller |&gt;\n  select(title, album_release_date) |&gt;\n  mutate(title_length = str_length(title),\n         year = str_sub(album_release_date, 1, 4),\n         title_lower = str_to_lower(title),\n         album_release_date2 = str_replace_all(album_release_date, \"-\", \"/\"))\nspot_new\n\n# A tibble: 10 × 6\n   title   album_release_date title_length year  title_lower album_release_date2\n   &lt;chr&gt;   &lt;chr&gt;                     &lt;int&gt; &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;              \n 1 Hear M… 2016-01-01                   11 2016  hear me now 2016/01/01         \n 2 Run th… 2011-06-24                   21 2011  run the wo… 2011/06/24         \n 3 Format… 2016-04-23                    9 2016  formation   2016/04/23         \n 4 7/11    2014-11-24                    4 2014  7/11        2014/11/24         \n 5 My Oh … 2019-12-06                   23 2019  my oh my (… 2019/12/06         \n 6 It's A… 2013-11-28                   14 2013  it's autom… 2013/11/28         \n 7 Poetic… 2012                         14 2012  poetic jus… 2012               \n 8 A.D.H.D 2011-07-02                    7 2011  a.d.h.d     2011/07/02         \n 9 Ya Est… 1990-01-01                    9 1990  ya estuvo   1990/01/01         \n10 Runnin… 2018-11-16                   49 2018  runnin (wi… 2018/11/16         \n\nmax_length &lt;- max(spot_new$title_length)\n\nstr_c(\"The longest title is\", max_length, \"characters long.\", sep = \" \")\n\n[1] \"The longest title is 49 characters long.\""
  },
  {
    "objectID": "09_strings_part2.html#important-functions-for-identifying-strings-which-match",
    "href": "09_strings_part2.html#important-functions-for-identifying-strings-which-match",
    "title": "Strings: In-class Exercises",
    "section": "Important functions for identifying strings which match",
    "text": "Important functions for identifying strings which match\nstr_view() : most useful for testing str_subset() : useful for printing matches to the console str_detect() : useful when working within a tibble\n\nIdentify the input type and output type for each of these examples:\n\n\nstr_view(spot_smaller$subgenre, \"pop\")\n\n[1] │ indie &lt;pop&gt;timism\n[2] │ post-teen &lt;pop&gt;\n[3] │ hip &lt;pop&gt;\n[4] │ hip &lt;pop&gt;\n[5] │ latin &lt;pop&gt;\n\nstr_view(spot_smaller$subgenre, \"pop\", match = NA)\n\n [1] │ indie &lt;pop&gt;timism\n [2] │ post-teen &lt;pop&gt;\n [3] │ hip &lt;pop&gt;\n [4] │ hip &lt;pop&gt;\n [5] │ latin &lt;pop&gt;\n [6] │ latin hip hop\n [7] │ hip hop\n [8] │ southern hip hop\n [9] │ latin hip hop\n[10] │ gangster rap\n\nstr_view(spot_smaller$subgenre, \"pop\", html = TRUE)\n\n\n\n\nstr_subset(spot_smaller$subgenre, \"pop\")\n\n[1] \"indie poptimism\" \"post-teen pop\"   \"hip pop\"         \"hip pop\"        \n[5] \"latin pop\"      \n\nstr_detect(spot_smaller$subgenre, \"pop\")\n\n [1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n\n\n\nUse str_detect to print the rows of the spot_smaller tibble containing songs that have “pop” in the subgenre. (i.e. make a new tibble with fewer rows)\nFind the mean song title length for songs with “pop” in the subgenre and songs without “pop” in the subgenre.\n\nProducing a table like this would be great:"
  },
  {
    "objectID": "09_strings_part2.html#matching-patterns-with-regular-expressions",
    "href": "09_strings_part2.html#matching-patterns-with-regular-expressions",
    "title": "Strings: In-class Exercises",
    "section": "Matching patterns with regular expressions",
    "text": "Matching patterns with regular expressions\n^abc string starts with abc abc$ string ends with abc . any character [abc] a or b or c [^abc] anything EXCEPT a or b or c\n\n# Guess the output!\n\nstr_view(spot_smaller$artist, \"^K\")\n\n[7] │ &lt;K&gt;endrick Lamar\n[8] │ &lt;K&gt;endrick Lamar\n[9] │ &lt;K&gt;id Frost\n\nstr_view(spot_smaller$album_release_date, \"01$\")\n\n[1] │ 2016-01-&lt;01&gt;\n[9] │ 1990-01-&lt;01&gt;\n\nstr_view(spot_smaller$title, \"^.. \")\n\n[5] │ &lt;My &gt;Oh My (feat. DaBaby)\n[9] │ &lt;Ya &gt;Estuvo\n\nstr_view(spot_smaller$artist, \"[^A-Za-z ]\")\n\n [2] │ Beyonc&lt;é&gt;\n [3] │ Beyonc&lt;é&gt;\n [4] │ Beyonc&lt;é&gt;\n[10] │ Mike WiLL Made&lt;-&gt;It\n\n\n\nGiven the corpus of common words in stringr::words, create regular expressions that find all words that:\n\n\nStart with “y”.\nEnd with “x”\nAre exactly three letters long.\nHave seven letters or more.\nStart with a vowel.\nEnd with ed, but not with eed.\nWords where q is not followed by u. (are there any in words?)\n\n\n# Try using str_view() or str_subset()\n\n# For example, to find words with \"tion\" at any point, I could use:\nstr_view(words, \"tion\")\n\n[181] │ condi&lt;tion&gt;\n[347] │ func&lt;tion&gt;\n[516] │ men&lt;tion&gt;\n[536] │ mo&lt;tion&gt;\n[543] │ na&lt;tion&gt;\n[631] │ posi&lt;tion&gt;\n[667] │ ques&lt;tion&gt;\n[695] │ rela&lt;tion&gt;\n[732] │ sec&lt;tion&gt;\n[804] │ sta&lt;tion&gt;\n\nstr_subset(words, \"tion\")\n\n [1] \"condition\" \"function\"  \"mention\"   \"motion\"    \"nation\"    \"position\" \n [7] \"question\"  \"relation\"  \"section\"   \"station\""
  },
  {
    "objectID": "09_strings_part2.html#more-useful-regular-expressions",
    "href": "09_strings_part2.html#more-useful-regular-expressions",
    "title": "Strings: In-class Exercises",
    "section": "More useful regular expressions:",
    "text": "More useful regular expressions:\n\\d - any number \\s - any space, tab, etc \\b - any boundary: space, ., etc.\n\nstr_view(spot_smaller$album_name, \"\\\\d\")\n\n[2] │ &lt;4&gt;\n[8] │ Section.&lt;8&gt;&lt;0&gt;\n\nstr_view(spot_smaller$album_name, \"\\\\s\")\n\n [1] │ Hear&lt; &gt;Me&lt; &gt;Now\n [4] │ BEYONCÉ&lt; &gt;[Platinum&lt; &gt;Edition]\n [6] │ It's&lt; &gt;Automatic\n [7] │ good&lt; &gt;kid,&lt; &gt;m.A.A.d&lt; &gt;city&lt; &gt;(Deluxe)\n [9] │ Hispanic&lt; &gt;Causing&lt; &gt;Panic\n[10] │ Creed&lt; &gt;II:&lt; &gt;The&lt; &gt;Album\n\nstr_view_all(spot_smaller$album_name, \"\\\\b\")\n\nWarning: `str_view_all()` was deprecated in stringr 1.5.0.\nℹ Please use `str_view()` instead.\n\n\n [1] │ &lt;&gt;Hear&lt;&gt; &lt;&gt;Me&lt;&gt; &lt;&gt;Now&lt;&gt;\n [2] │ &lt;&gt;4&lt;&gt;\n [3] │ &lt;&gt;Lemonade&lt;&gt;\n [4] │ &lt;&gt;BEYONCÉ&lt;&gt; [&lt;&gt;Platinum&lt;&gt; &lt;&gt;Edition&lt;&gt;]\n [5] │ &lt;&gt;Romance&lt;&gt;\n [6] │ &lt;&gt;It&lt;&gt;'&lt;&gt;s&lt;&gt; &lt;&gt;Automatic&lt;&gt;\n [7] │ &lt;&gt;good&lt;&gt; &lt;&gt;kid&lt;&gt;, &lt;&gt;m&lt;&gt;.&lt;&gt;A&lt;&gt;.&lt;&gt;A&lt;&gt;.&lt;&gt;d&lt;&gt; &lt;&gt;city&lt;&gt; (&lt;&gt;Deluxe&lt;&gt;)\n [8] │ &lt;&gt;Section&lt;&gt;.&lt;&gt;80&lt;&gt;\n [9] │ &lt;&gt;Hispanic&lt;&gt; &lt;&gt;Causing&lt;&gt; &lt;&gt;Panic&lt;&gt;\n[10] │ &lt;&gt;Creed&lt;&gt; &lt;&gt;II&lt;&gt;: &lt;&gt;The&lt;&gt; &lt;&gt;Album&lt;&gt;\n\n\nHere are the regular expression special characters that require an escape character (a preceding  ):  ^ $ . ? * | + ( ) [ {\nFor any characters with special properties, use  to “escape” its special meaning … but  is itself a special character … so we need two \\! (e.g. \\$, \\., etc.)\n\nstr_view(spot_smaller$title, \"$\")\n\n [1] │ Hear Me Now&lt;&gt;\n [2] │ Run the World (Girls)&lt;&gt;\n [3] │ Formation&lt;&gt;\n [4] │ 7/11&lt;&gt;\n [5] │ My Oh My (feat. DaBaby)&lt;&gt;\n [6] │ It's Automatic&lt;&gt;\n [7] │ Poetic Justice&lt;&gt;\n [8] │ A.D.H.D&lt;&gt;\n [9] │ Ya Estuvo&lt;&gt;\n[10] │ Runnin (with A$AP Rocky, A$AP Ferg & Nicki Minaj)&lt;&gt;\n\nstr_view(spot_smaller$title, \"\\\\$\")\n\n[10] │ Runnin (with A&lt;$&gt;AP Rocky, A&lt;$&gt;AP Ferg & Nicki Minaj)\n\n\n\nIn bigspotify, how many track_names include a $? Be sure you print the track_names you find and make sure the dollar sign is not just in a featured artist!\nIn bigspotify, how many track_names include a dollar amount (a $ followed by a number)."
  },
  {
    "objectID": "09_strings_part2.html#repetition",
    "href": "09_strings_part2.html#repetition",
    "title": "Strings: In-class Exercises",
    "section": "Repetition",
    "text": "Repetition\n? 0 or 1 times + 1 or more * 0 or more {n} exactly n times {n,} n or more times {,m} at most m times {n,m} between n and m times\n\nstr_view(spot_smaller$album_name, \"[A-Z]{2,}\")\n\n [4] │ &lt;BEYONC&gt;É [Platinum Edition]\n[10] │ Creed &lt;II&gt;: The Album\n\nstr_view(spot_smaller$album_release_date, \"\\\\d{4}-\\\\d{2}\")\n\n [1] │ &lt;2016-01&gt;-01\n [2] │ &lt;2011-06&gt;-24\n [3] │ &lt;2016-04&gt;-23\n [4] │ &lt;2014-11&gt;-24\n [5] │ &lt;2019-12&gt;-06\n [6] │ &lt;2013-11&gt;-28\n [8] │ &lt;2011-07&gt;-02\n [9] │ &lt;1990-01&gt;-01\n[10] │ &lt;2018-11&gt;-16\n\n\nUse at least 1 repetition symbol when solving 8-10 below\n\nModify the first regular expression above to also pick up “m.A.A.d” (in addition to “BEYONC” and “II”). That is, pick up strings where there might be a period between capital letters.\nCreate some strings that satisfy these regular expressions and explain.\n\n\n“^.*$”\n“\\{.+\\}”\n\n\nCreate regular expressions to find all stringr::words that:\n\n\nStart with three consonants.\nHave two or more vowel-consonant pairs in a row."
  },
  {
    "objectID": "09_strings_part2.html#useful-functions-for-handling-patterns",
    "href": "09_strings_part2.html#useful-functions-for-handling-patterns",
    "title": "Strings: In-class Exercises",
    "section": "Useful functions for handling patterns",
    "text": "Useful functions for handling patterns\nstr_extract() : extract a string that matches a pattern str_count() : count how many times a pattern occurs within a string\n\nstr_extract(spot_smaller$album_release_date, \"\\\\d{4}-\\\\d{2}\")\n\n [1] \"2016-01\" \"2011-06\" \"2016-04\" \"2014-11\" \"2019-12\" \"2013-11\" NA       \n [8] \"2011-07\" \"1990-01\" \"2018-11\"\n\nspot_smaller |&gt;\n  select(album_release_date) |&gt;\n  mutate(year_month = str_extract(album_release_date, \"\\\\d{4}-\\\\d{2}\"))\n\n# A tibble: 10 × 2\n   album_release_date year_month\n   &lt;chr&gt;              &lt;chr&gt;     \n 1 2016-01-01         2016-01   \n 2 2011-06-24         2011-06   \n 3 2016-04-23         2016-04   \n 4 2014-11-24         2014-11   \n 5 2019-12-06         2019-12   \n 6 2013-11-28         2013-11   \n 7 2012               &lt;NA&gt;      \n 8 2011-07-02         2011-07   \n 9 1990-01-01         1990-01   \n10 2018-11-16         2018-11   \n\nspot_smaller |&gt;\n  select(artist) |&gt;\n  mutate(n_vowels = str_count(artist, \"[aeiou]\"))\n\n# A tibble: 10 × 2\n   artist            n_vowels\n   &lt;chr&gt;                &lt;int&gt;\n 1 Alok                     1\n 2 Beyoncé                  2\n 3 Beyoncé                  2\n 4 Beyoncé                  2\n 5 Camila Cabello           6\n 6 Freestyle                3\n 7 Kendrick Lamar           4\n 8 Kendrick Lamar           4\n 9 Kid Frost                2\n10 Mike WiLL Made-It        5\n\n\n\nIn the spot_smaller dataset, how many words are in each title? (hint \\b)\nIn the spot_smaller dataset, extract the first word from every title. Show how you would print out these words as a vector and how you would create a new column on the spot_smaller tibble. That is, produce this:\n\n\n# [1] \"Hear\"      \"Run\"       \"Formation\" \"7/11\"      \"My\"        \"It's\"     \n# [7] \"Poetic\"    \"A.D.H.D\"   \"Ya\"        \"Runnin\"   \n\nThen this:\n\n# A tibble: 10 × 2\n#   title                                             first_word\n#   &lt;chr&gt;                                             &lt;chr&gt;     \n# 1 Hear Me Now                                       Hear      \n# 2 Run the World (Girls)                             Run       \n# 3 Formation                                         Formation \n# 4 7/11                                              7/11      \n# 5 My Oh My (feat. DaBaby)                           My        \n# 6 It's Automatic                                    It's      \n# 7 Poetic Justice                                    Poetic    \n# 8 A.D.H.D                                           A.D.H.D   \n# 9 Ya Estuvo                                         Ya        \n#10 Runnin (with A$AP Rocky, A$AP Ferg & Nicki Minaj) Runnin    \n\n\nWhich decades are popular for playlist_names? Using the bigspotify dataset, try doing each of these steps one at a time!\n\n\nfilter the bigspotify dataset to only include playlists that include something like “80’s” or “00’s” in their title.\ncreate a new column that extracts the decade\nuse count to find how many playlists include each decade\nwhat if you include both “80’s” and “80s”?\nhow can you count “80’s” and “80s” together in your final tibble?"
  },
  {
    "objectID": "09_strings_part2.html#grouping-and-backreferences",
    "href": "09_strings_part2.html#grouping-and-backreferences",
    "title": "Strings: In-class Exercises",
    "section": "Grouping and backreferences",
    "text": "Grouping and backreferences\n\n# find all fruits with repeated pair of letters.  \nfruit = stringr::fruit\nfruit\n\n [1] \"apple\"             \"apricot\"           \"avocado\"          \n [4] \"banana\"            \"bell pepper\"       \"bilberry\"         \n [7] \"blackberry\"        \"blackcurrant\"      \"blood orange\"     \n[10] \"blueberry\"         \"boysenberry\"       \"breadfruit\"       \n[13] \"canary melon\"      \"cantaloupe\"        \"cherimoya\"        \n[16] \"cherry\"            \"chili pepper\"      \"clementine\"       \n[19] \"cloudberry\"        \"coconut\"           \"cranberry\"        \n[22] \"cucumber\"          \"currant\"           \"damson\"           \n[25] \"date\"              \"dragonfruit\"       \"durian\"           \n[28] \"eggplant\"          \"elderberry\"        \"feijoa\"           \n[31] \"fig\"               \"goji berry\"        \"gooseberry\"       \n[34] \"grape\"             \"grapefruit\"        \"guava\"            \n[37] \"honeydew\"          \"huckleberry\"       \"jackfruit\"        \n[40] \"jambul\"            \"jujube\"            \"kiwi fruit\"       \n[43] \"kumquat\"           \"lemon\"             \"lime\"             \n[46] \"loquat\"            \"lychee\"            \"mandarine\"        \n[49] \"mango\"             \"mulberry\"          \"nectarine\"        \n[52] \"nut\"               \"olive\"             \"orange\"           \n[55] \"pamelo\"            \"papaya\"            \"passionfruit\"     \n[58] \"peach\"             \"pear\"              \"persimmon\"        \n[61] \"physalis\"          \"pineapple\"         \"plum\"             \n[64] \"pomegranate\"       \"pomelo\"            \"purple mangosteen\"\n[67] \"quince\"            \"raisin\"            \"rambutan\"         \n[70] \"raspberry\"         \"redcurrant\"        \"rock melon\"       \n[73] \"salal berry\"       \"satsuma\"           \"star fruit\"       \n[76] \"strawberry\"        \"tamarillo\"         \"tangerine\"        \n[79] \"ugli fruit\"        \"watermelon\"       \n\nstr_view(fruit, \"(..)\\\\1\", match = TRUE)\n\n [4] │ b&lt;anan&gt;a\n[20] │ &lt;coco&gt;nut\n[22] │ &lt;cucu&gt;mber\n[41] │ &lt;juju&gt;be\n[56] │ &lt;papa&gt;ya\n[73] │ s&lt;alal&gt; berry\n\n# why does the code below add \"pepper\" and even \"nectarine\"?\nstr_view(fruit, \"(..)(.*)\\\\1\", match = TRUE)\n\n [4] │ b&lt;anan&gt;a\n [5] │ bell &lt;peppe&gt;r\n[17] │ chili &lt;peppe&gt;r\n[20] │ &lt;coco&gt;nut\n[22] │ &lt;cucu&gt;mber\n[29] │ eld&lt;erber&gt;ry\n[41] │ &lt;juju&gt;be\n[51] │ &lt;nectarine&gt;\n[56] │ &lt;papa&gt;ya\n[73] │ s&lt;alal&gt; berry\n\n\nTips with backreference: - You must use () around the the thing you want to reference. - To backreference multiple times, use \\1 again. - The number refers to which spot you are referencing… e.g. \\2 references the second set of ()\n\nx1 &lt;- c(\"abxyba\", \"abccba\", \"xyaayx\", \"abxyab\", \"abcabc\")\nstr_view(x1, \"(.)(.)(..)\\\\2\\\\1\")\n\n[1] │ &lt;abxyba&gt;\n[2] │ &lt;abccba&gt;\n[3] │ &lt;xyaayx&gt;\n\nstr_view(x1, \"(.)(.)(..)\\\\1\\\\2\")\n\n[4] │ &lt;abxyab&gt;\n\nstr_view(x1, \"(.)(.)(.)\\\\1\\\\2\\\\3\")\n\n[5] │ &lt;abcabc&gt;\n\n\n\nDescribe to your groupmates what these expressions will match, and provide a word or expression as an example:\n\n\n(.)\\1\\1\n“(.)(.)(.).*\\3\\2\\1”\n\nWhich words in stringr::words match each expression?\n\nConstruct a regular expression to match words in stringr::words that contain a repeated pair of letters (e.g. “church” contains “ch” repeated twice) but not match repeated pairs of numbers (e.g. 507-786-3861).\nReformat the album_release_date variable in spot_smaller so that it is MM-DD-YYYY instead of YYYY-MM-DD. (Hint: str_replace().)\nBEFORE RUNNING IT, explain to your partner(s) what the following R chunk will do:\n\n\nsentences %&gt;% \n  str_replace(\"([^ ]+) ([^ ]+) ([^ ]+)\", \"\\\\1 \\\\3 \\\\2\") %&gt;% \n  head(5)\n\n[1] \"The canoe birch slid on the smooth planks.\" \n[2] \"Glue sheet the to the dark blue background.\"\n[3] \"It's to easy tell the depth of a well.\"     \n[4] \"These a days chicken leg is a rare dish.\"   \n[5] \"Rice often is served in round bowls.\""
  },
  {
    "objectID": "07_simulation.html",
    "href": "07_simulation.html",
    "title": "Simulation",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\nThis leans on parts of MDSR Chapter 13: Simulation.\n# Initial packages required\nlibrary(tidyverse)"
  },
  {
    "objectID": "07_simulation.html#simulating-behavior-under-a-null-hypothesis",
    "href": "07_simulation.html#simulating-behavior-under-a-null-hypothesis",
    "title": "Simulation",
    "section": "Simulating behavior under a null hypothesis",
    "text": "Simulating behavior under a null hypothesis\nWhile MDSR Section 13.2 has a cool example involving cancer genomics, we will consider a simpler example here.\nThis applet contains data from a 2005 study on the use of dolphin-facilitated therapy on the treatment of depression. In that study, 10 of the 15 subjects (67%) assigned to dolphin therapy showed improvement, compared to only 3 of the 15 subjects (20%) assigned to the control group. But with such small sample sizes, is this significant evidence that the dolphin group had greater improvement of their depressive symptoms? To answer that question, we can use simulation to conduct a randomization test.\nWe will simulate behavior in the “null world” where there is no real effect of treatment. In that case, the 13 total improvers would have improved no matter the treatment assigned, and the 17 total non-improvers would have not improved no matter the treatment assigned. So in the “null world”, treatment is a meaningless label that can be just as easily shuffled among subjects without any effect. In that world, the fact we observed a 47 percentage point difference in success rates (67 - 20) was just random luck. But we should ask: how often would we expect a difference as large as 47% by chance, assuming we’re living in the null world where there is no effect of treatment?\nYou could think about simulating this situation with the following steps:\n\nwrite code to calculate the difference in success rates in the observed data\nwrite a loop to calculate the differences in success rates from 1000 simulated data sets from the null world. Store those 1000 simulated differences\ncalculate how often we found a difference in the null world as large as that found in the observed data. In statistics, when this probability is below .05, we typically reject the null world, and conclude that there is likely a real difference between the two groups (i.e. a “statistically significant” difference)\n\n\nOn Your Own\n\nFollow the steps above to conduct a randomization test for the difference in two proportions. In addition to finding your probability in (3), plot the 1000 simulated differences and indicate where the observed difference of .47 falls in that “null distribution”.\n\nThe code below generates a tibble with our observed data. Notice how the sample() function can be used to shuffle the treatments among the 30 subjects.\n\ndolphin_data &lt;- tibble(treatment = rep(c(\"Dolphin\", \"Control\"), each = 15),\n                       improve = c(rep(\"Yes\", 10), rep(\"No\", 5), \n                                   rep(\"Yes\", 3), rep(\"No\", 12)))\nprint(dolphin_data, n = Inf)\n\n# A tibble: 30 × 2\n   treatment improve\n   &lt;chr&gt;     &lt;chr&gt;  \n 1 Dolphin   Yes    \n 2 Dolphin   Yes    \n 3 Dolphin   Yes    \n 4 Dolphin   Yes    \n 5 Dolphin   Yes    \n 6 Dolphin   Yes    \n 7 Dolphin   Yes    \n 8 Dolphin   Yes    \n 9 Dolphin   Yes    \n10 Dolphin   Yes    \n11 Dolphin   No     \n12 Dolphin   No     \n13 Dolphin   No     \n14 Dolphin   No     \n15 Dolphin   No     \n16 Control   Yes    \n17 Control   Yes    \n18 Control   Yes    \n19 Control   No     \n20 Control   No     \n21 Control   No     \n22 Control   No     \n23 Control   No     \n24 Control   No     \n25 Control   No     \n26 Control   No     \n27 Control   No     \n28 Control   No     \n29 Control   No     \n30 Control   No     \n\nsample(dolphin_data$treatment)\n\n [1] \"Dolphin\" \"Dolphin\" \"Control\" \"Dolphin\" \"Dolphin\" \"Dolphin\" \"Control\"\n [8] \"Dolphin\" \"Dolphin\" \"Control\" \"Control\" \"Control\" \"Control\" \"Control\"\n[15] \"Dolphin\" \"Dolphin\" \"Control\" \"Dolphin\" \"Dolphin\" \"Control\" \"Dolphin\"\n[22] \"Control\" \"Control\" \"Control\" \"Dolphin\" \"Dolphin\" \"Control\" \"Control\"\n[29] \"Control\" \"Dolphin\""
  },
  {
    "objectID": "07_simulation.html#evaluate-conditions-for-a-statistical-test",
    "href": "07_simulation.html#evaluate-conditions-for-a-statistical-test",
    "title": "Simulation",
    "section": "Evaluate conditions for a statistical test",
    "text": "Evaluate conditions for a statistical test\nMost statistical tests rely on underlying mathematical conditions so that p-values, confidence intervals, and prediction intervals are valid. It’s important to know how robust tests are to these conditions - will they still perform well even with small or medium sized violations of the conditions?\nHere we will focus on the two-sample t-test, which tests whether or not two independent groups have a significant difference in their means. The key mathematical conditions for this test are:\n\nindependence between and within groups\ndata is normally distributed within each group\ndata has equal spread within each group (this condition is relaxed when we use the Welch approximation)\n\nAnd, how important these conditions are can further depend on:\n\nactual and relative sample sizes within each group\nactual and relative spreads within each group\nactual and relative shape of data within each group\n\nLet’s set up a simulation to generate responses from two independent groups, and then we’ll see how certain conditions might affect our ability to test mean differences between those groups.\nWe will need to use randomizing functions: functions in R that generate random values from specific probability distributions. We will always have to specify parameters for the distribution, since each distribution family can take on different shapes depending on the parameter settings. For instance, normal distributions are defined by their mean and standard deviation.\n\n# rnorm() produces random values from a normal distribution\nnormal_data &lt;- tibble(x = rnorm(1000, mean = 100, sd = 20))\nnormal_data |&gt;\n  summarise(mean = mean(x),\n            sd = sd(x))\n\n# A tibble: 1 × 2\n   mean    sd\n  &lt;dbl&gt; &lt;dbl&gt;\n1  99.8  20.2\n\nggplot(normal_data, aes(x = x)) +\n  geom_density()\n\n\n\n# rexp() produces random values from an exponential distribution\nexp_data &lt;- tibble(x = rexp(1000, rate = .1))    # mean = sd = 1 / rate\nexp_data |&gt;\n  summarise(mean = mean(x),\n            sd = sd(x))\n\n# A tibble: 1 × 2\n   mean    sd\n  &lt;dbl&gt; &lt;dbl&gt;\n1  9.94  10.5\n\nggplot(exp_data, aes(x = x)) +\n  geom_density()\n\n\n\n# help(\"Distributions\") for other shapes\n\nHere’s a simulation to examine the coverage of 95% confidence intervals. When we find a 95% confidence interval for the true difference between two means, mathematical theory tells us that our interval should be “correct” 95% of the time. That is, in 95% of repeated applications (or samplings), the reported confidence intervals should contain the true mean difference from the entire population of interest. That’s pretty powerful stuff! But if conditions are violated, are we still justified in declaring 95% confidence in our results?\nOften we set var.equal = FALSE to use the Welch approximation which doesn’t require equal spread in the two groups (that’s actually the default in t.test()) but mathematical theory (and extensions to the ANOVA F-test) actually is based on equal variances.\n\n# initial settings\nmean1 &lt;- 40\nmean2 &lt;- 40\ntruediff &lt;- mean1 - mean2\nsd2 &lt;- 10\nsd_ratio &lt;- 1\nsd1 &lt;- sd_ratio * sd2\nn1 &lt;- 10\nn2 &lt;- 100\nnumsims &lt;- 1000\n\n# Try one sampling to see if 95% CI contains the truediff\nsamp1 &lt;- rnorm(n1, mean1, sd1)\nsamp2 &lt;- rnorm(n2, mean2, sd2)\nresult &lt;- t.test(x = samp1, y = samp2, var.equal = TRUE)\nlower &lt;- result$conf.int[1]\nlower\n\n[1] -11.93131\n\nupper &lt;- result$conf.int[2]\nupper\n\n[1] 0.9941391\n\ncontains_truediff &lt;- (lower &lt;= truediff & upper &gt;= truediff)\ncontains_truediff\n\n[1] TRUE\n\n# Plot the sampling above\nsim_data &lt;- tibble(response = c(samp1, samp2), \n       group = c(rep(\"Group 1\", n1), rep(\"Group 2\", n2)))\nmosaic::favstats(response ~ group, data = sim_data)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\n\n    group      min       Q1   median       Q3      max     mean        sd   n\n1 Group 1 26.50248 29.33180 34.69331 39.39470 49.96450 35.16614  7.321545  10\n2 Group 2 18.69936 34.88092 40.68079 46.89929 65.68006 40.63473 10.027553 100\n  missing\n1       0\n2       0\n\nggplot(sim_data, aes(x = response, y = group)) +\n  geom_boxplot()\n\n\n\n# Now repeat 1000 times!\ncontains_truediff &lt;- vector(\"logical\", numsims)\nfor (i in 1:numsims) {\n  samp1 &lt;- rnorm(n1, mean1, sd1)\n  samp2 &lt;- rnorm(n2, mean2, sd2)\n  result &lt;- t.test(x = samp1, y = samp2, var.equal = TRUE)\n  lower &lt;- result$conf.int[1]\n  upper &lt;- result$conf.int[2]\n  contains_truediff[i] &lt;- (lower &lt;= truediff & upper &gt;= truediff)\n}\ncoverage &lt;- mean(contains_truediff)\ncoverage\n\n[1] 0.942\n\n\n\nOn Your Own\n\nUse the simulation above to plot many different values of sd_ratio vs confidence interval coverage.\nAdapt the simulation above to examine confidence interval width rather than confidence interval coverage.\nExamine the effect of non-normal data. Does the effect change depending on the relative sample sizes in each group? Note that, if you use a distribution like rgamma() or rexp() the parameters are different, but we still want the mean to equal that in the normal distribution."
  },
  {
    "objectID": "07_simulation.html#find-the-power-of-a-statistical-test",
    "href": "07_simulation.html#find-the-power-of-a-statistical-test",
    "title": "Simulation",
    "section": "Find the power of a statistical test",
    "text": "Find the power of a statistical test\nThe power of a statistical test is the probability that it rejects the null hypothesis when the null hypothesis is false. In other words, it’s the probability that a statistical test can detect when a true difference exists. The power depends on a number of factors, including:\n\nsample size\ntype I error level\nvariability in the data\nsize of the true difference\n\nThe following steps can be followed to simulate a power calculation\n\nsimulate data where is a true difference or effect\nrun your desired test on the simulated data and record if the null hypothesis was rejected or not (i.e. if the p-value was below .05)\nrepeat a large number of times and record the total proportion of times that the null hypothesis was rejected - that is the power of the test under those conditions\noften we will then repeat (1)-(3) under different conditions - different sizes of the true effect, different amount of variability, different sample sizes, etc.\n\n\nOn Your Own\n\nHere is a real email I received from a real research physician. My response was formed after running several simulations…\n\n\nDo you mind if I ask for some statistical guidance? This is probably a pretty straightforward question but I’m struggling with it a bit.\n\n\nOne of our docs wants to conduct a study looking at a biomarker called PARP. The hypothesis is that expression of PARP is correlated with decreased survival in breast cancer. The test will be done on archived tissue and will be read as either positive or negative. The hypothesis is that patients who overexpress PARP will have a response rate to chemotherapy of 10% and that non-expressers will have a response rate of 25%.\n\n\nThe proposed patient population is 220 patients. This is the power analysis:\n\n\nSample Size for Response Rate (2-sample comparison, 1-sided, alpha=0.05)\n\n\nPower = 80\n\n\nNull Hypothesis (H0) = .05 (Response Rate for Overexpression Group)\n\n\nAlternative Hypothesis (H1) = .25 (Response Rate for Normal Expression Group)\n\n\nSample Size per Group (STPlan) = 35\n\n\nSample Size per Group (nQuery) = 39\n\n\nPower = 85\n\n\nNull Hypothesis (H0) = .05 (Response Rate for Overexpression Group)\n\n\nAlternative Hypothesis (H1) = .25 (Response Rate for Normal Expression Group)\n\n\nSample Size per Group (STPlan) = 41\n\n\nSample Size per Group (nQuery) = 45\n\n\nPower = 80\n\n\nNull Hypothesis (H0) = .10 (Response Rate for Overexpression Group)\n\n\nAlternative Hypothesis (H1) = .25 (Response Rate for Normal Expression Group)\n\n\nSample Size per Group (STPlan) = 76\n\n\nSample Size per Group (nQuery) = 79\n\n\netc. (Many more runs of their power software with different levels of power and different assumptions about HO and H1)\n\n\nNow the unknown variable is what proportion of tumors will express PARP. From looking at this it appears that he is assuming that the sample size will be equal in each group but in fact, it appears that PARP expression may be present in 80-90% of tumors. Let’s assume that 80% of tumors are expressers and 20% are non-expressers. I’m not crazy in assuming that will have a significant impact on our needed study size to show a p value of 0.05 at 80% power with the above assumptions, am I?\n\nProvide a plot (ideally) or table to answer this physician’s question. Your plot should be based on several simulated power calculations under different assumptions about (a) the percentage of PARP expressers, and (b) the true response rates in both groups.\nNote that rbinom(1, size = N, prob = P) will simulate the number of responders in a group with N subjects who each have probability P of responding."
  },
  {
    "objectID": "05_data_types.html",
    "href": "05_data_types.html",
    "title": "Data types",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\nThis leans on parts of R4DS Chapter 27: A field guide to base R, in addition to parts of the first edition of R4DS.\n# Initial packages required\nlibrary(tidyverse)"
  },
  {
    "objectID": "05_data_types.html#what-is-a-vector",
    "href": "05_data_types.html#what-is-a-vector",
    "title": "Data types",
    "section": "What is a vector?",
    "text": "What is a vector?\nWe’ve seen them:\n\n1:5 \n\n[1] 1 2 3 4 5\n\nc(3, 6, 1, 7)\n\n[1] 3 6 1 7\n\nc(\"a\", \"b\", \"c\")\n\n[1] \"a\" \"b\" \"c\"\n\nx &lt;- c(0:3, NA)\nis.na(x)\n\n[1] FALSE FALSE FALSE FALSE  TRUE\n\nsqrt(x)\n\n[1] 0.000000 1.000000 1.414214 1.732051       NA\n\n\nThis doesn’t really fit the mathematical definition of a vector (direction and magnitude)… its really just some numbers (or letters, or TRUE’s…) strung together."
  },
  {
    "objectID": "05_data_types.html#types-of-vectors",
    "href": "05_data_types.html#types-of-vectors",
    "title": "Data types",
    "section": "Types of vectors",
    "text": "Types of vectors\nAtomic vectors are homogeneous… they can contain only one “type”. Types include logical, integer, double, and character (Also complex and raw, but we will ignore those).\nLists can be heterogeneous…. they can be made up of vectors of different types, or even of other lists!\nNULL denotes the absence of a vector (whereas NA denotes absence of a value in a vector).\nLet’s check out some vector types:\n\nx &lt;- c(0:3, NA)\ntypeof(x)\n\n[1] \"integer\"\n\nsqrt(x)\n\n[1] 0.000000 1.000000 1.414214 1.732051       NA\n\ntypeof(sqrt(x))\n\n[1] \"double\"\n\n\n[Pause to Ponder:] State the types of the following vectors, then use typeof() to check:\n\nis.na(x)\n\n[1] FALSE FALSE FALSE FALSE  TRUE\n\nx &gt; 2\n\n[1] FALSE FALSE FALSE  TRUE    NA\n\nc(\"apple\", \"banana\", \"pear\")\n\n[1] \"apple\"  \"banana\" \"pear\"  \n\n\nA logical vector can be implicitly coerced to numeric - T to 1 and F to 0\n\nx &lt;- sample(1:20, 100, replace = TRUE)\ny &lt;- x &gt; 10\nis_logical(y)\n\n[1] TRUE\n\nas.numeric(y)\n\n  [1] 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1\n [38] 1 1 1 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0\n [75] 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 0 0\n\nsum(y)  # how many are greater than 10?\n\n[1] 43\n\nmean(y) # what proportion are greater than 10?\n\n[1] 0.43\n\n\nIf there are multiple data types in a vector, then the most complex type wins, because you cannot mix types in a vector (although you can in a list)\n\ntypeof(c(TRUE, 1L))\n\n[1] \"integer\"\n\ntypeof(c(1L, 1.5))\n\n[1] \"double\"\n\ntypeof(c(1.5, \"a\"))\n\n[1] \"character\"\n\n\nIntegers are whole numbers. “double” refers to “Double-precision” representation of fractional values… don’t worry about the details here (Google it if you care), but just recognize that computers have to round at some point. “Double-precision” tries to store numbers precisely and efficiently.\nBut weird stuff can happen:\n\ny &lt;- sqrt(2) ^2\ny\n\n[1] 2\n\ny == 2\n\n[1] FALSE\n\n\nthe function near is better here:\n\nnear(y, 2)\n\n[1] TRUE\n\n\nAnd doubles have a couple extra possible values: Inf, -Inf, and NaN, in addition to NA:\n\n1/0\n\n[1] Inf\n\n-1/0\n\n[1] -Inf\n\n0/0\n\n[1] NaN\n\nInf*0\n\n[1] NaN\n\nInf/Inf\n\n[1] NaN\n\nInf/NA\n\n[1] NA\n\nInf*NA\n\n[1] NA\n\n\nIt’s not a good idea to check for special values (NA, NaN, Inf, -Inf) with ==. Use these instead:\n\nis.finite(Inf)\n\n[1] FALSE\n\nis.infinite(Inf)\n\n[1] TRUE\n\nis.finite(NA)\n\n[1] FALSE\n\nis.finite(NaN)\n\n[1] FALSE\n\nis.infinite(NA)\n\n[1] FALSE\n\nis.infinite(NaN)\n\n[1] FALSE\n\nis.na(NA)\n\n[1] TRUE\n\nis.na(NaN)\n\n[1] TRUE\n\nis.nan(NA)\n\n[1] FALSE\n\nis.nan(NaN)\n\n[1] TRUE\n\nis.na(Inf)\n\n[1] FALSE\n\nis.nan(Inf)\n\n[1] FALSE\n\n\nWhy not use == ?\n\n# Sometimes it works how you think it would:\n1/0\n\n[1] Inf\n\n1/0 == Inf\n\n[1] TRUE\n\n# Sometimes it doesn't (Because NA is contagious!)\n0/0\n\n[1] NaN\n\n0/0 == NaN    \n\n[1] NA\n\nNA == NA \n\n[1] NA\n\nx &lt;- c(0, 1, 1/0, 0/0)\n# Doesn't work well\nx == NA\n\n[1] NA NA NA NA\n\nx == Inf\n\n[1] FALSE FALSE  TRUE    NA\n\n# Works better\nis.na(x)\n\n[1] FALSE FALSE FALSE  TRUE\n\nis.infinite(x)\n\n[1] FALSE FALSE  TRUE FALSE\n\n\nAnother note: technically, each type of vector has its own type of NA… this usually doesn’t matter, but is good to know in case one day you get very very strange errors."
  },
  {
    "objectID": "05_data_types.html#augmented-vectors",
    "href": "05_data_types.html#augmented-vectors",
    "title": "Data types",
    "section": "Augmented vectors",
    "text": "Augmented vectors\nVectors may carry additional metadata in the form of attributes which create augmented vectors.\n\nFactors are built on top of integer vectors\nDates and date-times are built on top of numeric (either integer or double) vectors\nData frames and tibbles are built on top of lists"
  },
  {
    "objectID": "05_data_types.html#naming-items-in-vectors",
    "href": "05_data_types.html#naming-items-in-vectors",
    "title": "Data types",
    "section": "Naming items in vectors",
    "text": "Naming items in vectors\nEach element of a vector can be named, either when it is created or with setnames from package purrr.\n\nx &lt;- c(a = 1, b = 2, c = 3)\nx\n\na b c \n1 2 3 \n\n\nThis is more commonly used when you’re dealing with lists or tibbles (which are just a special kind of list!)\n\ntibble(x = 1:4, y = 5:8)\n\n# A tibble: 4 × 2\n      x     y\n  &lt;int&gt; &lt;int&gt;\n1     1     5\n2     2     6\n3     3     7\n4     4     8"
  },
  {
    "objectID": "05_data_types.html#subsetting-vectors",
    "href": "05_data_types.html#subsetting-vectors",
    "title": "Data types",
    "section": "Subsetting vectors",
    "text": "Subsetting vectors\nSo many ways to do this.\nI. Subset with numbers.\nUse positive integers to keep elements at those positions:\n\nx &lt;- c(\"one\", \"two\", \"three\", \"four\", \"five\")\nx[1]\n\n[1] \"one\"\n\nx[4]\n\n[1] \"four\"\n\nx[1:2]\n\n[1] \"one\" \"two\"\n\n\n[Pause to Ponder:] How would you extract values 1 and 3?\nYou can also repeat values:\n\nx[c(1, 1, 3, 3, 5, 5, 2, 2, 4, 4, 4)]\n\n [1] \"one\"   \"one\"   \"three\" \"three\" \"five\"  \"five\"  \"two\"   \"two\"   \"four\" \n[10] \"four\"  \"four\" \n\n\nUse negative integers to drop elements:\n\nx[-3]\n\n[1] \"one\"  \"two\"  \"four\" \"five\"\n\n\n[Pause to Ponder:] How would you drop values 2 and 4?\nWhat happens if you mix positive and negative values?\n\nx[c(1, -1)]\n\nError in x[c(1, -1)]: only 0's may be mixed with negative subscripts\n\n\nYou can just subset with 0… this isn’t usually helpful, except perhaps for testing weird cases when you write functions:\n\nx[0]\n\ncharacter(0)\n\n\n\nSubset with a logical vector (“Logical subsetting”).\n\n\nx == \"one\"\n\n[1]  TRUE FALSE FALSE FALSE FALSE\n\nx[x == \"one\"]\n\n[1] \"one\"\n\ny &lt;- c(10, 3, NA, 5, 8, 1, NA)\nis.na(y)\n\n[1] FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE\n\ny[!is.na(y)]\n\n[1] 10  3  5  8  1\n\n\n\nIf named, subset with a character vector.\n\n\nz &lt;- c(abc = 1, def = 2, xyz = 3)\nz[\"abc\"]\n\nabc \n  1 \n\n# A slightly more useful example:\nsummary(y)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n    1.0     3.0     5.0     5.4     8.0    10.0       2 \n\nsummary(y)[\"Min.\"]\n\nMin. \n   1 \n\n\n[Pause to Ponder:] Extract abc and xyz from the vector z, and then extract the mean from summary(y)\nNote: Using $ is just for lists (and tibbles, since tibbles are lists)! Not atomic vectors!\n\nz$abc\n\nError in z$abc: $ operator is invalid for atomic vectors\n\n\n\nBlank space. (Don’t subset).\n\n\nx\n\n[1] \"one\"   \"two\"   \"three\" \"four\"  \"five\" \n\nx[]\n\n[1] \"one\"   \"two\"   \"three\" \"four\"  \"five\" \n\n\nThis seems kind of silly. But blank is useful for higher-dimensional objects… like a matrix, or data frame. But our book doesn’t use matrices, so this may be the last one you see this semester:\n\nz &lt;- matrix(1:8, nrow= 2)\nz\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    3    5    7\n[2,]    2    4    6    8\n\nz[1, ]\n\n[1] 1 3 5 7\n\nz[, 1]\n\n[1] 1 2\n\nz[, -3]\n\n     [,1] [,2] [,3]\n[1,]    1    3    7\n[2,]    2    4    8\n\n\nWe could use this with tibbles too, but it is generally better to use the column names (more readable, and less likely to get the wrong columns by accident), and you should probably use select, filter, or slice:\n\nmpg[, 1:2]\n\n# A tibble: 234 × 2\n   manufacturer model     \n   &lt;chr&gt;        &lt;chr&gt;     \n 1 audi         a4        \n 2 audi         a4        \n 3 audi         a4        \n 4 audi         a4        \n 5 audi         a4        \n 6 audi         a4        \n 7 audi         a4        \n 8 audi         a4 quattro\n 9 audi         a4 quattro\n10 audi         a4 quattro\n# ℹ 224 more rows\n\nmpg[1:3, ]\n\n# A tibble: 3 × 11\n  manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class \n  &lt;chr&gt;        &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; \n1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa…\n2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa…\n3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa…"
  },
  {
    "objectID": "05_data_types.html#recycling",
    "href": "05_data_types.html#recycling",
    "title": "Data types",
    "section": "Recycling",
    "text": "Recycling\nWhat does R do with vectors:\n\n1:5 + 1:5\n\n[1]  2  4  6  8 10\n\n1:5 * 1:5\n\n[1]  1  4  9 16 25\n\n1:5 + 2\n\n[1] 3 4 5 6 7\n\n1:5 * 2\n\n[1]  2  4  6  8 10\n\n\nThis last two lines makes sense… but R is doing something important here, called recycling. In other words, it is really doing this:\n\n1:5 * c(2, 2, 2, 2, 2)\n\n[1]  2  4  6  8 10\n\n\nYou never need to do this explicit iteration! (This is different from some other more general purpose computing languages…. R was built for analyzing data, so this type of behavior is really desirable!)\nR can recycle longer vectors too, and only warns you if lengths are not multiples of each other:\n\n1:10 + 1:2\n\n [1]  2  4  4  6  6  8  8 10 10 12\n\n1:10 + 1:3\n\nWarning in 1:10 + 1:3: longer object length is not a multiple of shorter object\nlength\n\n\n [1]  2  4  6  5  7  9  8 10 12 11\n\n\nHowever, functions within the tidyverse will not allow you to recycle anything other than scalars (math word for single number… in R, a vector of length 1).\n\n#OK:\ntibble(x = 1:4, y = 1)\n\n# A tibble: 4 × 2\n      x     y\n  &lt;int&gt; &lt;dbl&gt;\n1     1     1\n2     2     1\n3     3     1\n4     4     1\n\n#not OK:\ntibble(x = 1:4, y = 1:2)\n\nError in `tibble()`:\n! Tibble columns must have compatible sizes.\n• Size 4: Existing data.\n• Size 2: Column `y`.\nℹ Only values of size one are recycled.\n\n\nTo intentionally recycle, use rep:\n\nrep(1:3, times = 2)\n\n[1] 1 2 3 1 2 3\n\nrep(1:3, each = 2)\n\n[1] 1 1 2 2 3 3"
  },
  {
    "objectID": "05_data_types.html#lists",
    "href": "05_data_types.html#lists",
    "title": "Data types",
    "section": "Lists",
    "text": "Lists\nLists can contain a mix of objects, even other lists.\nAs noted previously, tibbles are an augmented list. Augmented lists have additional attributes. For example, the names of the columns in a tibble.\nAnother list you may have encountered in a stats class is output from lm, linear regression:\n\nmpg_model &lt;- lm(hwy ~ cty, data = mpg)\n\nmpg_model\n\n\nCall:\nlm(formula = hwy ~ cty, data = mpg)\n\nCoefficients:\n(Intercept)          cty  \n      0.892        1.337  \n\ntypeof(mpg_model)\n\n[1] \"list\"\n\nstr(mpg_model)\n\nList of 12\n $ coefficients : Named num [1:2] 0.892 1.337\n  ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"cty\"\n $ residuals    : Named num [1:234] 4.0338 0.0214 3.3588 1.0214 3.7087 ...\n  ..- attr(*, \"names\")= chr [1:234] \"1\" \"2\" \"3\" \"4\" ...\n $ effects      : Named num [1:234] -358.566 -86.887 3.121 0.787 3.458 ...\n  ..- attr(*, \"names\")= chr [1:234] \"(Intercept)\" \"cty\" \"\" \"\" ...\n $ rank         : int 2\n $ fitted.values: Named num [1:234] 25 29 27.6 29 22.3 ...\n  ..- attr(*, \"names\")= chr [1:234] \"1\" \"2\" \"3\" \"4\" ...\n $ assign       : int [1:2] 0 1\n $ qr           :List of 5\n  ..$ qr   : num [1:234, 1:2] -15.2971 0.0654 0.0654 0.0654 0.0654 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:234] \"1\" \"2\" \"3\" \"4\" ...\n  .. .. ..$ : chr [1:2] \"(Intercept)\" \"cty\"\n  .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  ..$ qraux: num [1:2] 1.07 1.06\n  ..$ pivot: int [1:2] 1 2\n  ..$ tol  : num 1e-07\n  ..$ rank : int 2\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 232\n $ xlevels      : Named list()\n $ call         : language lm(formula = hwy ~ cty, data = mpg)\n $ terms        :Classes 'terms', 'formula'  language hwy ~ cty\n  .. ..- attr(*, \"variables\")= language list(hwy, cty)\n  .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:2] \"hwy\" \"cty\"\n  .. .. .. ..$ : chr \"cty\"\n  .. ..- attr(*, \"term.labels\")= chr \"cty\"\n  .. ..- attr(*, \"order\")= int 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. ..- attr(*, \"predvars\")= language list(hwy, cty)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. ..- attr(*, \"names\")= chr [1:2] \"hwy\" \"cty\"\n $ model        :'data.frame':  234 obs. of  2 variables:\n  ..$ hwy: int [1:234] 29 29 31 30 26 26 27 26 25 28 ...\n  ..$ cty: int [1:234] 18 21 20 21 16 18 18 18 16 20 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language hwy ~ cty\n  .. .. ..- attr(*, \"variables\")= language list(hwy, cty)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"hwy\" \"cty\"\n  .. .. .. .. ..$ : chr \"cty\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"cty\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(hwy, cty)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"hwy\" \"cty\"\n - attr(*, \"class\")= chr \"lm\"\n\n\nThere are three ways to extract from a list. Check out the pepper shaker analogy in Section 27.3.3 (note: shaker = list)\n\n[] returns new, smaller list (fewer pepper packs in shaker)\n[[]] drills down one level (individual pepper packs not in shaker)\n\nI. [ to extract a sub-list. The result is a list.\n\nmpg_model[1]\n\n$coefficients\n(Intercept)         cty \n  0.8920411   1.3374556 \n\ntypeof(mpg_model[1])\n\n[1] \"list\"\n\n\nyou can also do this by name, rather than number:\n\nmpg_model[\"coefficients\"]\n\n$coefficients\n(Intercept)         cty \n  0.8920411   1.3374556 \n\n\n\n[[ extracts a single component from the list… It removes a level of hierarchy\n\n\nmpg_model[[1]]\n\n(Intercept)         cty \n  0.8920411   1.3374556 \n\ntypeof(mpg_model[[1]])\n\n[1] \"double\"\n\n\nAgain, it can be done by name instead:\n\nmpg_model[[\"coefficients\"]]\n\n(Intercept)         cty \n  0.8920411   1.3374556 \n\n\n\n$ is a shorthand way of extracting elements by name… it is similar to [[ in that it removes a level of hierarchy. You don’t need quotes. (We’ve seen this with tibbles before too!)\n\n\nmpg_model$coefficients\n\n(Intercept)         cty \n  0.8920411   1.3374556"
  },
  {
    "objectID": "05_data_types.html#str",
    "href": "05_data_types.html#str",
    "title": "Data types",
    "section": "str",
    "text": "str\nThe str function allows us to see the structure of a list, as well as any attributes.\n\nmpg\n\n# A tibble: 234 × 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n 2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n 3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n 4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n 5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n 6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n 7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n 8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n 9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n# ℹ 224 more rows\n\nstr(mpg)\n\ntibble [234 × 11] (S3: tbl_df/tbl/data.frame)\n $ manufacturer: chr [1:234] \"audi\" \"audi\" \"audi\" \"audi\" ...\n $ model       : chr [1:234] \"a4\" \"a4\" \"a4\" \"a4\" ...\n $ displ       : num [1:234] 1.8 1.8 2 2 2.8 2.8 3.1 1.8 1.8 2 ...\n $ year        : int [1:234] 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 ...\n $ cyl         : int [1:234] 4 4 4 4 6 6 6 4 4 4 ...\n $ trans       : chr [1:234] \"auto(l5)\" \"manual(m5)\" \"manual(m6)\" \"auto(av)\" ...\n $ drv         : chr [1:234] \"f\" \"f\" \"f\" \"f\" ...\n $ cty         : int [1:234] 18 21 20 21 16 18 18 18 16 20 ...\n $ hwy         : int [1:234] 29 29 31 30 26 26 27 26 25 28 ...\n $ fl          : chr [1:234] \"p\" \"p\" \"p\" \"p\" ...\n $ class       : chr [1:234] \"compact\" \"compact\" \"compact\" \"compact\" ...\n\nmpg_model\n\n\nCall:\nlm(formula = hwy ~ cty, data = mpg)\n\nCoefficients:\n(Intercept)          cty  \n      0.892        1.337  \n\nstr(mpg_model)\n\nList of 12\n $ coefficients : Named num [1:2] 0.892 1.337\n  ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"cty\"\n $ residuals    : Named num [1:234] 4.0338 0.0214 3.3588 1.0214 3.7087 ...\n  ..- attr(*, \"names\")= chr [1:234] \"1\" \"2\" \"3\" \"4\" ...\n $ effects      : Named num [1:234] -358.566 -86.887 3.121 0.787 3.458 ...\n  ..- attr(*, \"names\")= chr [1:234] \"(Intercept)\" \"cty\" \"\" \"\" ...\n $ rank         : int 2\n $ fitted.values: Named num [1:234] 25 29 27.6 29 22.3 ...\n  ..- attr(*, \"names\")= chr [1:234] \"1\" \"2\" \"3\" \"4\" ...\n $ assign       : int [1:2] 0 1\n $ qr           :List of 5\n  ..$ qr   : num [1:234, 1:2] -15.2971 0.0654 0.0654 0.0654 0.0654 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:234] \"1\" \"2\" \"3\" \"4\" ...\n  .. .. ..$ : chr [1:2] \"(Intercept)\" \"cty\"\n  .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  ..$ qraux: num [1:2] 1.07 1.06\n  ..$ pivot: int [1:2] 1 2\n  ..$ tol  : num 1e-07\n  ..$ rank : int 2\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 232\n $ xlevels      : Named list()\n $ call         : language lm(formula = hwy ~ cty, data = mpg)\n $ terms        :Classes 'terms', 'formula'  language hwy ~ cty\n  .. ..- attr(*, \"variables\")= language list(hwy, cty)\n  .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:2] \"hwy\" \"cty\"\n  .. .. .. ..$ : chr \"cty\"\n  .. ..- attr(*, \"term.labels\")= chr \"cty\"\n  .. ..- attr(*, \"order\")= int 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. ..- attr(*, \"predvars\")= language list(hwy, cty)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. ..- attr(*, \"names\")= chr [1:2] \"hwy\" \"cty\"\n $ model        :'data.frame':  234 obs. of  2 variables:\n  ..$ hwy: int [1:234] 29 29 31 30 26 26 27 26 25 28 ...\n  ..$ cty: int [1:234] 18 21 20 21 16 18 18 18 16 20 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language hwy ~ cty\n  .. .. ..- attr(*, \"variables\")= language list(hwy, cty)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"hwy\" \"cty\"\n  .. .. .. .. ..$ : chr \"cty\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"cty\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(hwy, cty)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"hwy\" \"cty\"\n - attr(*, \"class\")= chr \"lm\"\n\n\nAs you can see, the mpg_model is a very complicated list with lots of attributes. The elements of the list can be all different types.\nThe last attribute is the object class, which it lists as lm.\n\nclass(mpg_model)\n\n[1] \"lm\"\n\n\nNow let’s see how extracting from a list works with a tibble (since a tibble is built on top of a list).\n\nugly_data &lt;- tibble(\n  truefalse = c(\"TRUE\", \"FALSE\", \"NA\"),\n  numbers = c(\"1\", \"2\", \"3\"),\n  dates = c(\"2010-01-01\", \"1979-10-14\", \"2013-08-17\"),\n  more_numbers = c(\"1\", \"231\", \".\")\n)\nugly_data\n\n# A tibble: 3 × 4\n  truefalse numbers dates      more_numbers\n  &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;       \n1 TRUE      1       2010-01-01 1           \n2 FALSE     2       1979-10-14 231         \n3 NA        3       2013-08-17 .           \n\nstr(ugly_data)   # we've seen str before... stands for \"structure\"\n\ntibble [3 × 4] (S3: tbl_df/tbl/data.frame)\n $ truefalse   : chr [1:3] \"TRUE\" \"FALSE\" \"NA\"\n $ numbers     : chr [1:3] \"1\" \"2\" \"3\"\n $ dates       : chr [1:3] \"2010-01-01\" \"1979-10-14\" \"2013-08-17\"\n $ more_numbers: chr [1:3] \"1\" \"231\" \".\"\n\npretty_data &lt;- ugly_data %&gt;% \n  mutate(truefalse = parse_logical(truefalse),\n         numbers = parse_number(numbers),\n         dates = parse_date(dates),\n         more_numbers = parse_number(more_numbers))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `more_numbers = parse_number(more_numbers)`.\nCaused by warning:\n! 1 parsing failure.\nrow col expected actual\n  3  -- a number      .\n\npretty_data\n\n# A tibble: 3 × 4\n  truefalse numbers dates      more_numbers\n  &lt;lgl&gt;       &lt;dbl&gt; &lt;date&gt;            &lt;dbl&gt;\n1 TRUE            1 2010-01-01            1\n2 FALSE           2 1979-10-14          231\n3 NA              3 2013-08-17           NA\n\nstr(pretty_data)\n\ntibble [3 × 4] (S3: tbl_df/tbl/data.frame)\n $ truefalse   : logi [1:3] TRUE FALSE NA\n $ numbers     : num [1:3] 1 2 3\n $ dates       : Date[1:3], format: \"2010-01-01\" \"1979-10-14\" ...\n $ more_numbers: num [1:3] 1 231 NA\n  ..- attr(*, \"problems\")= tibble [1 × 4] (S3: tbl_df/tbl/data.frame)\n  .. ..$ row     : int 3\n  .. ..$ col     : int NA\n  .. ..$ expected: chr \"a number\"\n  .. ..$ actual  : chr \".\"\n\n# Get a smaller tibble\npretty_data[1]\n\n# A tibble: 3 × 1\n  truefalse\n  &lt;lgl&gt;    \n1 TRUE     \n2 FALSE    \n3 NA       \n\nclass(pretty_data[1])\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\ntypeof(pretty_data[1])\n\n[1] \"list\"\n\npretty_data[2:3]\n\n# A tibble: 3 × 2\n  numbers dates     \n    &lt;dbl&gt; &lt;date&gt;    \n1       1 2010-01-01\n2       2 1979-10-14\n3       3 2013-08-17\n\npretty_data[1, 3:4]\n\n# A tibble: 1 × 2\n  dates      more_numbers\n  &lt;date&gt;            &lt;dbl&gt;\n1 2010-01-01            1\n\npretty_data[\"dates\"]\n\n# A tibble: 3 × 1\n  dates     \n  &lt;date&gt;    \n1 2010-01-01\n2 1979-10-14\n3 2013-08-17\n\npretty_data[c(\"dates\", \"more_numbers\")]\n\n# A tibble: 3 × 2\n  dates      more_numbers\n  &lt;date&gt;            &lt;dbl&gt;\n1 2010-01-01            1\n2 1979-10-14          231\n3 2013-08-17           NA\n\npretty_data %&gt;% select(dates, more_numbers) \n\n# A tibble: 3 × 2\n  dates      more_numbers\n  &lt;date&gt;            &lt;dbl&gt;\n1 2010-01-01            1\n2 1979-10-14          231\n3 2013-08-17           NA\n\npretty_data %&gt;% select(dates, more_numbers) %&gt;% slice(1:2) \n\n# A tibble: 2 × 2\n  dates      more_numbers\n  &lt;date&gt;            &lt;dbl&gt;\n1 2010-01-01            1\n2 1979-10-14          231\n\n# Remove a level of hierarchy - drill down one level to get a new object\npretty_data$dates\n\n[1] \"2010-01-01\" \"1979-10-14\" \"2013-08-17\"\n\nclass(pretty_data$dates)\n\n[1] \"Date\"\n\ntypeof(pretty_data$dates)\n\n[1] \"double\"\n\npretty_data[[1]]\n\n[1]  TRUE FALSE    NA\n\nclass(pretty_data[[1]])\n\n[1] \"logical\"\n\ntypeof(pretty_data[[1]])\n\n[1] \"logical\"\n\n\n[Pause to Ponder:] Predict what these lines will produce BEFORE running them:\n\npretty_data[[c(\"dates\", \"more_numbers\")]]\n\nError in `pretty_data[[c(\"dates\", \"more_numbers\")]]`:\n! Can't extract column with `c(\"dates\", \"more_numbers\")`.\n✖ Subscript `c(\"dates\", \"more_numbers\")` must be size 1, not 2.\n\npretty_data[[2]][[3]]\n\n[1] 3\n\npretty_data[[2]][3]\n\n[1] 3\n\npretty_data[[2]][c(TRUE, FALSE, TRUE)]\n\n[1] 1 3\n\npretty_data[[1]][c(1, 2, 1, 2)]\n\n[1]  TRUE FALSE  TRUE FALSE"
  },
  {
    "objectID": "05_data_types.html#generic-functions",
    "href": "05_data_types.html#generic-functions",
    "title": "Data types",
    "section": "Generic functions",
    "text": "Generic functions\nAnother important feature of R is generic functions. Some functions, like plot and summary for example, behave very differently depending on the class of their input.\n\nclass(mpg)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nsummary(mpg)\n\n manufacturer          model               displ            year     \n Length:234         Length:234         Min.   :1.600   Min.   :1999  \n Class :character   Class :character   1st Qu.:2.400   1st Qu.:1999  \n Mode  :character   Mode  :character   Median :3.300   Median :2004  \n                                       Mean   :3.472   Mean   :2004  \n                                       3rd Qu.:4.600   3rd Qu.:2008  \n                                       Max.   :7.000   Max.   :2008  \n      cyl           trans               drv                 cty       \n Min.   :4.000   Length:234         Length:234         Min.   : 9.00  \n 1st Qu.:4.000   Class :character   Class :character   1st Qu.:14.00  \n Median :6.000   Mode  :character   Mode  :character   Median :17.00  \n Mean   :5.889                                         Mean   :16.86  \n 3rd Qu.:8.000                                         3rd Qu.:19.00  \n Max.   :8.000                                         Max.   :35.00  \n      hwy             fl               class          \n Min.   :12.00   Length:234         Length:234        \n 1st Qu.:18.00   Class :character   Class :character  \n Median :24.00   Mode  :character   Mode  :character  \n Mean   :23.44                                        \n 3rd Qu.:27.00                                        \n Max.   :44.00                                        \n\nclass(mpg_model)\n\n[1] \"lm\"\n\nsummary(mpg_model)\n\n\nCall:\nlm(formula = hwy ~ cty, data = mpg)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.3408 -1.2790  0.0214  1.0338  4.0461 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.89204    0.46895   1.902   0.0584 .  \ncty          1.33746    0.02697  49.585   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.752 on 232 degrees of freedom\nMultiple R-squared:  0.9138,    Adjusted R-squared:  0.9134 \nF-statistic:  2459 on 1 and 232 DF,  p-value: &lt; 2.2e-16\n\n\nAs a simpler case, consider the mean function.\n\nmean\n\nfunction (x, ...) \nUseMethod(\"mean\")\n&lt;bytecode: 0x00000137dbf49fb0&gt;\n&lt;environment: namespace:base&gt;\n\n\nAs a generic function, we can see what methods are available:\n\nmethods(mean)\n\n[1] mean.Date        mean.default     mean.difftime    mean.POSIXct    \n[5] mean.POSIXlt     mean.quosure*    mean.vctrs_vctr*\nsee '?methods' for accessing help and source code\n\n\n\nmean(c(20, 21, 23))\n\n[1] 21.33333\n\nlibrary(lubridate)\ndate_test &lt;- ymd(c(\"2020-03-20\", \"2020-03-21\", \"2020-03-23\"))\nmean(date_test)\n\n[1] \"2020-03-21\""
  },
  {
    "objectID": "05_data_types.html#what-makes-tibbles-special",
    "href": "05_data_types.html#what-makes-tibbles-special",
    "title": "Data types",
    "section": "What makes Tibbles special?",
    "text": "What makes Tibbles special?\nTibbles are lists that: - have names attributes (column/variable names) as well as row.names attributes. - have elements that are all vectors of the same length\n\nattributes(mpg)\n\n$class\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n$row.names\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n[109] 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n[127] 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n[145] 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n[163] 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n[181] 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n[199] 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216\n[217] 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234\n\n$names\n [1] \"manufacturer\" \"model\"        \"displ\"        \"year\"         \"cyl\"         \n [6] \"trans\"        \"drv\"          \"cty\"          \"hwy\"          \"fl\"          \n[11] \"class\""
  },
  {
    "objectID": "05_data_types.html#on-your-own",
    "href": "05_data_types.html#on-your-own",
    "title": "Data types",
    "section": "On Your Own",
    "text": "On Your Own\n\nThe dataset roster includes 24 names (the first 24 alphabetically on this list of names). Let’s suppose this is our class, and you want to divide students into 6 groups. Modify the code below using the rep function to create groups in two different ways.\n\n\nbabynames &lt;- read_csv(\"https://proback.github.io/264_fall_2024/Data/babynames_2000.csv\")\n\nroster &lt;- babynames %&gt;%\n  sample_n(size = 24) %&gt;%\n  select(name) \n\nroster %&gt;%\n  mutate(group_method1 = , \n         group_method2 = )\n\n\nHere’s is a really crazy list that tells you some stuff about data science.\n\n\ndata_sci &lt;- list(first = c(\"first it must work\", \"then it can be\" , \"pretty\"),\n                 DRY = c(\"Do not\", \"Repeat\", \"Yourself\"),\n                 dont_forget = c(\"garbage\", \"in\", \"out\"),\n                 our_first_tibble = mpg,\n                 integers = 1:25,\n                 doubles = sqrt(1:25),\n                 tidyverse = c(pack1 = \"ggplot2\", pack2 = \"dplyr\", \n                               pack3 = \"lubridate\", etc = \"and more!\"),\n                 opinion = list(\"MSCS 264 is\",  \"awesome!\", \"amazing!\", \"rainbows!\")\n                  )\n\nUse str to learn about data_sci.\nNow, figure out how to get exactly the following outputs. Bonus points if you can do it more than one way!\n[1] “first it must work” “then it can be” “pretty”\n$DRY [1] “Do not” “Repeat” “Yourself”\n[1] 3 1 4 1 5 9 3\n  pack1         etc \n“ggplot2” “and more!”\n[1] “rainbows!”\n[1] “garbage” “in” “garbage” “out”"
  },
  {
    "objectID": "03_maps_part2.html",
    "href": "03_maps_part2.html",
    "title": "Working with geospatial data",
    "section": "",
    "text": "Based on Chapter 17 from Modern Data Science with R.\nYou can download this .qmd file from here. Just hit the Download Raw File button.\n\n# Initial packages required (we'll be adding more)\nlibrary(tidyverse)\nlibrary(mdsr)      # package associated with our MDSR book\nlibrary(sf)        \n# sf = support for simple features, a standardized way to encode spatial vector data\n\nOur goal in “Maps - Part 2” is to learn about how to work with shapefiles, which are an open data structure for encoding spatial information. We will learn about projections (from three-dimensional space into two-dimensional space) and how to create informative, spatially-aware visualizations. We will just skim the surface in 264; for a much more thorough coverage, take our Spatial Statistics course!\n\nSections 17.1: Intro to spatial data - the famous John Snow case study\nThe most famous early analysis of geospatial data was done by physician John Snow in 1854. In a certain London neighborhood, an outbreak of cholera killed 127 people in three days, resulting in a mass exodus of the local residents. At the time it was thought that cholera was an airborne disease caused by breathing foul air. Snow was critical of this theory, and set about discovering the true transmission mechanism.\n\n# the mdsr package contains data from the cholera outbreak in 1854\n\n# CholeraDeaths is in the sf class - a simple feature collection\n#   with 250 features (locations where people died) and 2 fields\n#   (number who died and location geometry)\nCholeraDeaths\n\nSimple feature collection with 250 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 529160.3 ymin: 180857.9 xmax: 529655.9 ymax: 181306.2\nProjected CRS: +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs\nFirst 10 features:\n   Id Count                  geometry\n1   0     3 POINT (529308.7 181031.4)\n2   0     2 POINT (529312.2 181025.2)\n3   0     1 POINT (529314.4 181020.3)\n4   0     1 POINT (529317.4 181014.3)\n5   0     4 POINT (529320.7 181007.9)\n6   0     2   POINT (529336.7 181006)\n7   0     2 POINT (529290.1 181024.4)\n8   0     2   POINT (529301 181021.2)\n9   0     3   POINT (529285 181020.2)\n10  0     2 POINT (529288.4 181031.8)\n\n# There is no context in this original plot - we want to include the\n#   underlying London street map and the location of water pumps\nplot(CholeraDeaths[\"Count\"])\n\n\n\n\n\n\nSection 17.2: Spatial data structures\nThe most commonly used format for spatial data is called a shapefile. There are many other formats, and while we won’t master all of the details in MSCS 264, there are some important basic notions that one must have in order to work with spatial data.\nShapefiles evolved as the native file format of the ArcView program developed by the Environmental Systems Research Institute (Esri) and have since become an open specification. They can be downloaded from many different government websites and other locations that publish spatial data. Spatial data consists not of rows and columns, but of geometric objects like points, lines, and polygons. Shapefiles contain vector-based instructions for drawing the boundaries of countries, counties, and towns, etc. As such, shapefiles are richer - and more complicated - data containers than simple data frames.\nFirst, the term “shapefile” is somewhat of a misnomer, as there are several files that you must have in order to read spatial data. These files have extensions like .shp, .shx, and .dbf, and they are typically stored in a common directory.\nThere are many packages for R that specialize in working with spatial data, but we will focus on the most recent: sf. This package provides a tidyverse-friendly set of class definitions and functions for spatial objects in R. These will have the class sf (we will learn more about classes later in 264!\n\n# First, load shapefiles for London in 1854, along with information\n#   about deaths and pumps\nsnow_url &lt;- \"https://raw.githubusercontent.com/proback/264_fall_2024/main/Data/SnowGIS_SHP.zip\"\nsnow_zip &lt;- fs::path(tempdir(), \"SnowGIS_SHP.zip\")\ndownload.file(snow_url, destfile = snow_zip)\nsnow_raw &lt;- fs::path(tempdir(), \"SnowGIS_SHP\")\nunzip(snow_zip, exdir = snow_raw)\ndsn &lt;- fs::path(snow_raw, \"SnowGIS_SHP\")\nlist.files(dsn)  # note 22 files\n\n [1] \"Cholera_Deaths.dbf\"          \"Cholera_Deaths.prj\"         \n [3] \"Cholera_Deaths.sbn\"          \"Cholera_Deaths.sbx\"         \n [5] \"Cholera_Deaths.shp\"          \"Cholera_Deaths.shx\"         \n [7] \"OSMap.tfw\"                   \"OSMap.tif\"                  \n [9] \"OSMap_Grayscale.tfw\"         \"OSMap_Grayscale.tif\"        \n[11] \"OSMap_Grayscale.tif.aux.xml\" \"OSMap_Grayscale.tif.ovr\"    \n[13] \"Pumps.dbf\"                   \"Pumps.prj\"                  \n[15] \"Pumps.sbx\"                   \"Pumps.shp\"                  \n[17] \"Pumps.shx\"                   \"README.txt\"                 \n[19] \"SnowMap.tfw\"                 \"SnowMap.tif\"                \n[21] \"SnowMap.tif.aux.xml\"         \"SnowMap.tif.ovr\"            \n\nst_layers(dsn)   # 1 layer for 8 pumps and 1 for 250 death locations\n\nDriver: ESRI Shapefile \nAvailable layers:\n      layer_name geometry_type features fields                       crs_name\n1 Cholera_Deaths         Point      250      2 OSGB36 / British National Grid\n2          Pumps         Point        8      1 OSGB36 / British National Grid\n\n# You can also downloaded zip file and uploaded it into R, but this uses a ton of space!\n# dsn &lt;- fs::path(\"Data/SnowGIS_SHP\")\n\n# How to obtain the CholeraDeaths data we examined earlier\nCholeraDeaths &lt;- st_read(dsn, layer = \"Cholera_Deaths\")\n\nReading layer `Cholera_Deaths' from data source \n  `C:\\Users\\roback\\AppData\\Local\\Temp\\RtmpANGxoX\\SnowGIS_SHP\\SnowGIS_SHP' \n  using driver `ESRI Shapefile'\nSimple feature collection with 250 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 529160.3 ymin: 180857.9 xmax: 529655.9 ymax: 181306.2\nProjected CRS: OSGB36 / British National Grid\n\nclass(CholeraDeaths)\n\n[1] \"sf\"         \"data.frame\"\n\nCholeraDeaths\n\nSimple feature collection with 250 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 529160.3 ymin: 180857.9 xmax: 529655.9 ymax: 181306.2\nProjected CRS: OSGB36 / British National Grid\nFirst 10 features:\n   Id Count                  geometry\n1   0     3 POINT (529308.7 181031.4)\n2   0     2 POINT (529312.2 181025.2)\n3   0     1 POINT (529314.4 181020.3)\n4   0     1 POINT (529317.4 181014.3)\n5   0     4 POINT (529320.7 181007.9)\n6   0     2   POINT (529336.7 181006)\n7   0     2 POINT (529290.1 181024.4)\n8   0     2   POINT (529301 181021.2)\n9   0     3   POINT (529285 181020.2)\n10  0     2 POINT (529288.4 181031.8)\n\n\n\n\nSection 17.3: Making maps\n\n# make basic map of deaths with correct lat/long information\nggplot(CholeraDeaths) +\n  geom_sf()    \n\n\n\n# assumes (x,y) info stored in a column called \"geometry\", so we don't\n#   explicitly have to specify the x and y aesthetics\n\n# place deaths on layout of London streets using ggspatial\n#   Note that aesthetics work like other geoms\nlibrary(ggspatial)\nlibrary(prettymapr)\nggplot(CholeraDeaths) + \n  annotation_map_tile(type = \"osm\", zoomin = 0, progress = \"none\") + \n  geom_sf(aes(size = Count), alpha = 0.7)   \n\n\n\n# Notice that points are off.  For example, there should be a cluster\n#   on Broadwick St, and deaths should be in homes and not streets\n\nst_bbox(CholeraDeaths)   # bounding box\n\n    xmin     ymin     xmax     ymax \n529160.3 180857.9 529655.9 181306.2 \n\n# Turns out the geospatial coordinates of CholeraDeaths and ggspatial\n#   are not the same - it comes down to projections\n\n[Pause to ponder:] What do the different options in annotation_map_tile() do? You might check out the help screen…\n\n\nSection 17.3.2: Projections\nThe process of converting locations in a three-dimensional geographic coordinate system to a two-dimensional representation is called projection. It is simply not possible to faithfully preserve all properties present in a three-dimensional space in a two-dimensional space. Thus there is no one best projection system - each has its own advantages and disadvantages.\n\nlibrary(mapproj)\nlibrary(maps)\n\nmap(\"world\", projection = \"mercator\", wrap = TRUE)\n\n\n\nmap(\"world\", projection = \"cylequalarea\", param = 45, wrap = TRUE)\n\n\n\n\n[Pause to ponder:] Describe differences between the first world map (Mercator projection) and the second (Gall-Peters projection).\nHere’s a clever map showing the Mercator projection with the true size and shape of each country overlaid.\nTwo common general-purpose map projections are the Lambert conformal conic projection and the Albers equal-area conic projection. In the former, angles are preserved, while in the latter neither scale nor shape are preserved, but gross distortions of both are minimized.\n\n# Scales specified to be true on the 20th and 50th parallels\n# Note that default resolution of 0 doesn't provide enough detail\nmap(\n  \"state\", projection = \"lambert\", \n  parameters = c(lat0 = 20, lat1 = 50), wrap = TRUE, resolution = -5,\n)\n\n\n\nmap(\n  \"state\", projection = \"albers\", \n  parameters = c(lat0 = 20, lat1 = 50), wrap = TRUE, resolution = -5,\n)\n\n\n\n\nA coordinate reference system (CRS) is needed to keep track of geographic locations. There are three main components to a CRS: ellipsoid, datum, and a projection. Every spatially-aware object in R can have a projection. Three formats that are common for storing information about the projection of a geospatial object are EPSG (an integer from the European Petroleum Survey Group), PROJ.4 (a cryptic string of text), and WKT (Well-Known Text, which can be retrieved or set using the st_crs() command).\nA few common CRSs are:\n\nEPSG:4326 - Also known as WGS84, this is the standard for GPS systems and Google Earth.\nEPSG:3857 - A Mercator projection used in maps tiles3 by Google Maps, Open Street Maps, etc.\nEPSG:27700 - Also known as OSGB 1936, or the British National Grid: United Kingdom Ordnance Survey. It is commonly used in Britain.\n\n\nst_crs(CholeraDeaths)\n\nCoordinate Reference System:\n  User input: OSGB36 / British National Grid \n  wkt:\nPROJCRS[\"OSGB36 / British National Grid\",\n    BASEGEOGCRS[\"OSGB36\",\n        DATUM[\"Ordnance Survey of Great Britain 1936\",\n            ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4277]],\n    CONVERSION[\"British National Grid\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",49,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-2,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996012717,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",400000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",-100000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"United Kingdom (UK) - offshore to boundary of UKCS within 49°45'N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\"],\n        BBOX[49.75,-9.01,61.01,2.01]],\n    ID[\"EPSG\",27700]]\n\n# Uses a transverse Mercator method and the datum (model of the Earth)\n#   is OSGB 1936 = British National Grid\n\n# The st_crs() function will translate from the shorthand EPSG code \n#   to the full-text PROJ.4 strings and WKT.\nst_crs(4326)$epsg\n\n[1] 4326\n\nst_crs(3857)$Wkt\n\n[1] \"PROJCS[\\\"WGS 84 / Pseudo-Mercator\\\",GEOGCS[\\\"WGS 84\\\",DATUM[\\\"WGS_1984\\\",SPHEROID[\\\"WGS 84\\\",6378137,298.257223563,AUTHORITY[\\\"EPSG\\\",\\\"7030\\\"]],AUTHORITY[\\\"EPSG\\\",\\\"6326\\\"]],PRIMEM[\\\"Greenwich\\\",0,AUTHORITY[\\\"EPSG\\\",\\\"8901\\\"]],UNIT[\\\"degree\\\",0.0174532925199433,AUTHORITY[\\\"EPSG\\\",\\\"9122\\\"]],AUTHORITY[\\\"EPSG\\\",\\\"4326\\\"]],PROJECTION[\\\"Mercator_1SP\\\"],PARAMETER[\\\"central_meridian\\\",0],PARAMETER[\\\"scale_factor\\\",1],PARAMETER[\\\"false_easting\\\",0],PARAMETER[\\\"false_northing\\\",0],UNIT[\\\"metre\\\",1],AXIS[\\\"Easting\\\",EAST],AXIS[\\\"Northing\\\",NORTH],EXTENSION[\\\"PROJ4\\\",\\\"+proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0 +x_0=0 +y_0=0 +k=1 +units=m +nadgrids=@null +wktext +no_defs\\\"],AUTHORITY[\\\"EPSG\\\",\\\"3857\\\"]]\"\n\nst_crs(27700)$proj4string\n\n[1] \"+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs\"\n\n# To get Cholera Deaths to line up with Google Maps (Open Street Map tiles) we need to convert to the EPSG 4326 system, since even though Google Maps tiles (and Open Street Map tiles) are projected in the espg:3857 system, they are confusingly returned with coordinates in the epsg:4326 system.  Iyiyi!\ncholera_4326 &lt;- CholeraDeaths |&gt;\n  st_transform(4326)\nst_bbox(cholera_4326)\n\n      xmin       ymin       xmax       ymax \n-0.1400738 51.5118557 -0.1329335 51.5158345 \n\n# Better but not perfect\nggplot(cholera_4326) + \n  annotation_map_tile(type = \"osm\", zoomin = 0) + \n  geom_sf(aes(size = Count), alpha = 0.7)\n\nZoom: 17\n\n\n\n\n# The +datum and +towgs84 arguments were missing from our PROJ.4 string.\nst_crs(CholeraDeaths)$proj4string\n\n[1] \"+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs\"\n\n# If we first assert that the CholeraDeaths data is in epsg:27700. \n#   Then, projecting to epsg:4326 works as intended.\ncholera_latlong &lt;- CholeraDeaths |&gt;\n  st_set_crs(27700) |&gt;\n  st_transform(4326)\nsnow &lt;- ggplot(cholera_latlong) + \n  annotation_map_tile(type = \"osm\", zoomin = 0) + \n  geom_sf(aes(size = Count))\n\n# Add pumps in the same way, and we're done!\npumps &lt;- st_read(dsn, layer = \"Pumps\")\n\nReading layer `Pumps' from data source \n  `C:\\Users\\roback\\AppData\\Local\\Temp\\RtmpANGxoX\\SnowGIS_SHP\\SnowGIS_SHP' \n  using driver `ESRI Shapefile'\nSimple feature collection with 8 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 529183.7 ymin: 180660.5 xmax: 529748.9 ymax: 181193.7\nProjected CRS: OSGB36 / British National Grid\n\npumps_latlong &lt;- pumps |&gt;\n  st_set_crs(27700) |&gt;\n  st_transform(4326)\n\n# The final plot is really only 3 layers - background tiles, points representing deaths, and points representing pumps - but they must be very carefully lined up!\nsnow +\n  geom_sf(data = pumps_latlong, size = 3, color = \"red\")\n\nZoom: 17\n\n\n\n\n\n\n\nSection 17.4: Extended example: NC Congressional Districts\nIn North Carolina, there are about the same number of Democratic and Republican voters in the state. In the fall of 2020, 10 of North Carolina’s 13 congressional representatives were Republican (with one seat currently vacant). How can this be? In this case, geospatial data can help us understand.\nNote: the seats are currently 7 and 7 (NC earned an additional seat for 2022 after the 2020 Census), but 3 are expected to flip back to Republicans again after yet another round of questionable redistricting\n\n# To install fec 12 the first time, uncomment the code below (you might have to install devtools as well):\n# devtools::install_github(\"baumer-lab/fec12\")\nlibrary(fec12)\nprint(results_house, width = Inf)\n\n# A tibble: 2,343 × 13\n   state district_id cand_id   incumbent party primary_votes primary_percent\n   &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;     &lt;lgl&gt;     &lt;chr&gt;         &lt;dbl&gt;           &lt;dbl&gt;\n 1 AL    01          H2AL01077 TRUE      R             48702          0.555 \n 2 AL    01          H2AL01176 FALSE     R             21308          0.243 \n 3 AL    01          H2AL01184 FALSE     R             13809          0.158 \n 4 AL    01          H0AL01030 FALSE     R              3854          0.0440\n 5 AL    02          H0AL02087 TRUE      R                NA         NA     \n 6 AL    02          H2AL02141 FALSE     D                NA         NA     \n 7 AL    03          H2AL03032 TRUE      R                NA         NA     \n 8 AL    03          H2AL03099 FALSE     D                NA         NA     \n 9 AL    04          H6AL04098 TRUE      R                NA         NA     \n10 AL    04          H2AL04055 FALSE     D             10971          0.514 \n   runoff_votes runoff_percent general_votes general_percent won   footnotes\n          &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;           &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt;    \n 1           NA             NA        196374           0.979 TRUE  &lt;NA&gt;     \n 2           NA             NA            NA          NA     FALSE &lt;NA&gt;     \n 3           NA             NA            NA          NA     FALSE &lt;NA&gt;     \n 4           NA             NA            NA          NA     FALSE &lt;NA&gt;     \n 5           NA             NA        180591           0.636 TRUE  &lt;NA&gt;     \n 6           NA             NA        103092           0.363 FALSE &lt;NA&gt;     \n 7           NA             NA        175306           0.640 TRUE  &lt;NA&gt;     \n 8           NA             NA         98141           0.358 FALSE &lt;NA&gt;     \n 9           NA             NA        199071           0.740 TRUE  &lt;NA&gt;     \n10           NA             NA         69706           0.259 FALSE &lt;NA&gt;     \n# ℹ 2,333 more rows\n\nresults_house |&gt;\n  group_by(state, district_id) |&gt;\n  summarize(N = n())\n\n# A tibble: 445 × 3\n# Groups:   state [56]\n   state district_id     N\n   &lt;chr&gt; &lt;chr&gt;       &lt;int&gt;\n 1 AK    00             10\n 2 AL    01              4\n 3 AL    02              2\n 4 AL    03              2\n 5 AL    04              3\n 6 AL    05              3\n 7 AL    06              6\n 8 AL    07              3\n 9 AR    01              6\n10 AR    02              4\n# ℹ 435 more rows\n\n\n[Pause to ponder:] Why are there 435 Representatives in the US House but 445 state/district combinations in our data? And how should we handle cases in which there’s just not 1 Democrat vs 1 Republican?\n\n# summary of the 13 congressional NC districts and the 2012 voting\ndistrict_elections &lt;- results_house |&gt;\n  mutate(district = parse_number(district_id)) |&gt;\n  group_by(state, district) |&gt;\n  summarize(\n    N = n(), \n    total_votes = sum(general_votes, na.rm = TRUE),\n    d_votes = sum(ifelse(party == \"D\", general_votes, 0), na.rm = TRUE),\n    r_votes = sum(ifelse(party == \"R\", general_votes, 0), na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  mutate(\n    other_votes = total_votes - d_votes - r_votes,\n    r_prop = r_votes / total_votes,  \n    winner = ifelse(r_votes &gt; d_votes, \"Republican\", \"Democrat\")\n  )\nnc_results &lt;- district_elections |&gt;\n  filter(state == \"NC\")\nnc_results |&gt;                  \n  select(-state)\n\n# A tibble: 13 × 8\n   district     N total_votes d_votes r_votes other_votes r_prop winner    \n      &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n 1        1     4      338066  254644   77288        6134  0.229 Democrat  \n 2        2     8      311397  128973  174066        8358  0.559 Republican\n 3        3     3      309885  114314  195571           0  0.631 Republican\n 4        4     4      348485  259534   88951           0  0.255 Democrat  \n 5        5     3      349197  148252  200945           0  0.575 Republican\n 6        6     4      364583  142467  222116           0  0.609 Republican\n 7        7     4      336736  168695  168041           0  0.499 Democrat  \n 8        8     8      301824  137139  160695        3990  0.532 Republican\n 9        9    13      375690  171503  194537        9650  0.518 Republican\n10       10     6      334849  144023  190826           0  0.570 Republican\n11       11    11      331426  141107  190319           0  0.574 Republican\n12       12     3      310908  247591   63317           0  0.204 Democrat  \n13       13     5      370610  160115  210495           0  0.568 Republican\n\n\n[Pause to ponder:]\n\nExplain how sum(ifelse(party == \"D\", general_votes, 0), na.rm = TRUE) works\nExplain why we use .groups = \"drop\". Hint: try excluding that line and running again.\nDo you see any potential problems with ifelse(r_votes &gt; d_votes, \"Republican\", \"Democrat\")?\nWhat observations can you make about the final nc_results table?\n\n\n# distribution of total number of votes is narrow by design\nnc_results |&gt;\n  skim(total_votes) |&gt;\n  select(-na)\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvar\nn\nmean\nsd\np0\np25\np50\np75\np100\n\n\n\n\ntotal_votes\n13\n337204.3\n24175.2\n301824\n311397\n336736\n349197\n375690\n\n\n\n\n# compare total Dem and Rep votes across NC in 2012\nnc_results |&gt;\n  summarize(\n    N = n(), \n    state_votes = sum(total_votes), \n    state_d = sum(d_votes), \n    state_r = sum(r_votes)\n  ) |&gt;\n  mutate(\n    d_prop = state_d / state_votes, \n    r_prop = state_r / state_votes\n  )\n\n# A tibble: 1 × 6\n      N state_votes state_d state_r d_prop r_prop\n  &lt;int&gt;       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1    13     4383656 2218357 2137167  0.506  0.488\n\n# Proportion of Rep votes by district\nnc_results |&gt;\n  select(district, r_prop, winner) |&gt;\n  arrange(desc(r_prop))\n\n# A tibble: 13 × 3\n   district r_prop winner    \n      &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n 1        3  0.631 Republican\n 2        6  0.609 Republican\n 3        5  0.575 Republican\n 4       11  0.574 Republican\n 5       10  0.570 Republican\n 6       13  0.568 Republican\n 7        2  0.559 Republican\n 8        8  0.532 Republican\n 9        9  0.518 Republican\n10        7  0.499 Democrat  \n11        4  0.255 Democrat  \n12        1  0.229 Democrat  \n13       12  0.204 Democrat  \n\n\nNow let’s layer the results above on a map of North Carolina to create an effective visualization of the situation. How does the shape of districts where Republicans won compare with the shape where Democrats won?\n\n# Download congressional district shapefiles for the 113th Congress from a UCLA website (don't sweat the details too much)\nsrc &lt;- \"http://cdmaps.polisci.ucla.edu/shp/districts113.zip\"\nlcl_zip &lt;- fs::path(tempdir(), \"districts113.zip\")\ndownload.file(src, destfile = lcl_zip)\nlcl_districts &lt;- fs::path(tempdir(), \"districts113\")\nunzip(lcl_zip, exdir = lcl_districts)\ndsn_districts &lt;- fs::path(lcl_districts, \"districtShapes\")\n\n# You can also downloaded zip file and uploaded it into R, but this uses a ton of space!\n# dsn_districts &lt;- fs::path(\"Data/districtShapes\")\n\n# read shapefiles into R as an sf object\nst_layers(dsn_districts)\n\nDriver: ESRI Shapefile \nAvailable layers:\n    layer_name geometry_type features fields crs_name\n1 districts113       Polygon      436     15    NAD83\n\n# be able to read as a data frame as well\ndistricts &lt;- st_read(dsn_districts, layer = \"districts113\") |&gt;\n  mutate(DISTRICT = parse_number(as.character(DISTRICT)))\n\nReading layer `districts113' from data source \n  `C:\\Users\\roback\\AppData\\Local\\Temp\\RtmpANGxoX\\districts113\\districtShapes' \n  using driver `ESRI Shapefile'\nSimple feature collection with 436 features and 15 fields (with 1 geometry empty)\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.1473 ymin: 18.91383 xmax: 179.7785 ymax: 71.35256\nGeodetic CRS:  NAD83\n\nhead(districts, width = Inf)\n\nSimple feature collection with 6 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -91.82307 ymin: 29.41135 xmax: -66.94983 ymax: 47.45969\nGeodetic CRS:  NAD83\n  STATENAME           ID DISTRICT STARTCONG ENDCONG DISTRICTSI COUNTY PAGE  LAW\n1 Louisiana 022113114006        6       113     114       &lt;NA&gt;   &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;\n2     Maine 023113114001        1       113     114       &lt;NA&gt;   &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;\n3     Maine 023113114002        2       113     114       &lt;NA&gt;   &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;\n4  Maryland 024113114001        1       113     114       &lt;NA&gt;   &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;\n5  Maryland 024113114002        2       113     114       &lt;NA&gt;   &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;\n6  Maryland 024113114003        3       113     114       &lt;NA&gt;   &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;\n  NOTE BESTDEC                  FINALNOTE RNOTE                 LASTCHANGE\n1 &lt;NA&gt;    &lt;NA&gt; {\"From US Census website\"}  &lt;NA&gt; 2016-05-29 16:44:10.857626\n2 &lt;NA&gt;    &lt;NA&gt; {\"From US Census website\"}  &lt;NA&gt; 2016-05-29 16:44:10.857626\n3 &lt;NA&gt;    &lt;NA&gt; {\"From US Census website\"}  &lt;NA&gt; 2016-05-29 16:44:10.857626\n4 &lt;NA&gt;    &lt;NA&gt; {\"From US Census website\"}  &lt;NA&gt; 2016-05-29 16:44:10.857626\n5 &lt;NA&gt;    &lt;NA&gt; {\"From US Census website\"}  &lt;NA&gt; 2016-05-29 16:44:10.857626\n6 &lt;NA&gt;    &lt;NA&gt; {\"From US Census website\"}  &lt;NA&gt; 2016-05-29 16:44:10.857626\n  FROMCOUNTY                       geometry\n1          F MULTIPOLYGON (((-91.82288 3...\n2          F MULTIPOLYGON (((-70.98905 4...\n3          F MULTIPOLYGON (((-71.08216 4...\n4          F MULTIPOLYGON (((-77.31156 3...\n5          F MULTIPOLYGON (((-76.8763 39...\n6          F MULTIPOLYGON (((-77.15622 3...\n\nclass(districts)\n\n[1] \"sf\"         \"data.frame\"\n\n# create basic plot with NC congressional districts\nnc_shp &lt;- districts |&gt;\n  filter(STATENAME == \"North Carolina\")\nnc_shp |&gt;\n  st_geometry() |&gt;\n  plot(col = gray.colors(nrow(nc_shp)))\n\n\n\n# Append election results to geospatial data\nnc_merged &lt;- nc_shp |&gt;\n  st_transform(4326) |&gt;\n  inner_join(nc_results, by = c(\"DISTRICT\" = \"district\"))\nhead(nc_merged, width = Inf)\n\nSimple feature collection with 6 features and 23 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -81.91811 ymin: 34.19118 xmax: -75.45998 ymax: 36.58812\nGeodetic CRS:  WGS 84\n       STATENAME           ID DISTRICT STARTCONG ENDCONG DISTRICTSI COUNTY PAGE\n1 North Carolina 037113114002        2       113     114       &lt;NA&gt;   &lt;NA&gt; &lt;NA&gt;\n2 North Carolina 037113114003        3       113     114       &lt;NA&gt;   &lt;NA&gt; &lt;NA&gt;\n3 North Carolina 037113114004        4       113     114       &lt;NA&gt;   &lt;NA&gt; &lt;NA&gt;\n4 North Carolina 037113114001        1       113     114       &lt;NA&gt;   &lt;NA&gt; &lt;NA&gt;\n5 North Carolina 037113114005        5       113     114       &lt;NA&gt;   &lt;NA&gt; &lt;NA&gt;\n6 North Carolina 037113114006        6       113     114       &lt;NA&gt;   &lt;NA&gt; &lt;NA&gt;\n   LAW NOTE BESTDEC                  FINALNOTE RNOTE                 LASTCHANGE\n1 &lt;NA&gt; &lt;NA&gt;    &lt;NA&gt; {\"From US Census website\"}  &lt;NA&gt; 2016-05-29 16:44:10.857626\n2 &lt;NA&gt; &lt;NA&gt;    &lt;NA&gt; {\"From US Census website\"}  &lt;NA&gt; 2016-05-29 16:44:10.857626\n3 &lt;NA&gt; &lt;NA&gt;    &lt;NA&gt; {\"From US Census website\"}  &lt;NA&gt; 2016-05-29 16:44:10.857626\n4 &lt;NA&gt; &lt;NA&gt;    &lt;NA&gt; {\"From US Census website\"}  &lt;NA&gt; 2016-05-29 16:44:10.857626\n5 &lt;NA&gt; &lt;NA&gt;    &lt;NA&gt; {\"From US Census website\"}  &lt;NA&gt; 2016-05-29 16:44:10.857626\n6 &lt;NA&gt; &lt;NA&gt;    &lt;NA&gt; {\"From US Census website\"}  &lt;NA&gt; 2016-05-29 16:44:10.857626\n  FROMCOUNTY state N total_votes d_votes r_votes other_votes    r_prop\n1          F    NC 8      311397  128973  174066        8358 0.5589842\n2          F    NC 3      309885  114314  195571           0 0.6311083\n3          F    NC 4      348485  259534   88951           0 0.2552506\n4          F    NC 4      338066  254644   77288        6134 0.2286181\n5          F    NC 3      349197  148252  200945           0 0.5754488\n6          F    NC 4      364583  142467  222116           0 0.6092330\n      winner                       geometry\n1 Republican MULTIPOLYGON (((-80.05325 3...\n2 Republican MULTIPOLYGON (((-78.27217 3...\n3   Democrat MULTIPOLYGON (((-79.47249 3...\n4   Democrat MULTIPOLYGON (((-78.95837 3...\n5 Republican MULTIPOLYGON (((-81.91805 3...\n6 Republican MULTIPOLYGON (((-80.97462 3...\n\n# Color based on winning party\n#   Note that geom_sf is part of ggplot2 package, while st_geometry is\n#   part of sf package\nnc &lt;- ggplot(data = nc_merged, aes(fill = winner)) +\n  annotation_map_tile(zoom = 6, type = \"osm\", progress = \"none\") + \n  geom_sf(alpha = 0.5) +\n  scale_fill_manual(\"Winner\", values = c(\"blue\", \"red\")) + \n  geom_sf_label(aes(label = DISTRICT), fill = \"white\") + \n  theme_void()\nnc\n\n\n\n# Color based on proportion Rep.  Be sure to let limits so centered at 0.5.\n# This is a choropleth map, where meaningful shading relates to some attribute\nnc +\n  aes(fill = r_prop) + \n  scale_fill_distiller(\n    \"Proportion\\nRepublican\", \n    palette = \"RdBu\", \n    limits = c(0.2, 0.8)\n  )\n\n\n\n# A leaflet map can allow us to zoom in and see where major cities fit, etc.\nlibrary(leaflet)\npal &lt;- colorNumeric(palette = \"RdBu\", domain = c(0, 1))\n\nleaflet_nc &lt;- leaflet(nc_merged) |&gt;\n  addTiles() |&gt;\n  addPolygons(\n    weight = 1, fillOpacity = 0.7, \n    color = ~pal(1 - r_prop),   # so red association with Reps\n    popup = ~paste(\"District\", DISTRICT, \"&lt;/br&gt;\", round(r_prop, 4))\n  ) |&gt;                          # popups show prop Republican\n  setView(lng = -80, lat = 35, zoom = 7)\nleaflet_nc\n\n\n\n\n\n[Pause to ponder:] What have you learned by layering the voting data on the voting districts of North Carolina?"
  },
  {
    "objectID": "01_review164.html",
    "href": "01_review164.html",
    "title": "Review of Data Science 1",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\n\nDeterminants of COVID vaccination rates\nFirst, a little detour to describe several alternatives for reading in data:\nIf you navigate to my Github account, and find the 264_fall_2024 repo, there is a Data folder inside. You can then click on vacc_Mar21.csv to see the data we want to download. This link should also get you there, but it’s good to be able to navigate there yourself.\n\n# Approach 1: create a Data folder in the same location where this .qmd file resides, and then store vaccinations_2021.csv in that Data folder\nvaccine_data &lt;- read_csv(\"Data/vaccinations_2021.csv\")\n\n# Approach 2: give R the complete path to the location of vaccinations_2021.csv, starting with Home (~)\nvaccine_data &lt;- read_csv(\"~/264_fall_2024/Data/vaccinations_2021.csv\")\n\n# Approach 3: link to our course webpage, and then know we have a Data folder containing all our csvs\nvaccine_data &lt;- read_csv(\"https://proback.github.io/264_fall_2024/Data/vaccinations_2021.csv\")\n\n# Approach 4: navigate to the data in GitHub, hit the Raw button, and copy that link\nvaccine_data &lt;- read_csv(\"https://raw.githubusercontent.com/proback/264_fall_2024/main/Data/vaccinations_2021.csv\")\n\nA recent Stat 272 project examined determinants of covid vaccination rates at the county level. Our data set contains 3053 rows (1 for each county in the US) and 14 columns; here is a quick description of the variables we’ll be using:\n\nstate = state the county is located in\ncounty = name of the county\nregion = region the state is located in\nmetro_status = Is the county considered “Metro” or “Non-metro”?\nrural_urban_code = from 1 (most urban) to 9 (most rural)\nperc_complete_vac = percent of county completely vaccinated as of 11/9/21\ntot_pop = total population in the county\nvotes_Trump = number of votes for Trump in the county in 2020\nvotes_Biden = number of votes for Biden in the county in 2020\nperc_Biden = percent of votes for Biden in the county in 2020\ned_somecol_perc = percent with some education beyond high school (but not a Bachelor’s degree)\ned_bachormore_perc = percent with a Bachelor’s degree or more\nunemployment_rate_2020 = county unemployment rate in 2020\nmedian_HHincome_2019 = county’s median household income in 2019\n\n\nConsider only Minnesota and its surrounding states (Iowa, Wisconsin, North Dakota, and South Dakota). We want to examine the relationship between the percentage who voted for Biden and the percentage of complete vaccinations by state. Generate two plots to examine this relationship:\n\n\nA scatterplot with points and smoothers colored by state. Make sure the legend is ordered in a meaningful way, and include good labels on your axes and your legend. Also leave off the error bars from your smoothers.\nOne plot per state containing a scatterplot and a smoother.\n\nDescribe which plot you prefer and why. What can you learn from your preferred plot?\n\nWe wish to compare the proportions of counties in each region with median household income above the national median ($69,560).\n\n\nFill in the blanks below to produce a segmented bar plot with regions ordered from highest proportion above the median to lowest.\nCreate a table of proportions by region to illustrate that your bar plot in (a) is in the correct order (you should find two regions that are really close when you just try to eyeball differences).\nExplain why we can replace fct_relevel(region, FILL IN CODE) with\n\nmutate(region_sort = fct_reorder(region, median_HHincome_2019 &lt; 69560, .fun = mean))\nbut not\nmutate(region_sort = fct_reorder(region, median_HHincome_2019 &lt; 69560))\n\nvaccine_data |&gt;\n  mutate(HHincome_vs_national = ifelse(median_HHincome_2019 &lt; 69560, FILL IN CODE)) |&gt;\n  mutate(region_sort = fct_relevel(region, FILL IN CODE)) |&gt;\n  ggplot(mapping = aes(x = region_sort, fill = HHincome_vs_national)) +\n    geom_bar(position = \"fill\")\n\n\nWe want to examine the distribution of total county populations and then see how it’s related to vaccination rates.\n\n\nCarefully and thoroughly explain why the two histograms below provide different plots.\n\n\nvaccine_data |&gt;\n  mutate(tot_pop_millions = tot_pop / 1000000) |&gt;\n  ggplot(mapping = aes(x = tot_pop_millions)) +\n    geom_histogram(bins = 40) +\n    labs(x = \"Total population in millions\")\n\n\n\nvaccine_data |&gt;\n  mutate(tot_pop_millions = tot_pop %/% 1000000) |&gt;\n  ggplot(mapping = aes(x = tot_pop_millions)) +\n    geom_histogram(bins = 40) +\n    labs(x = \"Total population in millions\")\n\n\n\n\n\nFind the top 5 counties in terms of total population.\nPlot a histogram of logged population and describe this distribution.\nPlot the relationship between log population and percent vaccinated using separate colors for Metro and Non-metro counties (be sure there’s no 3rd color used for NAs). Reduce the size and transparency of each point to make the plot more readable. Describe what you can learn from this plot.\n\n\nProduce 3 different plots for illustrating the relationship between the rural_urban_code and percent vaccinated. Hint: you can sometimes turn numeric variables into categorical variables for plotting purposes (e.g. as.factor(), ifelse()).\n\nState your favorite plot, why you like it better than the other two, and what you can learn from your favorite plot.\n\nBEFORE running the code below, sketch the plot that will be produced by R. AFTER running the code, describe what conclusion(s) can we draw from this plot?\n\n\nvaccine_data |&gt;\n  filter(!is.na(perc_Biden)) |&gt;\n  mutate(big_states = fct_lump(state, n = 10)) |&gt;\n  group_by(big_states) |&gt;\n  summarize(IQR_Biden = IQR(perc_Biden)) |&gt;\n  mutate(big_states = fct_reorder(big_states, IQR_Biden)) |&gt;\n  ggplot() + \n    geom_point(aes(x = IQR_Biden, y = big_states))\n\n\n\n\n\nIn this question we will focus only on the 12 states in the Midwest (i.e. where region == “Midwest”).\n\n\nCreate a tibble with the following information for each state. Order states from least to greatest state population.\n\n\nnumber of different rural_urban_codes represented among the state’s counties (there are 9 possible)\ntotal state population\nproportion of Metro counties\nmedian unemployment rate\n\n\nUse your tibble in (a) to produce a plot of the relationship between proportion of Metro counties and median unemployment rate. Points should be colored by the number of different rural_urban_codes in a state, but a single linear trend should be fit to all points. What can you conclude from the plot?\n\n\nGenerate an appropriate plot to compare vaccination rates between two subregions of the US: New England (which contains the states Maine, Vermont, New Hampshire, Massachusetts, Connecticut, Rhode Island) and the Upper Midwest (which, according to the USGS, contains the states Minnesota, Wisconsin, Michigan, Illinois, Indiana, and Iowa). What can you conclude from your plot?\n\nIn this next section, we consider a few variables that could have been included in our data set, but were NOT. Thus, you won’t be able to write and test code, but you nevertheless should be able to use your knowledge of the tidyverse to answer these questions.\nHere are the hypothetical variables:\n\nHR_party = party of that county’s US Representative (Republican, Democrat, Independent, Green, or Libertarian)\npeople_per_MD = number of residents per doctor (higher values = fewer doctors)\nperc_over_65 = percent of residents over 65 years old\nperc_white = percent of residents who identify as white\n\n\nHypothetical R chunk #1:\n\n\n# Hypothetical R chunk 1\ntemp &lt;- vaccine_data |&gt;\n  mutate(new_perc_vac = ifelse(perc_complete_vac &gt; 95, NA, perc_complete_vac),\n         MD_group = cut_number(people_per_MD, 3)) |&gt;\n  group_by(MD_group) |&gt;\n  summarise(n = n(),\n            mean_perc_vac = mean(new_perc_vac, na.rm = TRUE),\n            mean_white = mean(perc_white, na.rm = TRUE))\n\n\nDescribe the tibble temp created above. What would be the dimensions? What do rows and columns represent?\nWhat would happen if we replaced new_perc_vac = ifelse(perc_complete_vac &gt; 95, NA, perc_complete_vac) with new_perc_vac = ifelse(perc_complete_vac &gt; 95, perc_complete_vac, NA)?\nWhat would happen if we replaced mean_white = mean(perc_white, na.rm = TRUE) with mean_white = mean(perc_white)?\nWhat would happen if we removed group_by(MD_group)?\n\n\nHypothetical R chunk #2:\n\n\n# Hypothetical R chunk 2\nggplot(data = vaccine_data) +\n  geom_point(mapping = aes(x = perc_over_65, y = perc_complete_vac, \n                           color = HR_party)) +\n  geom_smooth()\n\ntemp &lt;- vaccine_data |&gt;\n  group_by(HR_party) |&gt;\n  summarise(var1 = n()) |&gt;\n  arrange(desc(var1)) |&gt;\n  slice_head(n = 3)\n\nvaccine_data |&gt;\n  ggplot(mapping = aes(x = fct_reorder(HR_party, perc_over_65, .fun = median), \n                       y = perc_over_65)) +\n    geom_boxplot()\n\n\nWhy would the first plot produce an error?\nDescribe the tibble temp created above. What would be the dimensions? What do rows and columns represent?\nWhat would happen if we replaced fct_reorder(HR_party, perc_over_65, .fun = median) with HR_party?\n\n\nHypothetical R chunk #3:\n\n\n# Hypothetical R chunk 3\nvaccine_data |&gt;\n  filter(!is.na(people_per_MD)) |&gt;\n  mutate(state_lump = fct_lump(state, n = 4)) |&gt;\n  group_by(state_lump, rural_urban_code) |&gt;\n  summarise(mean_people_per_MD = mean(people_per_MD)) |&gt;\n  ggplot(mapping = aes(x = rural_urban_code, y = mean_people_per_MD, \n      colour = fct_reorder2(state_lump, rural_urban_code, mean_people_per_MD))) +\n    geom_line()\n\n\nDescribe the tibble piped into the ggplot above. What would be the dimensions? What do rows and columns represent?\nCarefully describe the plot created above.\nWhat would happen if we removed filter(!is.na(people_per_MD))?\nWhat would happen if we replaced fct_reorder2(state_lump, rural_urban_code, mean_people_per_MD) with state_lump?"
  },
  {
    "objectID": "02_maps_part1.html",
    "href": "02_maps_part1.html",
    "title": "Creating informative maps",
    "section": "",
    "text": "Based on Section 3.2.3 from Modern Data Science with R.\nYou can download this .qmd file from here. Just hit the Download Raw File button.\n\n# Initial packages required (we'll be adding more)\nlibrary(tidyverse)\nlibrary(mdsr)      # package associated with our MDSR book\n\n\nOpening example\nHere is a simple choropleth map example from MDSR\n\n# CIACountries is a 236 x 8 data set with information on each country\n#   taken from the CIA factbook - gdp, education, internet use, etc.\nhead(CIACountries)\n\n         country      pop    area oil_prod   gdp educ   roadways net_users\n1    Afghanistan 32564342  652230        0  1900   NA 0.06462444       &gt;5%\n2        Albania  3029278   28748    20510 11900  3.3 0.62613051      &gt;35%\n3        Algeria 39542166 2381741  1420000 14500  4.3 0.04771929      &gt;15%\n4 American Samoa    54343     199        0 13000   NA 1.21105528      &lt;NA&gt;\n5        Andorra    85580     468       NA 37200   NA 0.68376068      &gt;60%\n6         Angola 19625353 1246700  1742000  7300  3.5 0.04125211      &gt;15%\n\nCIACountries |&gt;\n  select(country, oil_prod) |&gt;\n  mutate(oil_prod_disc = cut(oil_prod, \n                             breaks = c(0, 1e3, 1e5, 1e6, 1e7, 1e8), \n                             labels = c(\"&gt;1000\", \"&gt;10,000\", \"&gt;100,000\", \"&gt;1 million\", \"&gt;10 million\"))) |&gt;\n  # we won't use mWorldMap often, but it's a good quick illustration\n  mosaic::mWorldMap(key = \"country\") +\n  geom_polygon(aes(fill = oil_prod_disc)) + \n  scale_fill_brewer(\"Oil Prod. (bbl/day)\", na.value = \"white\") +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\nChoropleth Maps\nWhen you have specific regions (e.g. countries, states, counties, census tracts,…) and a value associated with each region.\nA choropleth map will color the entire region according to the value. For example, let’s consider state vaccination data from March 2021.\n\nvaccines &lt;- read_csv(\"https://proback.github.io/264_fall_2024/Data/vacc_Mar21.csv\") \n\nvacc_mar13 &lt;- vaccines |&gt;\n  filter(Date ==\"2021-03-13\") |&gt;\n  select(State, Date, people_vaccinated_per100, share_doses_used, Governor)\n\nvacc_mar13\n\n# A tibble: 50 × 5\n   State       Date       people_vaccinated_per100 share_doses_used Governor\n   &lt;chr&gt;       &lt;date&gt;                        &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;   \n 1 Alabama     2021-03-13                     17.2            0.671 R       \n 2 Alaska      2021-03-13                     27.0            0.686 R       \n 3 Arizona     2021-03-13                     21.5            0.821 R       \n 4 Arkansas    2021-03-13                     19.2            0.705 R       \n 5 California  2021-03-13                     20.3            0.726 D       \n 6 Colorado    2021-03-13                     20.8            0.801 D       \n 7 Connecticut 2021-03-13                     26.2            0.851 D       \n 8 Delaware    2021-03-13                     20.2            0.753 D       \n 9 Florida     2021-03-13                     20.1            0.766 R       \n10 Georgia     2021-03-13                     15.2            0.674 R       \n# ℹ 40 more rows\n\n\nThe tricky part of choropleth maps is getting the shapes (polygons) that make up the regions. This is really a pretty complex set of lines for R to draw!\nLuckily, some maps are already created in R in the maps package.\n\nlibrary(maps)\nus_states &lt;- map_data(\"state\")\nhead(us_states)\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\n# Note that points in the same \"group\" are connected with a line\n\nus_states |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(fill = \"white\", color = \"black\")\n\n\n\n\nOther maps provided by the maps package include US counties, France, Italy, New Zealand, and two different views of the world. If you want maps of other countries or regions, you can often find them online.\nSometimes maps may be provided as shapefiles. To use these, you’ll first need to read them into R and then turn them into tidy dataframes in order to use them with ggplot. See here:. More on shapefiles in Part 2.\nWhere the really cool stuff happens is when we join our data to the us_states dataframe. Notice that the state name appears in the “region” column of us_states, and that the state name is in all small letters. In vacc_mar13, the state name appears in the State column and is in title case. Thus, we have to be very careful when we join the state vaccine info to the state geography data.\nRun this line by line to see what it does:\n\nvacc_mar13 &lt;- vacc_mar13 |&gt;\n  mutate(State = str_to_lower(State))\n\nvacc_mar13 |&gt;\n  right_join(us_states, by = c(\"State\" = \"region\")) |&gt;\n  rename(region = State) |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(aes(fill = people_vaccinated_per100), color = \"black\")\n\n\n\n\noops, New York appears to be a problem.\n\nvacc_mar13 |&gt;\n  anti_join(us_states, by = c(\"State\" = \"region\"))\n\n# A tibble: 3 × 5\n  State          Date       people_vaccinated_per100 share_doses_used Governor\n  &lt;chr&gt;          &lt;date&gt;                        &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;   \n1 alaska         2021-03-13                     27.0            0.686 R       \n2 hawaii         2021-03-13                     22.8            0.759 D       \n3 new york state 2021-03-13                     21.7            0.764 D       \n\nus_states |&gt;\n  anti_join(vacc_mar13, by = c(\"region\" = \"State\")) |&gt;\n  count(region)\n\n                region   n\n1 district of columbia  10\n2             new york 495\n\n\n[Pause to ponder:] What did we learn by running anti_join() above?\nNotice that the us_states map also includes only the contiguous 48 states. This gives an example of creating really beautiful map insets for Alaska and Hawaii.\n\nvacc_mar13 &lt;- vacc_mar13 |&gt;\n  mutate(State = str_replace(State, \" state\", \"\"))\n\nvacc_mar13 |&gt;\n  anti_join(us_states, by = c(\"State\" = \"region\"))\n\n# A tibble: 2 × 5\n  State  Date       people_vaccinated_per100 share_doses_used Governor\n  &lt;chr&gt;  &lt;date&gt;                        &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;   \n1 alaska 2021-03-13                     27.0            0.686 R       \n2 hawaii 2021-03-13                     22.8            0.759 D       \n\nus_states |&gt;\n  anti_join(vacc_mar13, by = c(\"region\" = \"State\")) %&gt;%\n  count(region)\n\n                region  n\n1 district of columbia 10\n\n\nBetter.\n\nlibrary(viridis) # for color schemes\nvacc_mar13 |&gt;\n  right_join(us_states, by = c(\"State\" = \"region\")) |&gt;\n  rename(region = State) |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(aes(fill = people_vaccinated_per100), color = \"black\") + \n  labs(fill = \"People Vaccinated\\nper 100 pop.\") +\n  # This scales the longitude and latitude so that the shapes look correct.\n  coord_map() + \n  # This theme can give you a really clean look!\n  theme_void() +  \n  # you can change the fill scale for different color schemes.\n  scale_fill_viridis() \n\n\n\n\n[Pause to ponder:] Use autofill to play with different themes and scale_fills.\nNote: Map projections are actually pretty complicated, especially if you’re looking at large areas (e.g. world maps). It’s impossible to preserve both shape and area when projecting a sphere onto a flat surface, so that’s why you sometimes see such different maps of the world\nThere are a few different options in coord_map(). See the help menu, although this function is being phased out.\nYou can also use a categorical variable to color regions:\n\nvacc_mar13 |&gt;\n  right_join(us_states, by = c(\"State\" = \"region\")) |&gt;\n  rename(region = State) |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(aes(fill = Governor), color = \"darkgrey\", linewidth = 0.2) + \n  labs(fill = \"Governor\") +\n  # This scales the longitude and latitude so that the shapes look correct.\n  coord_map() + \n  # This theme can give you a really clean look!\n  theme_void() +  \n  # you can change the fill scale for different color schemes.\n  scale_fill_manual(values = c(\"blue\", \"red\")) \n\n\n\n\n\n\nMultiple maps!\n[Pause to ponder:] are we bothered by the warning about many-to-many when you run the code below?\n\nlibrary(lubridate)\nweekly_vacc &lt;- vaccines |&gt;\n  mutate(State = str_to_lower(State)) |&gt;\n  mutate(State = str_replace(State, \" state\", \"\"),\n         week = week(Date)) |&gt;\n  group_by(week, State) |&gt;\n  summarize(date = first(Date),\n            mean_daily_vacc = mean(daily_vaccinated/est_population*1000)) |&gt;\n  right_join(us_states, by =c(\"State\" = \"region\")) |&gt;\n  rename(region = State)\n\nweekly_vacc |&gt;\n  filter(week &gt; 2, week &lt; 11) |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(aes(fill = mean_daily_vacc), color = \"darkgrey\", size = 0.1) + \n  labs(fill = \"Weekly Average Daily Vaccinations per 1000\") +\n  coord_map() + \n  theme_void() + \n  scale_fill_viridis() + \n  facet_wrap(~date) + \n  theme(legend.position = \"bottom\") \n\n\n\n\n\n\nOther cool state maps\n\nstatebin (square representation of states)\n\nlibrary(statebins) # may need to install\n\nvacc_mar13 |&gt;\n  mutate(State = str_to_title(State)) |&gt;\n  statebins(state_col = \"State\",\n            value_col = \"people_vaccinated_per100\") + \n  # one nice layout. You can customize with usual ggplot themes.\n  theme_statebins() + \n  labs(fill = \"People Vaccinated per 100\")\n\n\n\n\n[Pause to ponder:] Why might one use a map like above instead of our previous choropleth maps?\nI used this example to create the code above. The original graph is located here.\n\n\n\nInteractive map with leaflet\nLeaflet is a powerful open-source JavaScript library for building interactive maps in HTML. Although the commands are different, the architecture is very similar to ggplot2. However, instead of putting data-based layers on top of a static map, leaflet allows you to put data-based layers on top of an interactive map. Because leaflet renders as HTML to allow interactivity, they are less effective as static pdfs.\nWith leaflet, you can have “pop-up” messages when you hover over points, and have a zoom-in and zoom-out option.\nTwo main features: addTiles() = Add background map setView() = Set where the map should originally zoom to\n\n# This part is for the pop-up messages.... some are weird or just \"\\n\" for example, so this turns the weird stuff to blanks. We could also probably do this with str_ functions.\nEncoding( x = airbnb.df$AboutListing ) &lt;- \"UTF-8\"\nairbnb.df$AboutListing &lt;-\n  iconv( x = airbnb.df$AboutListing\n         , from = \"UTF-8\"\n         , to = \"UTF-8\"\n         , sub = \"\" )\nhead(airbnb.df)\n\n# A tibble: 6 × 65\n  ListingID Title    UserID baseurl Price AboutListing HostName MemberDate   Lat\n      &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt;\n1    281552 Harvard… 1.47e6 https:…   175 \"\\n\"         Mary Ca… December …  42.4\n2    182613 Luxury … 8.76e5 https:…   249 \"Entire lar… Max      July 2011   42.4\n3   1587540 Cozy Ho… 2.00e6 https:…   225 \"\\n\"         Finola   March 2012  42.4\n4    469506 Luxury … 1.77e6 https:…   140 \"\\n\"         Rupal    February …  42.3\n5   3937268 Boston … 2.53e6 https:…    99 \"I offer a … Natasha  June 2012   42.3\n6   3036349 Top flo… 1.37e6 https:…    89 \"Two bedroo… Carol    November …  42.3\n# ℹ 56 more variables: Long &lt;dbl&gt;, BookInstantly &lt;chr&gt;, Cancellation &lt;chr&gt;,\n#   PageCounter &lt;dbl&gt;, PageNumber &lt;dbl&gt;, A_AC &lt;dbl&gt;, A_Breakfast &lt;dbl&gt;,\n#   A_CableTV &lt;dbl&gt;, A_CarbonMonoxDetector &lt;dbl&gt;, A_Doorman &lt;dbl&gt;,\n#   A_Dryer &lt;dbl&gt;, A_TV &lt;dbl&gt;, A_Elevator &lt;dbl&gt;, A_Essentials &lt;dbl&gt;,\n#   A_Events &lt;dbl&gt;, A_FamilyFriendly &lt;dbl&gt;, A_FireExt &lt;dbl&gt;, A_Fireplace &lt;dbl&gt;,\n#   A_FirstAidKit &lt;dbl&gt;, A_Gym &lt;dbl&gt;, A_Heat &lt;dbl&gt;, A_HotTub &lt;dbl&gt;,\n#   A_Intercom &lt;dbl&gt;, A_Internet &lt;dbl&gt;, A_Kitchen &lt;dbl&gt;, A_Parking &lt;dbl&gt;, …\n\n# This part makes the map!\nleaflet() |&gt;\n    addTiles() |&gt;\n    setView(lng = mean(airbnb.df$Long), lat = mean(airbnb.df$Lat), \n            zoom = 13) |&gt; \n    addCircleMarkers(data = airbnb.df,\n        lat = ~ Lat, \n        lng = ~ Long, \n        popup = ~ AboutListing, \n        radius = ~ S_Accomodates,  \n        # These last options describe how the circles look\n        weight = 2,\n        color = \"red\", \n        fillColor = \"yellow\")\n\n\n\n\n\n\n\n\nOn Your Own\nThe states dataset in the poliscidata package contains 135 variables on each of the 50 US states. See here for more detail.\nYour task is to create a two meaningful choropleth plots, one using a numeric variable and one using a categorical variable from states. Write a sentence or two describing what you can learn from each plot.\nHere’s some R code to get you going:\n\nlibrary(poliscidata)   # may have to install first\n\n# Be sure you know what the mutate statement below is doing!\nstate_data &lt;- as_tibble(poliscidata::states) |&gt;\n  mutate(state_name = str_squish(str_to_lower(as.character(state)))) |&gt;\n  select(-state)\nprint(state_data, n = 5, width = Inf)\n\n# A tibble: 50 × 135\n  abort_rank3 abortion_rank12 adv_or_more ba_or_more cig_tax12 cig_tax12_3\n  &lt;fct&gt;                 &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;      \n1 Less restr               35         9         26.6     2     HiTax      \n2 Mid                      20         7.7       22       0.425 LoTax      \n3 More restr                4         6.1       18.9     1.15  MidTax     \n4 More restr                5         9.3       25.6     2     HiTax      \n5 Less restr               49        10.7       29.9     0.87  MidTax     \n  conserv_advantage conserv_public dem_advantage govt_worker gun_rank3 \n              &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;     \n1              21.3           43.1         -12.2        28   Less restr\n2              36             44.7         -14.6        17.5 Mid       \n3              26.7           45.2          -1.4        17.6 Less restr\n4              19.5           36            -3.5        15.5 Less restr\n5               6.3           30.8          14.9        14.9 More restr\n  gun_rank11 gun_scale11 hr_cons_rank11 hr_conserv11 hr_lib_rank11 hr_liberal11\n       &lt;dbl&gt;       &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1         50           0           200          55.7          228          44.3\n2         17          14           152.         65.6          278.         34.4\n3         39           4           132.         69.3          295.         30.7\n4         50           0           156.         62.6          270.         37.4\n5          1          81           274.         54.8          152.         81.0\n  hs_or_more obama2012 obama_win12  pop2000  pop2010 pop2010_hun_thou\n       &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt;    &lt;dbl&gt;            &lt;dbl&gt;\n1       91.4      40.8 No            626932   710231             7.10\n2       82.1      38.4 No           4447100  4779736            47.8 \n3       82.4      36.9 No           2673400  2915918            29.2 \n4       84.2      44.4 No           5130632  6392017            63.9 \n5       80.6      60.2 Yes         33871648 37253956           373.  \n  popchng0010 popchngpct pot_policy          prochoice prolife relig_cath\n        &lt;dbl&gt;      &lt;dbl&gt; &lt;fct&gt;                   &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n1       83299       13.3 Medicinal                  58      37       14.6\n2      332636        7.5 Decrim                     36      54        6.6\n3      242518        9.1 Not legal                  40      55        5.9\n4     1261385       24.6 Medicinal                  56      39       27.3\n5     3382308       10   Medicinal / Decrim.        65      28       31.9\n  relig_prot relig_high relig_low religiosity3 romney2012 smokers12 stateid \n       &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;             &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;   \n1       50         31.3      39.5 Low                54.8        24 \"AK    \"\n2       79.3       55.7      14.3 High               60.6        25 \"AL    \"\n3       78.6       52.3      18.7 High               60.6        26 \"AR    \"\n4       43.3       36.6      33.9 Mid                53.5        21 \"AZ    \"\n5       37.8       34.5      36.6 Low                37.1        15 \"CA    \"\n  to_0812 uninsured_pct abort_rate05 abort_rate08 abortlaw3  abortlaw10 alcohol\n    &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt; &lt;fct&gt;           &lt;dbl&gt;   &lt;dbl&gt;\n1   -9.40          21.8         13.6         12   0-5 restr           5    3.02\n2   -2.90          18.8         11.9         12   6-8 restr           8    2.01\n3   -2.90          21.9          8.3          8.7 9-10 restr          9    1.83\n4   -3.10          20.5         16           15.2 6-8 restr           6    2.31\n5   -6.5           23.2         27.1         27.6 0-5 restr           4    2.33\n  attend_pct battle04 blkleg blkpct04 blkpct08 blkpct10 bush00 bush04 carfatal\n       &lt;dbl&gt; &lt;fct&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n1         22 No            2      3.6      4.3      4.7   58.6   61.1     17.4\n2         52 No           25     26.4     26.4     26.8   56.5   62.5     24.9\n3         50 No           11     15.8     15.8     16.1   51.3   54.3     25.6\n4         29 No            1      3.5      4.2      5     51.0   54.8     20.3\n5         33 No            5      6.8      6.7      7.2   41.7   44.4     12.1\n  carfatal07 cig_tax cig_tax_3   cigarettes college conpct_m cons_hr06 cons_hr09\n       &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;            &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1       15.2   2     $1.41-$2.58       6.22    26.7     36.3      72        75  \n2       25.9   0.425 $.07-$.64         9.41    21.1     40.7      77.7      72  \n3       23.7   0.59  $.07-$.64         8.51    19.1     38.9      56.2      28.5\n4       17.6   2     $1.41-$2.58       2.4     24.3     33.3      69        49.5\n5       11.7   0.87  $.695-$1.36       3.69    29.1     28.5      37.3      35.1\n  cook_index cook_index3 defexpen demhr11 dem_hr09 demnat06 dempct_m demstate06\n       &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n1      -13.4 More Rep        3556     0        0        0       26.1       43.3\n2      -13.2 More Rep        1757    14.3     42.9     22.2     38.9       60.7\n3       -8.8 More Rep         530    25       75       83.3     43.1       75.6\n4       -6.1 Even            1771    28.6     62.5     20       31.9       44.4\n5        7.4 More Dem        1106    64.2     64.2     63.6     41.3       60.8\n  demstate09 demstate13 density division      earmarks_pcap   evm   evo evo2012\n       &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;                 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1       46.7       36.7     1.2 Pacific               426.      3     0       0\n2       57.9       35.7    94.4 E. South Cent          38.9     9     0       0\n3       72.6       46.7    56   W. South Cent          26.7     6     0       0\n4       41.1       41.1    56.3 Mountain               20.9    10     0       0\n5       64.2       66.7   239.  Pacific                12.5     0    55      55\n  evr2012 gay_policy        gay_policy2  gay_policy_con gay_support gay_support3\n    &lt;dbl&gt; &lt;fct&gt;             &lt;fct&gt;        &lt;fct&gt;                &lt;dbl&gt; &lt;fct&gt;       \n1       3 Conservative      Conservative No                      56 Med         \n2       9 Most conservative Conservative Yes                     44 Low         \n3       6 Most conservative Conservative Yes                     44 Low         \n4      11 Conservative      Conservative No                      58 Med         \n5       0 Liberal           Liberal      No                      64 High        \n  gb_win00 gb_win04  gore00 gun_check gun_dealer gun_murder10 gun_rank_rev\n  &lt;fct&gt;    &lt;fct&gt;      &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 Bush win Bush win    27.7    12016.      139.           2.7            6\n2 Bush win Bush win    41.6     9025.       47.4          2.8           30\n3 Bush win Bush win    45.9     8443.       67.4          3.2           13\n4 Bush win Bush win    44.7     5314.       45.4          3.6           13\n5 Gore win Kerry win   53.4     3040.       21.6          3.4           48\n  gunlaw_rank gunlaw_rank3_rev gunlaw_scale hispanic04 hispanic08 hispanic10\n        &lt;dbl&gt; &lt;fct&gt;                   &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1          43 Fewer restr                 4        4.9        6.1        5.5\n2          19 Mid                        15        2.2        2.9        3.9\n3          36 Fewer restr                 6        4.4        5.6        6.4\n4          36 Fewer restr                 6       28         30.1       29.6\n5           1 More restr                 79       34.7       36.6       37.6\n  indpct_m kerry04 libpct_m mccain08 modpct_m nader00 obama08 obama_win08 over64\n     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;        &lt;dbl&gt;\n1     43.6    35.5     17.9     59.4     45.7   10.1     37.9 McCain win     6.4\n2     30.0    36.8     16.8     60.3     42.5    1.10    38.7 McCain win    13.2\n3     35.9    44.6     16.8     58.7     44.3    1.46    38.9 McCain win    13.8\n4     29.7    44.4     19.2     53.4     47.4    2.98    44.9 McCain win    12.7\n5     25.8    54.3     24.2     36.9     47.3    3.82    60.9 Obama win     10.7\n  permit pop_18_24 pop_18_24_10 prcapinc region relig_import religiosity\n   &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1   NA       11.1         10.6     34454 West           NA          -177\n2   27.6     10.0         10.0     27795 South          58.5         -13\n3   21.1     10.1          9.74    25725 South          53.1         -23\n4   46.2      9.61         9.90    28442 West           33.2        -140\n5   52.8      9.95        10.5     35019 West           28.8        -147\n  reppct_m rtw   secularism secularism3 seniority_sen2 south    to_0004 to_0408\n     &lt;dbl&gt; &lt;fct&gt;      &lt;dbl&gt; &lt;fct&gt;       &lt;fct&gt;          &lt;fct&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n1     30.3 No           177 Secular     Yes            Nonsouth  -12.4   -0.800\n2     31.2 Yes           13 Religious   No             South       4.2    4.60 \n3     21.0 Yes           23 Religious   No             South       3.31  -0.200\n4     38.3 Yes          140 Secular     No             Nonsouth   -2.32   1.90 \n5     32.9 No           147 Secular     No             Nonsouth    6.63   2.90 \n  trnout00 trnout04 unemploy union04 union07 union10 urban vep00_turnout\n     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;\n1     67.2     54.8      7.5    20.1    23.8    22.3  65.6          68.1\n2     50.6     54.8      5.8     9.7     9.5    10.9  55.4          51.6\n3     46.9     50.2      5.9     4.8     5.4     4.2  52.5          47.9\n4     44.6     42.3      5.1     6.3     8.8     6.5  88.2          45.6\n5     54.6     61.2      6.2    16.5    16.7    17.2  94.4          55.7\n  vep04_turnout vep08_turnout vep12_turnout womleg_2007 womleg_2010 womleg_2011\n          &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1          69.1          68.3          58.9        21.7        21.7        23.3\n2          57.2          61.8          58.9        12.9        12.9        13.6\n3          53.6          53.4          50.5        20.7        23          22.2\n4          54.1          56            52.9        34.4        31.1        34.4\n5          58.8          61.7          55.2        28.3        27.5        28.3\n  womleg_2015 state_name\n        &lt;dbl&gt; &lt;chr&gt;     \n1        28.3 alaska    \n2        14.3 alabama   \n3        20   arkansas  \n4        35.6 arizona   \n5        25.8 california\n# ℹ 45 more rows"
  },
  {
    "objectID": "04_functions.html",
    "href": "04_functions.html",
    "title": "Functions and tidy evaluation",
    "section": "",
    "text": "Based on Chapter 25 from R for Data Science\nYou can download this .qmd file from here. Just hit the Download Raw File button."
  },
  {
    "objectID": "04_functions.html#vector-functions",
    "href": "04_functions.html#vector-functions",
    "title": "Functions and tidy evaluation",
    "section": "Vector functions",
    "text": "Vector functions\n\nExample 1: Rescale variables from 0 to 1.\nThis code creates a 10 x 4 tibble filled with random values taken from a normal distribution with mean 0 and SD 1\n\ndf &lt;- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\ndf\n\n# A tibble: 10 × 4\n        a       b       c        d\n    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1 -0.530  0.607   0.471   0.356  \n 2  2.09  -0.306   0.703  -0.00167\n 3 -1.21  -0.677   0.663  -0.452  \n 4  0.620 -2.03   -0.674  -0.733  \n 5  0.852 -0.322  -1.55   -1.14   \n 6 -0.558  0.365  -0.0722  0.0268 \n 7 -0.267  0.760  -0.676   0.860  \n 8 -1.43   0.380  -0.462   2.66   \n 9  0.573 -0.865  -0.101  -0.630  \n10  0.271 -0.0565  0.0265  0.446  \n\n\nThis code below for rescaling variables from 0 to 1 is ripe for functions… we did it four times!\nIt’s easiest to start with working code and turn it into a function.\n\ndf$a &lt;- (df$a - min(df$a)) / (max(df$a) - min(df$a))\ndf$b &lt;- (df$b - min(df$b)) / (max(df$b) - min(df$b))\ndf$c &lt;- (df$c - min(df$c)) / (max(df$c) - min(df$c))\ndf$d &lt;- (df$d - min(df$d)) / (max(df$d) - min(df$d))\ndf\n\n# A tibble: 10 × 4\n        a     b     c     d\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 0.255  0.945 0.897 0.394\n 2 1      0.618 1     0.300\n 3 0.0618 0.485 0.982 0.182\n 4 0.582  0     0.388 0.108\n 5 0.648  0.612 0     0    \n 6 0.247  0.858 0.655 0.308\n 7 0.330  1     0.387 0.527\n 8 0      0.864 0.482 1    \n 9 0.569  0.417 0.643 0.135\n10 0.483  0.707 0.699 0.418\n\n\nNotice first what changes and what stays the same in each line. Then, if we look at the first line above, we see we have one value we’re using over and over: df$a. So our function will have one input. We’ll start with our code from that line, then replace the input (df$a) with x. We should give our function a name that explains what it does. The name should be a verb.\n\n# I'm going to show you how to write the function in class! \n# I have it in the code already below, but don't look yet!\n# Let's try to write it together first!\n\n. . . . . . . . .\n\n# Our function (first draft!)\nrescale01 &lt;- function(x) {\n  (x - min(x)) / (max(x) - min(x))\n}\n\nNote the general form of a function:\n\nname &lt;- function(arguments) {\n  body\n}\n\nEvery function contains 3 essential components:\n\nA name. The name should clearly evoke what the function does; hence, it is often a verb (action). Here we’ll use rescale01 because this function rescales a vector to lie between 0 and 1. snake_case is good; CamelCase is just okay.\nThe arguments. The arguments are things that vary across calls and they are usually nouns - first the data, then other details. Our analysis above tells us that we have just one; we’ll call it x because this is the conventional name for a numeric vector, but you can use any word.\nThe body. The body is the code that’s repeated across all the calls. By default a function will return the last statement; use return() to specify a return value\n\nSummary: Functions should be written for both humans and computers!\nOnce we have written a function we like, then we need to test it with different inputs!\n\ntemp &lt;- c(4, 6, 8, 9)\nrescale01(temp)\n\n[1] 0.0 0.4 0.8 1.0\n\ntemp0 &lt;- c(4, 6, 8, 9, NA)\nrescale01(temp0)\n\n[1] NA NA NA NA NA\n\n\nOK, so NA’s don’t work the way we want them to.\n\nrescale01 &lt;- function(x) {\n  (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))\n}\nrescale01(temp)\n\n[1] 0.0 0.4 0.8 1.0\n\nrescale01(temp0)\n\n[1] 0.0 0.4 0.8 1.0  NA\n\n\nWe can continue to improve our function. Here is another method, which uses the existing range function within R to avoid 3 max/min executions:\n\nrescale01 &lt;- function(x) {\n  rng &lt;- range(x, na.rm = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\nrescale01(temp)\n\n[1] 0.0 0.4 0.8 1.0\n\nrescale01(c(0, 5, 10))\n\n[1] 0.0 0.5 1.0\n\nrescale01(c(-10, 0, 10))\n\n[1] 0.0 0.5 1.0\n\nrescale01(c(1, 2, 3, NA, 5))\n\n[1] 0.00 0.25 0.50   NA 1.00\n\n\nWe should continue testing unusual inputs. Think carefully about how you want this function to behave… the current behavior is to include the Inf (infinity) value when calculating the range. You get strange output everywhere, but it’s pretty clear that there is a problem right away when you use the function. In the example below (rescale1), you ignore the infinity value when calculating the range. The function returns Inf for one value, and sensible stuff for the rest. In many cases this may be useful, but it could also hide a problem until you get deeper into an analysis.\n\nx &lt;- c(1:10, Inf)\nrescale01(x)\n\n [1]   0   0   0   0   0   0   0   0   0   0 NaN\n\nrescale1 &lt;- function(x) {\n  rng &lt;- range(x, na.rm = TRUE, finite = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\nrescale1(x)\n\n [1] 0.0000000 0.1111111 0.2222222 0.3333333 0.4444444 0.5555556 0.6666667\n [8] 0.7777778 0.8888889 1.0000000       Inf\n\n\nNow we’ve used functions to simplify original example. We will learn to simplify further in iterations (Ch 26)\n\ndf &lt;- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n# add a little noise\ndf$a[5] = NA\ndf$b[6] = Inf\ndf\n\n# A tibble: 10 × 4\n         a       b      c       d\n     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1  0.136   -3.09   2.43  -0.637 \n 2 -0.0792   1.80  -0.749 -0.394 \n 3  0.0655  -1.46  -0.259 -0.922 \n 4 -1.37     0.684  0.167 -0.207 \n 5 NA        0.500 -2.16  -1.41  \n 6 -1.29   Inf      1.85  -0.0476\n 7  0.740    3.31   1.37   1.63  \n 8 -1.23    -0.637  0.556 -0.0265\n 9 -1.01    -0.162  0.187 -0.508 \n10 -1.18     0.108  2.08   1.48  \n\ndf$a_new &lt;- rescale1(df$a)\ndf$b_new &lt;- rescale1(df$b)\ndf$c_new &lt;- rescale1(df$c)\ndf$d_new &lt;- rescale1(df$d)\ndf\n\n# A tibble: 10 × 8\n         a       b      c       d   a_new   b_new c_new d_new\n     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.136   -3.09   2.43  -0.637   0.714    0     1     0.254\n 2 -0.0792   1.80  -0.749 -0.394   0.612    0.764 0.308 0.334\n 3  0.0655  -1.46  -0.259 -0.922   0.681    0.255 0.415 0.160\n 4 -1.37     0.684  0.167 -0.207   0        0.590 0.507 0.396\n 5 NA        0.500 -2.16  -1.41   NA        0.561 0     0    \n 6 -1.29   Inf      1.85  -0.0476  0.0370 Inf     0.875 0.448\n 7  0.740    3.31   1.37   1.63    1        1     0.771 1    \n 8 -1.23    -0.637  0.556 -0.0265  0.0696   0.383 0.592 0.455\n 9 -1.01    -0.162  0.187 -0.508   0.173    0.458 0.512 0.297\n10 -1.18     0.108  2.08   1.48    0.0918   0.500 0.924 0.952\n\ndf %&gt;% \n  mutate(a_new = rescale1(a),\n         b_new = rescale1(b),\n         c_new = rescale1(c),\n         d_new = rescale1(d))\n\n# A tibble: 10 × 8\n         a       b      c       d   a_new   b_new c_new d_new\n     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.136   -3.09   2.43  -0.637   0.714    0     1     0.254\n 2 -0.0792   1.80  -0.749 -0.394   0.612    0.764 0.308 0.334\n 3  0.0655  -1.46  -0.259 -0.922   0.681    0.255 0.415 0.160\n 4 -1.37     0.684  0.167 -0.207   0        0.590 0.507 0.396\n 5 NA        0.500 -2.16  -1.41   NA        0.561 0     0    \n 6 -1.29   Inf      1.85  -0.0476  0.0370 Inf     0.875 0.448\n 7  0.740    3.31   1.37   1.63    1        1     0.771 1    \n 8 -1.23    -0.637  0.556 -0.0265  0.0696   0.383 0.592 0.455\n 9 -1.01    -0.162  0.187 -0.508   0.173    0.458 0.512 0.297\n10 -1.18     0.108  2.08   1.48    0.0918   0.500 0.924 0.952\n\n# Even better - from Chapter 26\ndf |&gt; mutate(across(a:d, rescale1))\n\n# A tibble: 10 × 8\n         a       b     c     d   a_new   b_new c_new d_new\n     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.714    0     1     0.254  0.714    0     1     0.254\n 2  0.612    0.764 0.308 0.334  0.612    0.764 0.308 0.334\n 3  0.681    0.255 0.415 0.160  0.681    0.255 0.415 0.160\n 4  0        0.590 0.507 0.396  0        0.590 0.507 0.396\n 5 NA        0.561 0     0     NA        0.561 0     0    \n 6  0.0370 Inf     0.875 0.448  0.0370 Inf     0.875 0.448\n 7  1        1     0.771 1      1        1     0.771 1    \n 8  0.0696   0.383 0.592 0.455  0.0696   0.383 0.592 0.455\n 9  0.173    0.458 0.512 0.297  0.173    0.458 0.512 0.297\n10  0.0918   0.500 0.924 0.952  0.0918   0.500 0.924 0.952\n\n\n\n\nOptions for handling NAs in functions\nBefore we try some practice problems, let’s consider various options for handling NAs in functions. We used the na.rm option within functions like min, max, and range in order to take care of missing values. But there are alternative approaches:\n\nfilter/remove the NA values before rescaling\ncreate an if statement to check if there are NAs; return an error if NAs exist\ncreate a removeNAs option in the function we are creating\n\nLet’s take a look at each alternative approach in turn:\n\nFilter/remove the NA values before rescaling\n\ndf &lt;- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\ndf$a[5] = NA\ndf\n\n# A tibble: 10 × 4\n        a      b        c       d\n    &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1  0.375  1.06  -0.946    0.0249\n 2 -0.243 -1.48  -0.847   -2.12  \n 3  0.433 -1.01   0.00881  0.840 \n 4  0.863 -0.899 -2.44    -2.82  \n 5 NA     -0.121 -0.0760   2.02  \n 6  0.904  0.253 -0.0769   0.184 \n 7  0.340  1.07  -0.591    1.31  \n 8  0.157 -1.16   1.08    -0.609 \n 9  0.218  0.557 -0.217   -3.42  \n10 -0.552 -0.898  0.309    0.0726\n\nrescale_basic &lt;- function(x) {\n  (x - min(x)) / (max(x) - min(x))\n}\n\ndf %&gt;%\n  filter(!is.na(a)) %&gt;%\n  mutate(new_a = rescale_basic(a))\n\n# A tibble: 9 × 5\n       a      b        c       d new_a\n   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1  0.375  1.06  -0.946    0.0249 0.637\n2 -0.243 -1.48  -0.847   -2.12   0.212\n3  0.433 -1.01   0.00881  0.840  0.677\n4  0.863 -0.899 -2.44    -2.82   0.972\n5  0.904  0.253 -0.0769   0.184  1    \n6  0.340  1.07  -0.591    1.31   0.613\n7  0.157 -1.16   1.08    -0.609  0.487\n8  0.218  0.557 -0.217   -3.42   0.529\n9 -0.552 -0.898  0.309    0.0726 0    \n\n\n[Pause to Ponder:] Do you notice anything in the output above that gives you pause?\n\n\nCreate an if statement to check if there are NAs; return an error if NAs exist\nFirst, here’s an example involving weighted means:\n\n# Create function to calculate weighted mean\nwt_mean &lt;- function(x, w) {\n  sum(x * w) / sum(w)\n}\nwt_mean(c(1, 10), c(1/3, 2/3))\n\n[1] 7\n\nwt_mean(1:6, 1:3)\n\n[1] 7.666667\n\n\n[Pause to Ponder:] Why is the answer to the last call above 7.67??\n\n# update function to handle cases where data and weights of unequal length\nwt_mean &lt;- function(x, w) {\n  if (length(x) != length(w)) {\n    stop(\"`x` and `w` must be the same length\", call. = FALSE)\n  } else {\n  sum(w * x) / sum(w)\n  }  \n}\nwt_mean(1:6, 1:3) \n\nError: `x` and `w` must be the same length\n\n# should produce an error now if weights and data different lengths\n#  - nice example of if and else\n\n[Pause to Ponder:] What does the call. option do?\nNow let’s apply this to our rescaling function\n\nrescale_w_error &lt;- function(x) {\n  if (is.na(sum(x))) {\n    stop(\"`x` cannot have NAs\", call. = FALSE)\n  } else {\n  (x - min(x)) / (max(x) - min(x))\n  }  \n}\n\ntemp &lt;- c(4, 6, 8, 9)\nrescale_w_error(temp)\n\n[1] 0.0 0.4 0.8 1.0\n\ntemp &lt;- c(4, 6, 8, 9, NA)\nrescale_w_error(temp)\n\nError: `x` cannot have NAs\n\n\n[Pause to Ponder:] Why can’t we just use if (is.na(x)) instead of is.na(sum(x))?\n\n\nCreate a removeNAs option in the function we are creating\n\nrescale_NAoption &lt;- function(x, removeNAs = FALSE) {\n  (x - min(x, na.rm = removeNAs)) / \n    (max(x, na.rm = removeNAs) - min(x, na.rm = removeNAs))\n} \n\ntemp &lt;- c(4, 6, 8, 9)\nrescale_NAoption(temp)\n\n[1] 0.0 0.4 0.8 1.0\n\ntemp &lt;- c(4, 6, 8, 9, NA)\nrescale_NAoption(temp, removeNAs = TRUE)\n\n[1] 0.0 0.4 0.8 1.0  NA\n\n\nOK, but all the other summary stats functions use na.rm as the input, so to be consistent, it’s probably better to do something slightly awkward like this:\n\nrescale_NAoption &lt;- function(x, na.rm = FALSE) {\n  (x - min(x, na.rm = na.rm)) / \n    (max(x, na.rm = na.rm) - min(x, na.rm = na.rm))\n} \n\ntemp &lt;- c(4, 6, 8, 9, NA)\nrescale_NAoption(temp, na.rm = TRUE)\n\n[1] 0.0 0.4 0.8 1.0  NA\n\n\nwt_mean() is an example of a “summary function (single value output) instead of a”mutate function” (vector output) like rescale01(). Here’s another summary function to produce the mean absolute percentage error:\n\nmape &lt;- function(actual, predicted) {\n  sum(abs((actual - predicted) / actual)) / length(actual)\n}\n\ny &lt;- c(2,6,3,8,5)\nyhat &lt;- c(2.5, 5.1, 4.4, 7.8, 6.1)\nmape(actual = y, predicted = yhat)\n\n[1] 0.2223333"
  },
  {
    "objectID": "04_functions.html#data-frame-functions",
    "href": "04_functions.html#data-frame-functions",
    "title": "Functions and tidy evaluation",
    "section": "Data frame functions",
    "text": "Data frame functions\nThese work like dplyr verbs, taking a data frame as the first argument, and then returning a data frame or a vector.\n\nDemonstration of tidy evaluation in functions\n\n# Start with working code then functionize\nggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +\n  geom_point(size = 0.75) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\nmake_plot &lt;- function(dataset, xvar, yvar, pt_size = 0.75)  {\n  ggplot(data = dataset, mapping = aes(x = xvar, y = yvar)) +\n    geom_point(size = pt_size) +\n    geom_smooth()\n}\n\nmake_plot(dataset = mpg, xvar = cty, yvar = hwy)  # Error!\n\nError in `geom_point()`:\n! Problem while computing aesthetics.\nℹ Error occurred in the 1st layer.\nCaused by error:\n! object 'cty' not found\n\n\nThe problem is tidy evaluation, which makes most common coding easier, but makes some less common things harder. Key terms to understand tidy evaluation:\n\nenv-variables = live in the environment (mpg)\ndata-variables = live in data frame or tibble (cty)\ndata masking = tidyverse use data-variables as if env-variables. That is, you don’t always need mpg$cty to access cty in tidyverse\n\nThe key idea behind data masking is that it blurs the line between the two different meanings of the word “variable”:\n\nenv-variables are “programming” variables that live in an environment. They are usually created with &lt;-.\ndata-variables are “statistical” variables that live in a data frame. They usually come from data files (e.g. .csv, .xls), or are created manipulating existing variables.\n\nThe solution is to embrace {{ }} data-variables which are user inputs into functions. One way to remember what’s happening, as suggested by our book authors, is to think of {{ }} as looking down a tunnel — {{ var }} will make a dplyr function look inside of var rather than looking for a variable called var.\nSee Section 25.3 of R4DS for more details (and there are plenty!).\n\n# This will work to make our plot!\nmake_plot &lt;- function(dataset, xvar, yvar, pt_size = 0.75)  {\n  ggplot(data = dataset, mapping = aes(x = {{ xvar }}, y = {{ yvar }})) +\n    geom_point(size = pt_size) +\n    geom_smooth()\n}\n\nmake_plot(dataset = mpg, xvar = cty, yvar = hwy)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nI often wish it were easier to get my own custom summary statistics for numeric variables in EDA rather than using mosaic:favstats(). Using group_by() and summarise() from the tidyverse reads clearly but takes so many lines, but if I only had to write the code once…\n\nsummary6 &lt;- function(data, var) {\n  data |&gt; summarize(\n    min = min({{ var }}, na.rm = TRUE),\n    mean = mean({{ var }}, na.rm = TRUE),\n    median = median({{ var }}, na.rm = TRUE),\n    max = max({{ var }}, na.rm = TRUE),\n    n = n(),\n    n_miss = sum(is.na({{ var }})),\n    .groups = \"drop\"    # to leave the data in an ungrouped state\n  )\n}\n\nmpg |&gt; summary6(hwy)\n\n# A tibble: 1 × 6\n    min  mean median   max     n n_miss\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n1    12  23.4     24    44   234      0\n\n\nEven cooler, I can use my new function with group_by()!\n\nmpg |&gt; \n  group_by(drv) |&gt;\n  summary6(hwy)\n\n# A tibble: 3 × 7\n  drv     min  mean median   max     n n_miss\n  &lt;chr&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n1 4        12  19.2     18    28   103      0\n2 f        17  28.2     28    44   106      0\n3 r        15  21       21    26    25      0\n\n\nYou can even pass conditions into a function using the embrace:\n[Pause to Ponder:] Predict what the code below will do, then uncomment it and run it to check. Think about: why use sort = sort? why not embrace df? why didn’t we need n in the arguments?\n\n#new_function &lt;- function(df, var, condition, sort = TRUE) {\n#  df |&gt;\n#    filter({{ condition }}) |&gt;\n#    count({{ var }}, sort = sort) |&gt;\n#    mutate(prop = n / sum(n))\n#}\n\n#mpg |&gt; new_function(var = manufacturer, \n#                    condition = manufacturer %in% c(\"audi\", \"honda\", \"hyundai\", \"nissan\", \"subaru\", \"toyota\", \"volkswagen\"))\n\n\n\nData-masking vs. tidy-selection (Section 25.3.4)\nWhy doesn’t the following code work?\n\ncount_missing &lt;- function(df, group_vars, x_var) {\n  df |&gt; \n    group_by({{ group_vars }}) |&gt; \n    summarize(\n      n_miss = sum(is.na({{ x_var }})),\n      .groups = \"drop\"\n    )\n}\n\nflights |&gt; \n  count_missing(c(year, month, day), dep_time)\n\nError in `group_by()`:\nℹ In argument: `c(year, month, day)`.\nCaused by error:\n! `c(year, month, day)` must be size 336776 or 1, not 1010328.\n\n\nThe problem is that group_by() uses data-masking rather than tidy-selection. These are the two most common subtypes of tidy evaluation:\n\nData-masking is used in functions like arrange(), filter(), and summarize() that compute with variables. Data masking is an R feature that blends programming variables that live inside environments (env-variables) with statistical variables stored in data frames (data-variables).\n\nTidy-selection is used for functions like select(), relocate(), and rename() that select variables. Tidy selection provides a concise dialect of R for selecting variables based on their names or properties.\n\nMore detail can be found here.\nThe error above can be solved by using the pick() function:\n\ncount_missing &lt;- function(df, group_vars, x_var) {\n  df |&gt; \n    group_by(pick({{ group_vars }})) |&gt; \n    summarize(\n      n_miss = sum(is.na({{ x_var }})),\n      .groups = \"drop\"\n  )\n}\n\nflights |&gt; \n  count_missing(c(year, month, day), dep_time)\n\n# A tibble: 365 × 4\n    year month   day n_miss\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1  2013     1     1      4\n 2  2013     1     2      8\n 3  2013     1     3     10\n 4  2013     1     4      6\n 5  2013     1     5      3\n 6  2013     1     6      1\n 7  2013     1     7      3\n 8  2013     1     8      4\n 9  2013     1     9      5\n10  2013     1    10      3\n# ℹ 355 more rows\n\n\n[Pause to Ponder:] Here’s another nice use of pick(). Predict what the function will do, then uncomment the code to see if you are correct.\n\n# Source: https://twitter.com/pollicipes/status/1571606508944719876\n#new_function &lt;- function(data, rows, cols) {\n#  data |&gt; \n#    count(pick(c({{ rows }}, {{ cols }}))) |&gt; \n#    pivot_wider(\n#      names_from = {{ cols }}, \n#      values_from = n,\n#      names_sort = TRUE,\n#      values_fill = 0\n#    )\n#}\n\n#mpg |&gt; new_function(c(manufacturer, model), cyl)"
  },
  {
    "objectID": "04_functions.html#plot-functions",
    "href": "04_functions.html#plot-functions",
    "title": "Functions and tidy evaluation",
    "section": "Plot functions",
    "text": "Plot functions\nLet’s say you find yourself making a lot of histograms:\n\nflights |&gt; \n  ggplot(aes(x = dep_time)) +\n  geom_histogram(bins = 25)\n\n\n\nflights |&gt; \n  ggplot(aes(x = air_time)) +\n  geom_histogram(bins = 35)\n\n\n\n\nJust use embrace to create a histogram-making function\n\nhistogram &lt;- function(df, var, bins = NULL) {\n  df |&gt; \n    ggplot(aes(x = {{ var }})) + \n    geom_histogram(bins = bins)\n}\n\nflights |&gt; histogram(air_time, 35)\n\n\n\n\nSince histogram() returns a ggplot, you can add any layers you want\n\nflights |&gt; \n  histogram(air_time, 35) +\n  labs(x = \"Flight time (minutes)\", y = \"Number of flights\")\n\n\n\n\nYou can also combine data wrangling with plotting. Note that we need the “walrus operator” (:=) since the variable name on the left is being generated with user-supplied data.\n\n# sort counts with highest values at top and counts on x-axis\nsorted_bars &lt;- function(df, var) {\n  df |&gt; \n    mutate({{ var }} := fct_rev(fct_infreq({{ var }})))  |&gt;\n    ggplot(aes(y = {{ var }})) +\n    geom_bar()\n}\n\nflights |&gt; sorted_bars(carrier)\n\n\n\n\nFinally, it would be really helpful to label plots based on user inputs. This is a bit more complicated, but still do-able!\nFor this, we’ll need the rlang package. rlang is a low-level package that’s used by just about every other package in the tidyverse because it implements tidy evaluation (as well as many other useful tools).\nCheck out the following update of our histogram() function which uses the englue() function from the rlang package:\n\nhistogram &lt;- function(df, var, bins) {\n  label &lt;- rlang::englue(\"A histogram of {{var}} with binwidth {bins}\")\n  \n  df |&gt; \n    ggplot(aes(x = {{ var }})) + \n    geom_histogram(bins = bins) + \n    labs(title = label)\n}\n\nflights |&gt; histogram(air_time, 35)\n\n\n\n\n\nOn Your Own\n\nRewrite this code snippet as a function: x / sum(x, na.rm = TRUE). This code creates weights which sum to 1, where NA values are ignored. Test it for at least two different vectors. (Make sure at least one has NAs!)\nCreate a function to calculate the standard error of a variable, where SE = square root of the variance divided by the sample size. Hint: start with a vector like x &lt;- 0:5 or x &lt;- gss_cat$age and write code to find the SE of x, then turn it into a function to handle any vector x. Note: var is the function to find variance in R and sqrt does square root. length will also be handy. Test your function on two vectors that do not include NAs (i.e. do not worry about removing NAs at this point).\nUse your se function within summarize to get a table of the mean and s.e. of hwy and cty by class in the mpg dataset.\nUse your se function within summarize to get a table of the mean and s.e. of arr_delay and dep_delay by carrier in the flights dataset. Why does the output look like this?\nMake your se function handle NAs with an na.rm option. Test your new function (you can call it se again) on a vector that doesn’t include NA and on the same vector with an added NA. Be sure to check that it gives the expected output with na.rm = TRUE and na.rm = FALSE. Make na.rm = FALSE the default value. Repeat #4. (Hint: be sure when you divide by sample size you don’t count any NAs)\nCreate both_na(), a function that takes two vectors of the same length and returns how many positions have an NA in both vectors. Hint: create two vectors like test_x &lt;- c(1, 2, 3, NA, NA) and test_y &lt;- c(NA, 1, 2, 3, NA) and write code that works for test_x and test_y, then turn it into a function that can handle any x and y. (In this case, the answer would be 1, since both vectors have NA in the 5th position.) Test it for at least one more combination of x and y.\nRun your code from (6) with the following two vectors: test_x &lt;- c(1, 2, 3, NA, NA, NA) and test_y &lt;- c(NA, 1, 2, 3, NA). Did you get the output you wanted or expected? Modify your function using if, else, and stop to print an error if x and y are not the same length. Then test again with test_x, test_y and the sets of vectors you used in (6).\nHere is a way to get not_cancelled flights in the flights dataset:\n\n\nnot_cancelled &lt;- flights %&gt;% \n  filter(!is.na(dep_delay), !is.na(arr_delay))\n\nIs it necessary to check is.na for both departure and arrival? Using summarize, find the number of flights missing departure delay, arrival delay, and both. (Use your new function!)\n\nRead the code for each of the following three functions, puzzle out what they do, and then brainstorm better names.\n\n\nf1 &lt;- function(time1, time2) {\n  hour1 &lt;- time1 %/% 100\n  min1 &lt;- time1 %% 100\n  hour2 &lt;- time2 %/% 100\n  min2 &lt;- time2 %% 100\n  \n  (hour2 - hour1)*60 + (min2 - min1)\n}\n\n\nf2 &lt;- function(lengthcm, widthcm) {\n  (lengthcm / 2.54) * (widthcm / 2.54)\n}\n\n\nf3 &lt;- function(x) {\n  fct_collapse(x, \"non answer\" = c(\"No answer\", \"Refused\", \n                                   \"Don't know\", \"Not applicable\"))\n}\n\n\nExplain what the following function does and demonstrate by running foo1(x) with a few appropriately chosen vectors x. (Hint: set x and run the “guts” of the function piece by piece.)\n\n\nfoo1 &lt;- function(x) {\n  diff &lt;- x[-1] - x[1:(length(x) - 1)]\n  sum(diff &lt; 0)\n}\n\n\nThe foo1() function doesn’t perform well if a vector has missing values. Amend foo1() so that it produces a helpful error message and stops if there are any missing values in the input vector. Show that it works with appropriately chosen vectors x. Be sure you add error = TRUE to your R chunk, or else knitting will fail!\nWrite a function called greet using if, else if, and else to print out “good morning” if it’s before 12 PM, “good afternoon” if it’s between 12 PM and 5 PM, and “good evening” if it’s after 5 PM. Your function should work if you input a time like: greet(time = \"2018-05-03 17:38:01 CDT\") or if you input the current time with greet(time = Sys.time()). [Hint: check out the hour function in the lubridate package]\nWrite a function called summary_stats() to produce a custom set of summary statistics (n, mean, median, sd, IQR, min, and max) for any variable that you input from a tibble. Add an option to remove missing values, if any exist (and be sure the sample size n reflects the number of non-missing values). Thus, your function should have 3 inputs: data (the tibble of interest), x (the variable of interest), and na_option (with a default value of FALSE). Show that your function works for (a) the hwy variable in mpg_tbl &lt;- as_tibble(mpg), and (b) the age variable in gss_cat.\nAdd an option to (13) to produce summary statistics by group for a second variable. Show that your function works for (a) the hwy variable in mpg_tbl &lt;- as_tibble(mpg) grouped by drv, and (b) the age variable in gss_cat grouped by partyid.\nCreate a function that has a vector as the input and returns the last value. (Note: Be sure to use a name that does not write over an existing function!)\nSave your final table from (14) and write a function to draw a scatterplot of a measure of center (mean or median - user can choose) vs. a measure of spread (sd or IQR - user can choose), with points sized by sample size, to see if there is constant variance. Each point should be labeled with partyid, and the plot title should reflect the variables chosen by the user."
  },
  {
    "objectID": "06_iteration.html",
    "href": "06_iteration.html",
    "title": "Iteration",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\nThis leans on parts of R4DS Chapter 26: Iteration, in addition to parts of the first edition of R4DS.\n# Initial packages required\nlibrary(tidyverse)"
  },
  {
    "objectID": "06_iteration.html#iteration",
    "href": "06_iteration.html#iteration",
    "title": "Iteration",
    "section": "Iteration",
    "text": "Iteration\nReducing duplication of code will reduce errors and make debugging much easier. We’ve already seen how functions (Ch 25) can help reduce duplication by extracting repeated patterns of code. Another tool is iteration, when you find you’re doing the same thing to multiple inputs – repeating the same operation on different columns or datasets.\nHere we’ll see two important iteration paradigms: imperative programming and functional programming.\n\nImperation programming for iteration\nExamples: for loops and while loops\nPros: relatively easy to learn, make iteration very explicit so it’s obvious what’s happening, not as inefficient as some people believe\nCons: require lots of bookkeeping code that’s duplicated for every loop\nEvery for loop has three components:\n\noutput - plan ahead and allocate enough space for output\nsequence - determines what to loop over; cycles through different values of \\(i\\)\nbody - code that does the work; run repeatedly with different values of \\(i\\)\n\n\ndf &lt;- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\ndf\n\n# A tibble: 10 × 4\n        a      b      c      d\n    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1  0.273 -1.92  -1.76  -1.99 \n 2  0.774 -0.109  0.394 -0.763\n 3 -1.33   1.20  -0.762 -0.474\n 4  0.166 -0.459 -0.186 -0.207\n 5 -0.226 -1.34  -1.21   0.323\n 6  0.866  0.576 -0.525 -1.78 \n 7  0.413 -1.49   1.06   0.616\n 8  0.748  1.88   0.468  1.21 \n 9 -0.682 -0.894 -0.117 -0.534\n10 -0.260  1.25  -0.454 -0.294\n\n# want median of each column (w/o cutting and pasting)\n#   Be careful using square brackets vs double square brackets when\n#   selecting elements\nmedian(df[[1]])\n\n[1] 0.2195412\n\nmedian(df[1])\n\nError in median.default(df[1]): need numeric data\n\ndf[1]\n\n# A tibble: 10 × 1\n        a\n    &lt;dbl&gt;\n 1  0.273\n 2  0.774\n 3 -1.33 \n 4  0.166\n 5 -0.226\n 6  0.866\n 7  0.413\n 8  0.748\n 9 -0.682\n10 -0.260\n\ndf[[1]]\n\n [1]  0.2726745  0.7737568 -1.3255363  0.1664079 -0.2257948  0.8664420\n [7]  0.4126087  0.7479872 -0.6821753 -0.2604988\n\nclass(df[1])\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nclass(df[[1]])\n\n[1] \"numeric\"\n\n# basic for loop to take median of each column\noutput &lt;- vector(\"double\", ncol(df))  # 1. output\nfor (i in 1:4) {                      # 2. sequence\n  output[[i]] &lt;- median(df[[i]])      # 3. body\n}\noutput\n\n[1]  0.2195412 -0.2838017 -0.3199324 -0.3839214\n\n# ?seq_along - a safer option if had zero length vectors\noutput &lt;- vector(\"double\", ncol(df))  # 1. output\nfor (i in seq_along(df)) {            # 2. sequence\n  output[[i]] &lt;- median(df[[i]])      # 3. body\n}\noutput\n\n[1]  0.2195412 -0.2838017 -0.3199324 -0.3839214\n\n# use [[.]] even if don't have to to signal working with single elements\n\n# alternative solution - don't hardcode in \"4\"\noutput &lt;- vector(\"double\", ncol(df))  # 1. output\nfor(i in 1:ncol(df)) {                # 2. sequence\n  output[i] &lt;- median(df[[i]])        # 3. body\n}\noutput\n\n[1]  0.2195412 -0.2838017 -0.3199324 -0.3839214\n\n# another approach - no double square brackets since df not a tibble\ndf &lt;- as.data.frame(df)\noutput &lt;- vector(\"double\", ncol(df))  # 1. output\nfor(i in 1:ncol(df)) {                # 2. sequence\n  output[i] &lt;- median(df[,i])         # 3. body\n}\noutput\n\n[1]  0.2195412 -0.2838017 -0.3199324 -0.3839214\n\n\nOne advantage of seq_along(): works with unknown output length. However, the second approach below is much more efficient, since each iteration doesn’t copy all data from previous iterations.\n[Pause to Ponder:] What does the code below do? Be prepared to explain both chunks line-by-line!\n\n# for loop: unknown output length\n\nmeans &lt;- c(0, 1, 2)\noutput &lt;- double()\nfor (i in seq_along(means)) {\n  n &lt;- sample(100, 1)\n  output &lt;- c(output, rnorm(n, means[[i]]))\n}\nstr(output)        ## inefficient\n\n num [1:82] 0.056 -0.807 -0.554 0.475 1.066 ...\n\nout &lt;- vector(\"list\", length(means))\nfor (i in seq_along(means)) {\n  n &lt;- sample(100, 1)\n  out[[i]] &lt;- rnorm(n, means[[i]])\n}\nstr(out)           ## more efficient\n\nList of 3\n $ : num [1:39] -0.684 -0.101 1.133 0.921 -0.086 ...\n $ : num [1:50] -0.423 1.303 3.768 1.405 1.192 ...\n $ : num [1:78] 1.6 1.99 3.65 2.93 2.37 ...\n\nstr(unlist(out))   ## flatten list of vectors into single vector\n\n num [1:167] -0.684 -0.101 1.133 0.921 -0.086 ...\n\n\nFinally, the while() loop can be used with unknown sequence length. This is used more in simulation than in data analysis.\n[Pause to Ponder:] What does the following code do?\n\nflip &lt;- function() sample(c(\"T\", \"H\"), 1)\nflips &lt;- 0\nnheads &lt;- 0\nwhile (nheads &lt; 3) {\n  if (flip() == \"H\") {\n    nheads &lt;- nheads + 1\n  } else {\n    nheads &lt;- 0\n  }\n  flips &lt;- flips + 1\n}\nflips\n\n[1] 4\n\n\n\n\nFunctional programming for iteration\nExamples: map functions and across()\nPros: less code, fewer errors, code that’s easier to read; takes advantage of fact that R is a functional programming language\nCons: little more complicated to master vocabulary and use – a step up in abstraction\nR is a functional programming language. This means that it’s possible to wrap up for loops in a function, and call that function instead of using the for loop directly. Passing one function to another is a very powerful coding approach!!\n\n# Below you can avoid writing separate functions for mean, median, \n#   SD, etc. by column\ndf &lt;- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\ncol_summary &lt;- function(df, fun) {\n  out &lt;- vector(\"double\", length(df))\n  for (i in seq_along(df)) {\n    out[i] &lt;- fun(df[[i]])\n  }\n  out\n}\ncol_summary(df, median)\n\n[1] -0.34128271 -0.66109716 -0.38854038 -0.01046294\n\ncol_summary(df, mean)\n\n[1] -0.38840764 -0.52472311 -0.33971009  0.01949469\n\ncol_summary(df, IQR)\n\n[1] 1.060926 1.540889 1.449185 1.164345\n\n\nThe purrr package provides map functions to eliminate need for for loops, plus it makes code easier to read!\n\n# using map functions for summary stats by column as above\nmap_dbl(df, mean)\n\n          a           b           c           d \n-0.38840764 -0.52472311 -0.33971009  0.01949469 \n\nmap_dbl(df, median)\n\n          a           b           c           d \n-0.34128271 -0.66109716 -0.38854038 -0.01046294 \n\nmap_dbl(df, sd)\n\n        a         b         c         d \n0.7403583 1.1228672 0.9890651 0.8330836 \n\nmap_dbl(df, mean, trim = 0.5)\n\n          a           b           c           d \n-0.34128271 -0.66109716 -0.38854038 -0.01046294 \n\n# map_dbl means make a double vector\n# can also do map() for list, map_lgl(), map_int(), and map_chr()\n\n# even more clear\ndf %&gt;% map_dbl(mean)\n\n          a           b           c           d \n-0.38840764 -0.52472311 -0.33971009  0.01949469 \n\ndf %&gt;% map_dbl(median)\n\n          a           b           c           d \n-0.34128271 -0.66109716 -0.38854038 -0.01046294 \n\ndf %&gt;% map_dbl(sd)\n\n        a         b         c         d \n0.7403583 1.1228672 0.9890651 0.8330836 \n\n\nThe across() function from dplyr also works well:\n\ndf |&gt; summarize(\n  n = n(),\n  across(.cols = a:d, .fns = median, .names = \"median_{.col}\")\n)\n\n# A tibble: 1 × 5\n      n median_a median_b median_c median_d\n  &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1    10   -0.341   -0.661   -0.389  -0.0105\n\n# And if we're worried about NAs\ndf_miss &lt;- df\ndf_miss[2, 1] &lt;- NA\ndf_miss[4:5, 2] &lt;- NA\ndf_miss |&gt; \n  summarize(\n    across(\n      a:d,\n      list(\n        median = \\(x) median(x, na.rm = TRUE),\n        n_miss = \\(x) sum(is.na(x))\n      ),\n      .names = \"{.fn}_{.col}\"\n    ),\n    n = n(),\n  )\n\n# A tibble: 1 × 9\n  median_a n_miss_a median_b n_miss_b median_c n_miss_c median_d n_miss_d     n\n     &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt; &lt;int&gt;\n1   -0.145        1   -0.661        2   -0.389        0  -0.0105        0    10\n\n# where \\ is shorthand for an anonymous function\n\npivot_longer() with group_by() and summarize() also provides a nice solution:\n\nlong &lt;- df |&gt; \n  pivot_longer(a:d) |&gt; \n  group_by(name) |&gt; \n  summarize(\n    median = median(value),\n    mean = mean(value)\n  )\nlong\n\n# A tibble: 4 × 3\n  name   median    mean\n  &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 a     -0.341  -0.388 \n2 b     -0.661  -0.525 \n3 c     -0.389  -0.340 \n4 d     -0.0105  0.0195\n\n\nHere are a couple of other nice features of map functions: - perform analyses (like fitting a line) by subgroup - extracting components from a model or elements by position\n\n# fit linear model to each group based on cylinder\n#   - split designed to split into new dfs (unlike group_by)\n#   - map returns a vector or list, which can be limiting\nmap = purrr::map\nmodels &lt;- as_tibble(mtcars) %&gt;% \n  split(.$cyl) %&gt;% \n  map(function(df) lm(mpg ~ wt, data = df))\nmodels\n\n$`4`\n\nCall:\nlm(formula = mpg ~ wt, data = df)\n\nCoefficients:\n(Intercept)           wt  \n     39.571       -5.647  \n\n\n$`6`\n\nCall:\nlm(formula = mpg ~ wt, data = df)\n\nCoefficients:\n(Intercept)           wt  \n      28.41        -2.78  \n\n\n$`8`\n\nCall:\nlm(formula = mpg ~ wt, data = df)\n\nCoefficients:\n(Intercept)           wt  \n     23.868       -2.192  \n\nmodels[[1]]\n\n\nCall:\nlm(formula = mpg ~ wt, data = df)\n\nCoefficients:\n(Intercept)           wt  \n     39.571       -5.647  \n\n# shortcut using purrr - 1-sided formulas\nmodels &lt;- mtcars %&gt;% \n  split(.$cyl) %&gt;% \n  map(~lm(mpg ~ wt, data = .))\nmodels\n\n$`4`\n\nCall:\nlm(formula = mpg ~ wt, data = .)\n\nCoefficients:\n(Intercept)           wt  \n     39.571       -5.647  \n\n\n$`6`\n\nCall:\nlm(formula = mpg ~ wt, data = .)\n\nCoefficients:\n(Intercept)           wt  \n      28.41        -2.78  \n\n\n$`8`\n\nCall:\nlm(formula = mpg ~ wt, data = .)\n\nCoefficients:\n(Intercept)           wt  \n     23.868       -2.192  \n\n# extract named components from each model\nstr(models)\n\nList of 3\n $ 4:List of 12\n  ..$ coefficients : Named num [1:2] 39.57 -5.65\n  .. ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"wt\"\n  ..$ residuals    : Named num [1:11] -3.6701 2.8428 1.0169 5.2523 -0.0513 ...\n  .. ..- attr(*, \"names\")= chr [1:11] \"Datsun 710\" \"Merc 240D\" \"Merc 230\" \"Fiat 128\" ...\n  ..$ effects      : Named num [1:11] -88.433 10.171 0.695 6.231 1.728 ...\n  .. ..- attr(*, \"names\")= chr [1:11] \"(Intercept)\" \"wt\" \"\" \"\" ...\n  ..$ rank         : int 2\n  ..$ fitted.values: Named num [1:11] 26.5 21.6 21.8 27.1 30.5 ...\n  .. ..- attr(*, \"names\")= chr [1:11] \"Datsun 710\" \"Merc 240D\" \"Merc 230\" \"Fiat 128\" ...\n  ..$ assign       : int [1:2] 0 1\n  ..$ qr           :List of 5\n  .. ..$ qr   : num [1:11, 1:2] -3.317 0.302 0.302 0.302 0.302 ...\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:11] \"Datsun 710\" \"Merc 240D\" \"Merc 230\" \"Fiat 128\" ...\n  .. .. .. ..$ : chr [1:2] \"(Intercept)\" \"wt\"\n  .. .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  .. ..$ qraux: num [1:2] 1.3 1.5\n  .. ..$ pivot: int [1:2] 1 2\n  .. ..$ tol  : num 1e-07\n  .. ..$ rank : int 2\n  .. ..- attr(*, \"class\")= chr \"qr\"\n  ..$ df.residual  : int 9\n  ..$ xlevels      : Named list()\n  ..$ call         : language lm(formula = mpg ~ wt, data = .)\n  ..$ terms        :Classes 'terms', 'formula'  language mpg ~ wt\n  .. .. ..- attr(*, \"variables\")= language list(mpg, wt)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"mpg\" \"wt\"\n  .. .. .. .. ..$ : chr \"wt\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"wt\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: 0x000001e7a53a1b88&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(mpg, wt)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"wt\"\n  ..$ model        :'data.frame':   11 obs. of  2 variables:\n  .. ..$ mpg: num [1:11] 22.8 24.4 22.8 32.4 30.4 33.9 21.5 27.3 26 30.4 ...\n  .. ..$ wt : num [1:11] 2.32 3.19 3.15 2.2 1.61 ...\n  .. ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language mpg ~ wt\n  .. .. .. ..- attr(*, \"variables\")= language list(mpg, wt)\n  .. .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. .. ..$ : chr [1:2] \"mpg\" \"wt\"\n  .. .. .. .. .. ..$ : chr \"wt\"\n  .. .. .. ..- attr(*, \"term.labels\")= chr \"wt\"\n  .. .. .. ..- attr(*, \"order\")= int 1\n  .. .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. .. ..- attr(*, \"response\")= int 1\n  .. .. .. ..- attr(*, \".Environment\")=&lt;environment: 0x000001e7a53a1b88&gt; \n  .. .. .. ..- attr(*, \"predvars\")= language list(mpg, wt)\n  .. .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"wt\"\n  ..- attr(*, \"class\")= chr \"lm\"\n $ 6:List of 12\n  ..$ coefficients : Named num [1:2] 28.41 -2.78\n  .. ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"wt\"\n  ..$ residuals    : Named num [1:7] -0.125 0.584 1.929 -0.69 0.355 ...\n  .. ..- attr(*, \"names\")= chr [1:7] \"Mazda RX4\" \"Mazda RX4 Wag\" \"Hornet 4 Drive\" \"Valiant\" ...\n  ..$ effects      : Named num [1:7] -52.235 -2.427 2.111 -0.353 0.679 ...\n  .. ..- attr(*, \"names\")= chr [1:7] \"(Intercept)\" \"wt\" \"\" \"\" ...\n  ..$ rank         : int 2\n  ..$ fitted.values: Named num [1:7] 21.1 20.4 19.5 18.8 18.8 ...\n  .. ..- attr(*, \"names\")= chr [1:7] \"Mazda RX4\" \"Mazda RX4 Wag\" \"Hornet 4 Drive\" \"Valiant\" ...\n  ..$ assign       : int [1:2] 0 1\n  ..$ qr           :List of 5\n  .. ..$ qr   : num [1:7, 1:2] -2.646 0.378 0.378 0.378 0.378 ...\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:7] \"Mazda RX4\" \"Mazda RX4 Wag\" \"Hornet 4 Drive\" \"Valiant\" ...\n  .. .. .. ..$ : chr [1:2] \"(Intercept)\" \"wt\"\n  .. .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  .. ..$ qraux: num [1:2] 1.38 1.12\n  .. ..$ pivot: int [1:2] 1 2\n  .. ..$ tol  : num 1e-07\n  .. ..$ rank : int 2\n  .. ..- attr(*, \"class\")= chr \"qr\"\n  ..$ df.residual  : int 5\n  ..$ xlevels      : Named list()\n  ..$ call         : language lm(formula = mpg ~ wt, data = .)\n  ..$ terms        :Classes 'terms', 'formula'  language mpg ~ wt\n  .. .. ..- attr(*, \"variables\")= language list(mpg, wt)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"mpg\" \"wt\"\n  .. .. .. .. ..$ : chr \"wt\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"wt\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: 0x000001e7a546d630&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(mpg, wt)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"wt\"\n  ..$ model        :'data.frame':   7 obs. of  2 variables:\n  .. ..$ mpg: num [1:7] 21 21 21.4 18.1 19.2 17.8 19.7\n  .. ..$ wt : num [1:7] 2.62 2.88 3.21 3.46 3.44 ...\n  .. ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language mpg ~ wt\n  .. .. .. ..- attr(*, \"variables\")= language list(mpg, wt)\n  .. .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. .. ..$ : chr [1:2] \"mpg\" \"wt\"\n  .. .. .. .. .. ..$ : chr \"wt\"\n  .. .. .. ..- attr(*, \"term.labels\")= chr \"wt\"\n  .. .. .. ..- attr(*, \"order\")= int 1\n  .. .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. .. ..- attr(*, \"response\")= int 1\n  .. .. .. ..- attr(*, \".Environment\")=&lt;environment: 0x000001e7a546d630&gt; \n  .. .. .. ..- attr(*, \"predvars\")= language list(mpg, wt)\n  .. .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"wt\"\n  ..- attr(*, \"class\")= chr \"lm\"\n $ 8:List of 12\n  ..$ coefficients : Named num [1:2] 23.87 -2.19\n  .. ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"wt\"\n  ..$ residuals    : Named num [1:14] 2.374 -1.741 1.455 1.61 -0.381 ...\n  .. ..- attr(*, \"names\")= chr [1:14] \"Hornet Sportabout\" \"Duster 360\" \"Merc 450SE\" \"Merc 450SL\" ...\n  ..$ effects      : Named num [1:14] -56.499 -6.003 0.816 1.22 -0.807 ...\n  .. ..- attr(*, \"names\")= chr [1:14] \"(Intercept)\" \"wt\" \"\" \"\" ...\n  ..$ rank         : int 2\n  ..$ fitted.values: Named num [1:14] 16.3 16 14.9 15.7 15.6 ...\n  .. ..- attr(*, \"names\")= chr [1:14] \"Hornet Sportabout\" \"Duster 360\" \"Merc 450SE\" \"Merc 450SL\" ...\n  ..$ assign       : int [1:2] 0 1\n  ..$ qr           :List of 5\n  .. ..$ qr   : num [1:14, 1:2] -3.742 0.267 0.267 0.267 0.267 ...\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:14] \"Hornet Sportabout\" \"Duster 360\" \"Merc 450SE\" \"Merc 450SL\" ...\n  .. .. .. ..$ : chr [1:2] \"(Intercept)\" \"wt\"\n  .. .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  .. ..$ qraux: num [1:2] 1.27 1.11\n  .. ..$ pivot: int [1:2] 1 2\n  .. ..$ tol  : num 1e-07\n  .. ..$ rank : int 2\n  .. ..- attr(*, \"class\")= chr \"qr\"\n  ..$ df.residual  : int 12\n  ..$ xlevels      : Named list()\n  ..$ call         : language lm(formula = mpg ~ wt, data = .)\n  ..$ terms        :Classes 'terms', 'formula'  language mpg ~ wt\n  .. .. ..- attr(*, \"variables\")= language list(mpg, wt)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"mpg\" \"wt\"\n  .. .. .. .. ..$ : chr \"wt\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"wt\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: 0x000001e7a54ada58&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(mpg, wt)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"wt\"\n  ..$ model        :'data.frame':   14 obs. of  2 variables:\n  .. ..$ mpg: num [1:14] 18.7 14.3 16.4 17.3 15.2 10.4 10.4 14.7 15.5 15.2 ...\n  .. ..$ wt : num [1:14] 3.44 3.57 4.07 3.73 3.78 ...\n  .. ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language mpg ~ wt\n  .. .. .. ..- attr(*, \"variables\")= language list(mpg, wt)\n  .. .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. .. ..$ : chr [1:2] \"mpg\" \"wt\"\n  .. .. .. .. .. ..$ : chr \"wt\"\n  .. .. .. ..- attr(*, \"term.labels\")= chr \"wt\"\n  .. .. .. ..- attr(*, \"order\")= int 1\n  .. .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. .. ..- attr(*, \"response\")= int 1\n  .. .. .. ..- attr(*, \".Environment\")=&lt;environment: 0x000001e7a54ada58&gt; \n  .. .. .. ..- attr(*, \"predvars\")= language list(mpg, wt)\n  .. .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"wt\"\n  ..- attr(*, \"class\")= chr \"lm\"\n\nstr(models[[1]])\n\nList of 12\n $ coefficients : Named num [1:2] 39.57 -5.65\n  ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"wt\"\n $ residuals    : Named num [1:11] -3.6701 2.8428 1.0169 5.2523 -0.0513 ...\n  ..- attr(*, \"names\")= chr [1:11] \"Datsun 710\" \"Merc 240D\" \"Merc 230\" \"Fiat 128\" ...\n $ effects      : Named num [1:11] -88.433 10.171 0.695 6.231 1.728 ...\n  ..- attr(*, \"names\")= chr [1:11] \"(Intercept)\" \"wt\" \"\" \"\" ...\n $ rank         : int 2\n $ fitted.values: Named num [1:11] 26.5 21.6 21.8 27.1 30.5 ...\n  ..- attr(*, \"names\")= chr [1:11] \"Datsun 710\" \"Merc 240D\" \"Merc 230\" \"Fiat 128\" ...\n $ assign       : int [1:2] 0 1\n $ qr           :List of 5\n  ..$ qr   : num [1:11, 1:2] -3.317 0.302 0.302 0.302 0.302 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:11] \"Datsun 710\" \"Merc 240D\" \"Merc 230\" \"Fiat 128\" ...\n  .. .. ..$ : chr [1:2] \"(Intercept)\" \"wt\"\n  .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  ..$ qraux: num [1:2] 1.3 1.5\n  ..$ pivot: int [1:2] 1 2\n  ..$ tol  : num 1e-07\n  ..$ rank : int 2\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 9\n $ xlevels      : Named list()\n $ call         : language lm(formula = mpg ~ wt, data = .)\n $ terms        :Classes 'terms', 'formula'  language mpg ~ wt\n  .. ..- attr(*, \"variables\")= language list(mpg, wt)\n  .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:2] \"mpg\" \"wt\"\n  .. .. .. ..$ : chr \"wt\"\n  .. ..- attr(*, \"term.labels\")= chr \"wt\"\n  .. ..- attr(*, \"order\")= int 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=&lt;environment: 0x000001e7a53a1b88&gt; \n  .. ..- attr(*, \"predvars\")= language list(mpg, wt)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"wt\"\n $ model        :'data.frame':  11 obs. of  2 variables:\n  ..$ mpg: num [1:11] 22.8 24.4 22.8 32.4 30.4 33.9 21.5 27.3 26 30.4 ...\n  ..$ wt : num [1:11] 2.32 3.19 3.15 2.2 1.61 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language mpg ~ wt\n  .. .. ..- attr(*, \"variables\")= language list(mpg, wt)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"mpg\" \"wt\"\n  .. .. .. .. ..$ : chr \"wt\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"wt\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: 0x000001e7a53a1b88&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(mpg, wt)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"wt\"\n - attr(*, \"class\")= chr \"lm\"\n\nstr(summary(models[[1]]))\n\nList of 11\n $ call         : language lm(formula = mpg ~ wt, data = .)\n $ terms        :Classes 'terms', 'formula'  language mpg ~ wt\n  .. ..- attr(*, \"variables\")= language list(mpg, wt)\n  .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:2] \"mpg\" \"wt\"\n  .. .. .. ..$ : chr \"wt\"\n  .. ..- attr(*, \"term.labels\")= chr \"wt\"\n  .. ..- attr(*, \"order\")= int 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=&lt;environment: 0x000001e7a53a1b88&gt; \n  .. ..- attr(*, \"predvars\")= language list(mpg, wt)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"wt\"\n $ residuals    : Named num [1:11] -3.6701 2.8428 1.0169 5.2523 -0.0513 ...\n  ..- attr(*, \"names\")= chr [1:11] \"Datsun 710\" \"Merc 240D\" \"Merc 230\" \"Fiat 128\" ...\n $ coefficients : num [1:2, 1:4] 39.57 -5.65 4.35 1.85 9.1 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:2] \"(Intercept)\" \"wt\"\n  .. ..$ : chr [1:4] \"Estimate\" \"Std. Error\" \"t value\" \"Pr(&gt;|t|)\"\n $ aliased      : Named logi [1:2] FALSE FALSE\n  ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"wt\"\n $ sigma        : num 3.33\n $ df           : int [1:3] 2 9 2\n $ r.squared    : num 0.509\n $ adj.r.squared: num 0.454\n $ fstatistic   : Named num [1:3] 9.32 1 9\n  ..- attr(*, \"names\")= chr [1:3] \"value\" \"numdf\" \"dendf\"\n $ cov.unscaled : num [1:2, 1:2] 1.701 -0.705 -0.705 0.308\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:2] \"(Intercept)\" \"wt\"\n  .. ..$ : chr [1:2] \"(Intercept)\" \"wt\"\n - attr(*, \"class\")= chr \"summary.lm\"\n\nmodels %&gt;% \n  map(summary) %&gt;% \n  map_dbl(\"r.squared\")\n\n        4         6         8 \n0.5086326 0.4645102 0.4229655 \n\n# can use integer to select elements by position\nx &lt;- list(list(1, 2, 3), list(4, 5, 6), list(7, 8, 9))\nx %&gt;% map_dbl(2)\n\n[1] 2 5 8"
  },
  {
    "objectID": "06_iteration.html#on-your-own",
    "href": "06_iteration.html#on-your-own",
    "title": "Iteration",
    "section": "On Your Own",
    "text": "On Your Own\n\nCompute the mean of every column of the mtcars data set using (a) a for loop, (b) a map function, (c) the across() function, and (d) pivot_longer().\nWrite a function that prints the mean of each numeric column in a data frame. Try it on the iris data set. (Hint: keep(is.numeric))\nEliminate the for loop in each of the following examples by taking advantage of an existing function that works with vectors:\n\n\nout &lt;- \"\"\nfor (x in letters) {\n  out &lt;- stringr::str_c(out, x)\n}\nout\n\n[1] \"abcdefghijklmnopqrstuvwxyz\"\n\nx &lt;- runif(100)\nout &lt;- vector(\"numeric\", length(x))\nout[1] &lt;- x[1]\nfor (i in 2:length(x)) {\n  out[i] &lt;- out[i - 1] + x[i]\n}\nout\n\n  [1]  0.2298192  0.7396212  0.9636481  1.5756795  2.5325020  3.2362434\n  [7]  3.7308866  4.3501698  5.1159023  5.3785484  5.5228874  5.8521585\n [13]  6.5914763  7.5216726  7.8583275  8.6152528  9.4534832 10.3175669\n [19] 10.4685286 10.9452788 11.7269982 12.3500076 13.0300204 13.8342599\n [25] 14.2178326 15.0241320 15.7946528 16.3829957 17.2537409 18.0705103\n [31] 18.8775207 18.9031846 19.1303851 20.0966374 20.5390098 20.9802217\n [37] 21.2010060 21.8672204 22.6182600 23.0643428 23.7638705 24.1058465\n [43] 24.3226382 24.5855245 24.8414715 25.5017068 25.7003655 26.6636219\n [49] 27.1381374 27.7639631 28.1345246 28.7335309 29.2613443 29.4208063\n [55] 30.0543533 30.8663370 31.8454409 32.7880599 32.8876777 33.7232427\n [61] 34.3144583 35.2616667 36.1371533 36.9189763 37.3456079 37.9803608\n [67] 38.0115946 38.6205860 39.2780778 40.1053406 40.1709954 40.4588830\n [73] 40.6325953 41.1252302 42.0234198 42.5210954 42.6652068 42.9478020\n [79] 43.2625077 43.7633444 43.8738468 44.0207676 44.1715583 44.4657958\n [85] 44.6268709 44.7596331 45.5476096 45.9607185 46.1011396 46.9384628\n [91] 47.4997912 47.7629511 48.7565571 49.2312426 50.1462279 50.1986740\n [97] 50.6959804 51.0335659 51.4416906 51.4620414\n\n\n\nCompute the number of unique values in each column of the iris data set using at least 2 of your favorite iteration methods. Bonus points if you can use pivot_longer()!"
  },
  {
    "objectID": "08_strings_part1.html",
    "href": "08_strings_part1.html",
    "title": "Strings: Pre-class Video",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\nThis uses parts of MDSR Ch 14: Strings and Ch 15: Regular Expressions (both the first and second editions).\nlibrary(tidyverse)\n#spotify &lt;- read_csv(\"Data/spotify.csv\") \nspotify &lt;- read_csv(\"https://proback.github.io/264_fall_2024/Data/spotify.csv\")\n\nspot_smaller &lt;- spotify |&gt;\n  select(title, artist, album_release_date, album_name, subgenre, playlist_name)\n\nspot_smaller &lt;- spot_smaller[c(5, 32, 49, 52,  83, 175, 219, 231,  246, 265), ]\nspot_smaller\n\n# A tibble: 10 × 6\n   title             artist album_release_date album_name subgenre playlist_name\n   &lt;chr&gt;             &lt;chr&gt;  &lt;chr&gt;              &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;        \n 1 Hear Me Now       Alok   2016-01-01         Hear Me N… indie p… \"Chillout & …\n 2 Run the World (G… Beyon… 2011-06-24         4          post-te… \"post-teen a…\n 3 Formation         Beyon… 2016-04-23         Lemonade   hip pop  \"Feeling Acc…\n 4 7/11              Beyon… 2014-11-24         BEYONCÉ [… hip pop  \"Feeling Acc…\n 5 My Oh My (feat. … Camil… 2019-12-06         Romance    latin p… \"2020 Hits &…\n 6 It's Automatic    Frees… 2013-11-28         It's Auto… latin h… \"80's Freest…\n 7 Poetic Justice    Kendr… 2012               good kid,… hip hop  \"Hip Hop Con…\n 8 A.D.H.D           Kendr… 2011-07-02         Section.80 souther… \"Hip-Hop 'n …\n 9 Ya Estuvo         Kid F… 1990-01-01         Hispanic … latin h… \"HIP-HOP: La…\n10 Runnin (with A$A… Mike … 2018-11-16         Creed II:… gangste… \"RAP Gangsta\""
  },
  {
    "objectID": "08_strings_part1.html#a-string-is-just-a-set-of-characters.",
    "href": "08_strings_part1.html#a-string-is-just-a-set-of-characters.",
    "title": "Strings: Pre-class Video",
    "section": "A string is just a set of characters.",
    "text": "A string is just a set of characters.\n\nsingle_string &lt;- \"this is a string!\"\nsingle_string\n\n[1] \"this is a string!\"\n\nstring_vector &lt;- c(\"this\", \"is\", \"a\", \"vector\", \"of strings\")\n\nstring_vector\n\n[1] \"this\"       \"is\"         \"a\"          \"vector\"     \"of strings\"\n\n# This is a tibble with many columns of \"string variables\", or \"character variables\"\nspot_smaller\n\n# A tibble: 10 × 6\n   title             artist album_release_date album_name subgenre playlist_name\n   &lt;chr&gt;             &lt;chr&gt;  &lt;chr&gt;              &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;        \n 1 Hear Me Now       Alok   2016-01-01         Hear Me N… indie p… \"Chillout & …\n 2 Run the World (G… Beyon… 2011-06-24         4          post-te… \"post-teen a…\n 3 Formation         Beyon… 2016-04-23         Lemonade   hip pop  \"Feeling Acc…\n 4 7/11              Beyon… 2014-11-24         BEYONCÉ [… hip pop  \"Feeling Acc…\n 5 My Oh My (feat. … Camil… 2019-12-06         Romance    latin p… \"2020 Hits &…\n 6 It's Automatic    Frees… 2013-11-28         It's Auto… latin h… \"80's Freest…\n 7 Poetic Justice    Kendr… 2012               good kid,… hip hop  \"Hip Hop Con…\n 8 A.D.H.D           Kendr… 2011-07-02         Section.80 souther… \"Hip-Hop 'n …\n 9 Ya Estuvo         Kid F… 1990-01-01         Hispanic … latin h… \"HIP-HOP: La…\n10 Runnin (with A$A… Mike … 2018-11-16         Creed II:… gangste… \"RAP Gangsta\"\n\n# Each column of the tibble is a vector of strings.\nspot_smaller$title\n\n [1] \"Hear Me Now\"                                      \n [2] \"Run the World (Girls)\"                            \n [3] \"Formation\"                                        \n [4] \"7/11\"                                             \n [5] \"My Oh My (feat. DaBaby)\"                          \n [6] \"It's Automatic\"                                   \n [7] \"Poetic Justice\"                                   \n [8] \"A.D.H.D\"                                          \n [9] \"Ya Estuvo\"                                        \n[10] \"Runnin (with A$AP Rocky, A$AP Ferg & Nicki Minaj)\"\n\n# Each item in the tibble is a string.\nspot_smaller$title[1]\n\n[1] \"Hear Me Now\""
  },
  {
    "objectID": "08_strings_part1.html#functions-that-start-str_-do-stuff-to-strings",
    "href": "08_strings_part1.html#functions-that-start-str_-do-stuff-to-strings",
    "title": "Strings: Pre-class Video",
    "section": "Functions that start str_ do stuff to strings!",
    "text": "Functions that start str_ do stuff to strings!\n\nstr_length()\n\n# when the intput to str_length is a single string, the output is a single value:\nstr_length(\"hi\")\n\n[1] 2\n\nstr_length(single_string)\n\n[1] 17\n\n# when the input to str_length is a vector, the output is a vector:\nstr_length(string_vector)\n\n[1]  4  2  1  6 10\n\n\nstr_length takes a vector input and creates a vector output (or a single value input and returns a single value output)…. this makes it easy to use within a mutate!\n\nspot_smaller |&gt;\n  select(title) |&gt;\n  mutate(title_length = str_length(title))\n\n# A tibble: 10 × 2\n   title                                             title_length\n   &lt;chr&gt;                                                    &lt;int&gt;\n 1 Hear Me Now                                                 11\n 2 Run the World (Girls)                                       21\n 3 Formation                                                    9\n 4 7/11                                                         4\n 5 My Oh My (feat. DaBaby)                                     23\n 6 It's Automatic                                              14\n 7 Poetic Justice                                              14\n 8 A.D.H.D                                                      7\n 9 Ya Estuvo                                                    9\n10 Runnin (with A$AP Rocky, A$AP Ferg & Nicki Minaj)           49\n\n\n\n\nstr_sub()\nThis function creates substrings (shorter strings)\n\n# When the input is a single string, the output is a single string\nsingle_string\n\n[1] \"this is a string!\"\n\nstr_sub(single_string, 1, 7)\n\n[1] \"this is\"\n\nstr_sub(single_string, 8, 9)\n\n[1] \" a\"\n\nstr_sub(single_string, 9, 9)\n\n[1] \"a\"\n\n# When the input is a vector of strings, what do you think the output will be?\nstring_vector\n\n[1] \"this\"       \"is\"         \"a\"          \"vector\"     \"of strings\"\n\nstr_sub(string_vector, 1, 2)\n\n[1] \"th\" \"is\" \"a\"  \"ve\" \"of\"\n\n\nHow can we use str_sub to get just the year of the album_release_date? Try it here! Then scroll down for solution.\n\nspot_smaller\n\n# A tibble: 10 × 6\n   title             artist album_release_date album_name subgenre playlist_name\n   &lt;chr&gt;             &lt;chr&gt;  &lt;chr&gt;              &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;        \n 1 Hear Me Now       Alok   2016-01-01         Hear Me N… indie p… \"Chillout & …\n 2 Run the World (G… Beyon… 2011-06-24         4          post-te… \"post-teen a…\n 3 Formation         Beyon… 2016-04-23         Lemonade   hip pop  \"Feeling Acc…\n 4 7/11              Beyon… 2014-11-24         BEYONCÉ [… hip pop  \"Feeling Acc…\n 5 My Oh My (feat. … Camil… 2019-12-06         Romance    latin p… \"2020 Hits &…\n 6 It's Automatic    Frees… 2013-11-28         It's Auto… latin h… \"80's Freest…\n 7 Poetic Justice    Kendr… 2012               good kid,… hip hop  \"Hip Hop Con…\n 8 A.D.H.D           Kendr… 2011-07-02         Section.80 souther… \"Hip-Hop 'n …\n 9 Ya Estuvo         Kid F… 1990-01-01         Hispanic … latin h… \"HIP-HOP: La…\n10 Runnin (with A$A… Mike … 2018-11-16         Creed II:… gangste… \"RAP Gangsta\"\n\n\n. . . . . . . .\n\nspot_smaller |&gt;\n  select(title, artist, album_release_date) |&gt;\n  mutate(album_release_year = str_sub(album_release_date, 1, 4))\n\n# A tibble: 10 × 4\n   title                            artist album_release_date album_release_year\n   &lt;chr&gt;                            &lt;chr&gt;  &lt;chr&gt;              &lt;chr&gt;             \n 1 Hear Me Now                      Alok   2016-01-01         2016              \n 2 Run the World (Girls)            Beyon… 2011-06-24         2011              \n 3 Formation                        Beyon… 2016-04-23         2016              \n 4 7/11                             Beyon… 2014-11-24         2014              \n 5 My Oh My (feat. DaBaby)          Camil… 2019-12-06         2019              \n 6 It's Automatic                   Frees… 2013-11-28         2013              \n 7 Poetic Justice                   Kendr… 2012               2012              \n 8 A.D.H.D                          Kendr… 2011-07-02         2011              \n 9 Ya Estuvo                        Kid F… 1990-01-01         1990              \n10 Runnin (with A$AP Rocky, A$AP F… Mike … 2018-11-16         2018              \n\n\n\n\nstr_c()\nThis collapses multiple strings together into one string.\n\nstr_c(\"is\", \"this output\", \"a\", \"single value\", \"or\", \"a vector\", \"?\")\n\n[1] \"isthis outputasingle valueora vector?\"\n\n# like unite and separate, we can specify the separator:\n\nstr_c(\"is\", \"this output\", \"a\", \"single value\", \"or\", \"a vector\", \"?\", \n      sep = \" \")\n\n[1] \"is this output a single value or a vector ?\"\n\n\nWe can see that the input is a list of strings, and the output is a single string.\nSo… why is this useful?\n\nx &lt;- runif(1)\nx\n\n[1] 0.1083407\n\nstr_c(\"I can put other values, like\", x, \"in here!\", sep = \" \")\n\n[1] \"I can put other values, like 0.108340746955946 in here!\"\n\nspot_smaller\n\n# A tibble: 10 × 6\n   title             artist album_release_date album_name subgenre playlist_name\n   &lt;chr&gt;             &lt;chr&gt;  &lt;chr&gt;              &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;        \n 1 Hear Me Now       Alok   2016-01-01         Hear Me N… indie p… \"Chillout & …\n 2 Run the World (G… Beyon… 2011-06-24         4          post-te… \"post-teen a…\n 3 Formation         Beyon… 2016-04-23         Lemonade   hip pop  \"Feeling Acc…\n 4 7/11              Beyon… 2014-11-24         BEYONCÉ [… hip pop  \"Feeling Acc…\n 5 My Oh My (feat. … Camil… 2019-12-06         Romance    latin p… \"2020 Hits &…\n 6 It's Automatic    Frees… 2013-11-28         It's Auto… latin h… \"80's Freest…\n 7 Poetic Justice    Kendr… 2012               good kid,… hip hop  \"Hip Hop Con…\n 8 A.D.H.D           Kendr… 2011-07-02         Section.80 souther… \"Hip-Hop 'n …\n 9 Ya Estuvo         Kid F… 1990-01-01         Hispanic … latin h… \"HIP-HOP: La…\n10 Runnin (with A$A… Mike … 2018-11-16         Creed II:… gangste… \"RAP Gangsta\"\n\nsong_count &lt;- spot_smaller |&gt; \n  count(artist) |&gt;\n  slice_max(n, n = 1)\n\nsong_count\n\n# A tibble: 1 × 2\n  artist      n\n  &lt;chr&gt;   &lt;int&gt;\n1 Beyoncé     3\n\nsong_count$artist\n\n[1] \"Beyoncé\"\n\nsong_count$n\n\n[1] 3\n\nstr_c(\"The artist with the most songs in spot_smaller is\", song_count$artist, \"with\", song_count$n, \"songs.\", sep = \" \")\n\n[1] \"The artist with the most songs in spot_smaller is Beyoncé with 3 songs.\"\n\n\nWe can use this in a tibble too.\n\nspot_smaller |&gt;\n  select(artist, title) |&gt;\n  mutate(song_by = str_c(title, \"by\", artist, sep = \" \"))\n\n# A tibble: 10 × 3\n   artist            title                                             song_by  \n   &lt;chr&gt;             &lt;chr&gt;                                             &lt;chr&gt;    \n 1 Alok              Hear Me Now                                       Hear Me …\n 2 Beyoncé           Run the World (Girls)                             Run the …\n 3 Beyoncé           Formation                                         Formatio…\n 4 Beyoncé           7/11                                              7/11 by …\n 5 Camila Cabello    My Oh My (feat. DaBaby)                           My Oh My…\n 6 Freestyle         It's Automatic                                    It's Aut…\n 7 Kendrick Lamar    Poetic Justice                                    Poetic J…\n 8 Kendrick Lamar    A.D.H.D                                           A.D.H.D …\n 9 Kid Frost         Ya Estuvo                                         Ya Estuv…\n10 Mike WiLL Made-It Runnin (with A$AP Rocky, A$AP Ferg & Nicki Minaj) Runnin (…"
  },
  {
    "objectID": "08_strings_part1.html#str_to_lower-str_to_upper-str_to_title",
    "href": "08_strings_part1.html#str_to_lower-str_to_upper-str_to_title",
    "title": "Strings: Pre-class Video",
    "section": "str_to_lower(), str_to_upper(), str_to_title()",
    "text": "str_to_lower(), str_to_upper(), str_to_title()\nThese are pretty self explanatory.\n\nspot_smaller |&gt;\n  select(title) |&gt;\n  mutate(title_to_lower = str_to_lower(title),\n         title_to_upper = str_to_upper(title))\n\n# A tibble: 10 × 3\n   title                                           title_to_lower title_to_upper\n   &lt;chr&gt;                                           &lt;chr&gt;          &lt;chr&gt;         \n 1 Hear Me Now                                     hear me now    HEAR ME NOW   \n 2 Run the World (Girls)                           run the world… RUN THE WORLD…\n 3 Formation                                       formation      FORMATION     \n 4 7/11                                            7/11           7/11          \n 5 My Oh My (feat. DaBaby)                         my oh my (fea… MY OH MY (FEA…\n 6 It's Automatic                                  it's automatic IT'S AUTOMATIC\n 7 Poetic Justice                                  poetic justice POETIC JUSTICE\n 8 A.D.H.D                                         a.d.h.d        A.D.H.D       \n 9 Ya Estuvo                                       ya estuvo      YA ESTUVO     \n10 Runnin (with A$AP Rocky, A$AP Ferg & Nicki Min… runnin (with … RUNNIN (WITH …\n\n# title is already in title case, so: \nstr_to_title(\"makes this into title case\")\n\n[1] \"Makes This Into Title Case\""
  },
  {
    "objectID": "08_strings_part1.html#matching-patterns",
    "href": "08_strings_part1.html#matching-patterns",
    "title": "Strings: Pre-class Video",
    "section": "Matching Patterns",
    "text": "Matching Patterns\nIn addition to manipulating strings, we might what to search through them to find matches. For example, can I find all the songs that start with M? The songs from 2016? The album titles that include a number?\n\nstr_view()\nThis function is helpful for viewing. It returns rows that contain the pattern you’re searching for, highlighting the pattern between &lt;.&gt; symbols and in a different color.\nThe first input is the vector, and the second input is the string/substring/pattern you are looking for.\n\nstr_view(spot_smaller$album_release_date, \"2016\")\n\n[1] │ &lt;2016&gt;-01-01\n[3] │ &lt;2016&gt;-04-23\n\nstr_view(spot_smaller$title, \"M\")\n\n [1] │ Hear &lt;M&gt;e Now\n [5] │ &lt;M&gt;y Oh &lt;M&gt;y (feat. DaBaby)\n[10] │ Runnin (with A$AP Rocky, A$AP Ferg & Nicki &lt;M&gt;inaj)\n\nstr_view(spot_smaller$subgenre, \"pop\")\n\n[1] │ indie &lt;pop&gt;timism\n[2] │ post-teen &lt;pop&gt;\n[3] │ hip &lt;pop&gt;\n[4] │ hip &lt;pop&gt;\n[5] │ latin &lt;pop&gt;\n\nstr_view(spot_smaller$subgenre, \"hip hop\")\n\n[6] │ latin &lt;hip hop&gt;\n[7] │ &lt;hip hop&gt;\n[8] │ southern &lt;hip hop&gt;\n[9] │ latin &lt;hip hop&gt;\n\n\n\n\nstr_subset()\nstr_subset() takes a vector input and returns a (usually shorter) vector output. Compare the output from str_view() and str_subset() here. Both of these functions can be hard to work with in a tibble.\n\nstr_view(spot_smaller$title, \"M\")\n\n [1] │ Hear &lt;M&gt;e Now\n [5] │ &lt;M&gt;y Oh &lt;M&gt;y (feat. DaBaby)\n[10] │ Runnin (with A$AP Rocky, A$AP Ferg & Nicki &lt;M&gt;inaj)\n\nstr_subset(spot_smaller$title, \"M\")\n\n[1] \"Hear Me Now\"                                      \n[2] \"My Oh My (feat. DaBaby)\"                          \n[3] \"Runnin (with A$AP Rocky, A$AP Ferg & Nicki Minaj)\"\n\n\n\n\nstr_detect()\nstr_detect takes a vector of strings (or single string) input and returns a vector of TRUE/FALSE (or single value). This makes it easy to work with in tibbles, using mutate or filter.\n\nstr_view(spot_smaller$title, \"M\")\n\n [1] │ Hear &lt;M&gt;e Now\n [5] │ &lt;M&gt;y Oh &lt;M&gt;y (feat. DaBaby)\n[10] │ Runnin (with A$AP Rocky, A$AP Ferg & Nicki &lt;M&gt;inaj)\n\nstr_detect(spot_smaller$title, \"M\")\n\n [1]  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE\n\nstr_detect(\"hello\", \"ll\")\n\n[1] TRUE\n\nspot_smaller |&gt; \n  select(title, album_name, artist) |&gt;\n  mutate(includes_M = str_detect(title, \"M\"))\n\n# A tibble: 10 × 4\n   title                                            album_name artist includes_M\n   &lt;chr&gt;                                            &lt;chr&gt;      &lt;chr&gt;  &lt;lgl&gt;     \n 1 Hear Me Now                                      Hear Me N… Alok   TRUE      \n 2 Run the World (Girls)                            4          Beyon… FALSE     \n 3 Formation                                        Lemonade   Beyon… FALSE     \n 4 7/11                                             BEYONCÉ [… Beyon… FALSE     \n 5 My Oh My (feat. DaBaby)                          Romance    Camil… TRUE      \n 6 It's Automatic                                   It's Auto… Frees… FALSE     \n 7 Poetic Justice                                   good kid,… Kendr… FALSE     \n 8 A.D.H.D                                          Section.80 Kendr… FALSE     \n 9 Ya Estuvo                                        Hispanic … Kid F… FALSE     \n10 Runnin (with A$AP Rocky, A$AP Ferg & Nicki Mina… Creed II:… Mike … TRUE      \n\nspot_smaller |&gt;  \n  select(title, album_name, artist) |&gt;\n  filter(str_detect(title, \"M\"))\n\n# A tibble: 3 × 3\n  title                                             album_name          artist  \n  &lt;chr&gt;                                             &lt;chr&gt;               &lt;chr&gt;   \n1 Hear Me Now                                       Hear Me Now         Alok    \n2 My Oh My (feat. DaBaby)                           Romance             Camila …\n3 Runnin (with A$AP Rocky, A$AP Ferg & Nicki Minaj) Creed II: The Album Mike Wi…\n\nspot_smaller |&gt; \n   select(title, album_name, artist, subgenre) |&gt;\n   filter(str_detect(subgenre, \"pop\"))\n\n# A tibble: 5 × 4\n  title                   album_name                 artist         subgenre    \n  &lt;chr&gt;                   &lt;chr&gt;                      &lt;chr&gt;          &lt;chr&gt;       \n1 Hear Me Now             Hear Me Now                Alok           indie popti…\n2 Run the World (Girls)   4                          Beyoncé        post-teen p…\n3 Formation               Lemonade                   Beyoncé        hip pop     \n4 7/11                    BEYONCÉ [Platinum Edition] Beyoncé        hip pop     \n5 My Oh My (feat. DaBaby) Romance                    Camila Cabello latin pop   \n\n\n\n\nstr_extract()\nstr_extract() takes a vector (or single) of strings input and returns a vector (or single) string output\n\nsingle_string\n\n[1] \"this is a string!\"\n\nstr_extract(single_string, \"this\")\n\n[1] \"this\"\n\n\nstr_extract() is more interesting when we want to identify a particular pattern to extract from the string.\nFor instance:\n\nstr_extract(\"find first vowel\", \"[aeiou]\")\n\n[1] \"i\"\n\nstr_extract(\"any numb3rs?\", \"\\\\d\")\n\n[1] \"3\"\n\nnumbers_here &lt;- c(\"numb3rs\", \"ar3\", \"h1d1ing\", \"almost\", \"ev3ryw4ere\")\n\nstr_extract(numbers_here, \"\\\\d\")\n\n[1] \"3\" \"3\" \"1\" NA  \"3\"\n\nstr_view(numbers_here, \"\\\\d\")\n\n[1] │ numb&lt;3&gt;rs\n[2] │ ar&lt;3&gt;\n[3] │ h&lt;1&gt;d&lt;1&gt;ing\n[5] │ ev&lt;3&gt;ryw&lt;4&gt;ere\n\n\nBecause str_extract returns a vector of the same length as its input, it also can be used within a tibble.\n\nspot_smaller |&gt;\n  select(title, artist, album_name) |&gt;\n  mutate(numbers = str_extract(album_name, \"\\\\d\"))\n\n# A tibble: 10 × 4\n   title                                             artist   album_name numbers\n   &lt;chr&gt;                                             &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;  \n 1 Hear Me Now                                       Alok     Hear Me N… &lt;NA&gt;   \n 2 Run the World (Girls)                             Beyoncé  4          4      \n 3 Formation                                         Beyoncé  Lemonade   &lt;NA&gt;   \n 4 7/11                                              Beyoncé  BEYONCÉ [… &lt;NA&gt;   \n 5 My Oh My (feat. DaBaby)                           Camila … Romance    &lt;NA&gt;   \n 6 It's Automatic                                    Freesty… It's Auto… &lt;NA&gt;   \n 7 Poetic Justice                                    Kendric… good kid,… &lt;NA&gt;   \n 8 A.D.H.D                                           Kendric… Section.80 8      \n 9 Ya Estuvo                                         Kid Fro… Hispanic … &lt;NA&gt;   \n10 Runnin (with A$AP Rocky, A$AP Ferg & Nicki Minaj) Mike Wi… Creed II:… &lt;NA&gt;   \n\n\nThe patterns we show here, “\\d” and “[aeiou]” are called regular expressions."
  },
  {
    "objectID": "08_strings_part1.html#regular-expressions",
    "href": "08_strings_part1.html#regular-expressions",
    "title": "Strings: Pre-class Video",
    "section": "Regular Expressions",
    "text": "Regular Expressions\nRegular expressions are a way to write general patterns… for instance the string “\\d” will find any digit (number). We can also specify whether we want the string to start or end with a certain letter.\nNotice the difference between the regular expression “M” and “^M”, “o” and “o$”\n\nstr_view(spot_smaller$title, \"M\")\n\n [1] │ Hear &lt;M&gt;e Now\n [5] │ &lt;M&gt;y Oh &lt;M&gt;y (feat. DaBaby)\n[10] │ Runnin (with A$AP Rocky, A$AP Ferg & Nicki &lt;M&gt;inaj)\n\nstr_view(spot_smaller$title, \"^M\")\n\n[5] │ &lt;M&gt;y Oh My (feat. DaBaby)\n\nstr_view(spot_smaller$title, \"o\")\n\n [1] │ Hear Me N&lt;o&gt;w\n [2] │ Run the W&lt;o&gt;rld (Girls)\n [3] │ F&lt;o&gt;rmati&lt;o&gt;n\n [6] │ It's Aut&lt;o&gt;matic\n [7] │ P&lt;o&gt;etic Justice\n [9] │ Ya Estuv&lt;o&gt;\n[10] │ Runnin (with A$AP R&lt;o&gt;cky, A$AP Ferg & Nicki Minaj)\n\nstr_view(spot_smaller$title, \"o$\")\n\n[9] │ Ya Estuv&lt;o&gt;\n\n\nBut how do I look for a dollar sign in my string? I use  to “escape” the special behavior of $. But  itself has special behavior… so I need two of them.\n\nstr_view(spot_smaller$title, \"\\\\$\")\n\n[10] │ Runnin (with A&lt;$&gt;AP Rocky, A&lt;$&gt;AP Ferg & Nicki Minaj)\n\n\n\nExample problem\nAre there any album names that contain numbers?\nstep 1: use str_view() to figure out an appropriate regular expression to use for searching.\n\nstr_view(spot_smaller$album_name, \"\\\\d\")\n\n[2] │ &lt;4&gt;\n[8] │ Section.&lt;8&gt;&lt;0&gt;\n\n\nstep 2: what kind of output do I want?\n\n# A list of the album names?\nstr_subset(spot_smaller$album_name, \"\\\\d\")\n\n[1] \"4\"          \"Section.80\"\n\n# A tibble? \nspot_smaller |&gt;\n  filter(str_detect(album_name, \"\\\\d\"))\n\n# A tibble: 2 × 6\n  title              artist album_release_date album_name subgenre playlist_name\n  &lt;chr&gt;              &lt;chr&gt;  &lt;chr&gt;              &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;        \n1 Run the World (Gi… Beyon… 2011-06-24         4          post-te… post-teen al…\n2 A.D.H.D            Kendr… 2011-07-02         Section.80 souther… Hip-Hop 'n R…"
  },
  {
    "objectID": "08_strings_part1.html#more-regular-expressions",
    "href": "08_strings_part1.html#more-regular-expressions",
    "title": "Strings: Pre-class Video",
    "section": "More regular expressions",
    "text": "More regular expressions\n[abc] - a, b, or c\n\nstr_view(spot_smaller$subgenre, \"[hp]op\")\n\n[1] │ indie &lt;pop&gt;timism\n[2] │ post-teen &lt;pop&gt;\n[3] │ hip &lt;pop&gt;\n[4] │ hip &lt;pop&gt;\n[5] │ latin &lt;pop&gt;\n[6] │ latin hip &lt;hop&gt;\n[7] │ hip &lt;hop&gt;\n[8] │ southern hip &lt;hop&gt;\n[9] │ latin hip &lt;hop&gt;\n\nstr_view(spot_smaller$subgenre, \"hip [hp]op\")\n\n[3] │ &lt;hip pop&gt;\n[4] │ &lt;hip pop&gt;\n[6] │ latin &lt;hip hop&gt;\n[7] │ &lt;hip hop&gt;\n[8] │ southern &lt;hip hop&gt;\n[9] │ latin &lt;hip hop&gt;\n\n\n[^abc] anything EXCEPT abc.\n\nstr_view(spot_smaller$album_name, \"[^\\\\d]\")\n\n [1] │ &lt;H&gt;&lt;e&gt;&lt;a&gt;&lt;r&gt;&lt; &gt;&lt;M&gt;&lt;e&gt;&lt; &gt;&lt;N&gt;&lt;o&gt;&lt;w&gt;\n [3] │ &lt;L&gt;&lt;e&gt;&lt;m&gt;&lt;o&gt;&lt;n&gt;&lt;a&gt;&lt;d&gt;&lt;e&gt;\n [4] │ &lt;B&gt;&lt;E&gt;&lt;Y&gt;&lt;O&gt;&lt;N&gt;&lt;C&gt;&lt;É&gt;&lt; &gt;&lt;[&gt;&lt;P&gt;&lt;l&gt;&lt;a&gt;&lt;t&gt;&lt;i&gt;&lt;n&gt;&lt;u&gt;&lt;m&gt;&lt; &gt;&lt;E&gt;&lt;d&gt;&lt;i&gt;&lt;t&gt;&lt;i&gt;&lt;o&gt;&lt;n&gt;&lt;]&gt;\n [5] │ &lt;R&gt;&lt;o&gt;&lt;m&gt;&lt;a&gt;&lt;n&gt;&lt;c&gt;&lt;e&gt;\n [6] │ &lt;I&gt;&lt;t&gt;&lt;'&gt;&lt;s&gt;&lt; &gt;&lt;A&gt;&lt;u&gt;&lt;t&gt;&lt;o&gt;&lt;m&gt;&lt;a&gt;&lt;t&gt;&lt;i&gt;&lt;c&gt;\n [7] │ &lt;g&gt;&lt;o&gt;&lt;o&gt;&lt;d&gt;&lt; &gt;&lt;k&gt;&lt;i&gt;&lt;d&gt;&lt;,&gt;&lt; &gt;&lt;m&gt;&lt;.&gt;&lt;A&gt;&lt;.&gt;&lt;A&gt;&lt;.&gt;&lt;d&gt;&lt; &gt;&lt;c&gt;&lt;i&gt;&lt;t&gt;&lt;y&gt;&lt; &gt;&lt;(&gt;&lt;D&gt;&lt;e&gt;&lt;l&gt;&lt;u&gt;&lt;x&gt;&lt;e&gt;&lt;)&gt;\n [8] │ &lt;S&gt;&lt;e&gt;&lt;c&gt;&lt;t&gt;&lt;i&gt;&lt;o&gt;&lt;n&gt;&lt;.&gt;80\n [9] │ &lt;H&gt;&lt;i&gt;&lt;s&gt;&lt;p&gt;&lt;a&gt;&lt;n&gt;&lt;i&gt;&lt;c&gt;&lt; &gt;&lt;C&gt;&lt;a&gt;&lt;u&gt;&lt;s&gt;&lt;i&gt;&lt;n&gt;&lt;g&gt;&lt; &gt;&lt;P&gt;&lt;a&gt;&lt;n&gt;&lt;i&gt;&lt;c&gt;\n[10] │ &lt;C&gt;&lt;r&gt;&lt;e&gt;&lt;e&gt;&lt;d&gt;&lt; &gt;&lt;I&gt;&lt;I&gt;&lt;:&gt;&lt; &gt;&lt;T&gt;&lt;h&gt;&lt;e&gt;&lt; &gt;&lt;A&gt;&lt;l&gt;&lt;b&gt;&lt;u&gt;&lt;m&gt;\n\nstr_view(spot_smaller$album_name, \"[^a-zA-Z ]\")\n\n [2] │ &lt;4&gt;\n [4] │ BEYONC&lt;É&gt; &lt;[&gt;Platinum Edition&lt;]&gt;\n [6] │ It&lt;'&gt;s Automatic\n [7] │ good kid&lt;,&gt; m&lt;.&gt;A&lt;.&gt;A&lt;.&gt;d city &lt;(&gt;Deluxe&lt;)&gt;\n [8] │ Section&lt;.&gt;&lt;8&gt;&lt;0&gt;\n[10] │ Creed II&lt;:&gt; The Album"
  },
  {
    "objectID": "08_strings_part1.html#bonus-content-not-in-the-pre-class-video",
    "href": "08_strings_part1.html#bonus-content-not-in-the-pre-class-video",
    "title": "Strings: Pre-class Video",
    "section": "Bonus content not in the pre-class video",
    "text": "Bonus content not in the pre-class video\n\nstr_glue()\nThis is a nice alternative to str_c(), where you only need a single set of quotes, and anything inside curly brackets {} is evaluated like it’s outside the quotes.\n\n# Thus, this code from earlier...\n\nspot_smaller\n\n# A tibble: 10 × 6\n   title             artist album_release_date album_name subgenre playlist_name\n   &lt;chr&gt;             &lt;chr&gt;  &lt;chr&gt;              &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;        \n 1 Hear Me Now       Alok   2016-01-01         Hear Me N… indie p… \"Chillout & …\n 2 Run the World (G… Beyon… 2011-06-24         4          post-te… \"post-teen a…\n 3 Formation         Beyon… 2016-04-23         Lemonade   hip pop  \"Feeling Acc…\n 4 7/11              Beyon… 2014-11-24         BEYONCÉ [… hip pop  \"Feeling Acc…\n 5 My Oh My (feat. … Camil… 2019-12-06         Romance    latin p… \"2020 Hits &…\n 6 It's Automatic    Frees… 2013-11-28         It's Auto… latin h… \"80's Freest…\n 7 Poetic Justice    Kendr… 2012               good kid,… hip hop  \"Hip Hop Con…\n 8 A.D.H.D           Kendr… 2011-07-02         Section.80 souther… \"Hip-Hop 'n …\n 9 Ya Estuvo         Kid F… 1990-01-01         Hispanic … latin h… \"HIP-HOP: La…\n10 Runnin (with A$A… Mike … 2018-11-16         Creed II:… gangste… \"RAP Gangsta\"\n\nsong_count &lt;- spot_smaller |&gt; \n  count(artist) |&gt;\n  slice_max(n, n = 1)\nsong_count\n\n# A tibble: 1 × 2\n  artist      n\n  &lt;chr&gt;   &lt;int&gt;\n1 Beyoncé     3\n\nstr_c(\"The artist with the most songs in spot_smaller is\", song_count$artist, \"with\", song_count$n, \"songs.\", sep = \" \")\n\n[1] \"The artist with the most songs in spot_smaller is Beyoncé with 3 songs.\"\n\n# ... becomes this:\n\nsong_count |&gt; mutate(statement = str_glue(\"The artist with the most songs in spot_smaller is {artist} with {n} songs.\"))\n\n# A tibble: 1 × 3\n  artist      n statement                                                       \n  &lt;chr&gt;   &lt;int&gt; &lt;glue&gt;                                                          \n1 Beyoncé     3 The artist with the most songs in spot_smaller is Beyoncé with …\n\n# or \n\nstr_glue(\"The artist with the most songs in spot_smaller is {song_count$artist} with {song_count$n} songs.\")\n\nThe artist with the most songs in spot_smaller is Beyoncé with 3 songs.\n\n\nstr_glue() can also be applied to an entire column vector:\n\nspot_smaller |&gt;\n  mutate(statement = str_glue(\"{artist} released {album_name} on {album_release_date}.\")) |&gt;\n  select(statement)\n\n# A tibble: 10 × 1\n   statement                                                       \n   &lt;glue&gt;                                                          \n 1 Alok released Hear Me Now on 2016-01-01.                        \n 2 Beyoncé released 4 on 2011-06-24.                               \n 3 Beyoncé released Lemonade on 2016-04-23.                        \n 4 Beyoncé released BEYONCÉ [Platinum Edition] on 2014-11-24.      \n 5 Camila Cabello released Romance on 2019-12-06.                  \n 6 Freestyle released It's Automatic on 2013-11-28.                \n 7 Kendrick Lamar released good kid, m.A.A.d city (Deluxe) on 2012.\n 8 Kendrick Lamar released Section.80 on 2011-07-02.               \n 9 Kid Frost released Hispanic Causing Panic on 1990-01-01.        \n10 Mike WiLL Made-It released Creed II: The Album on 2018-11-16.   \n\n\nAnd if you wanted to include {} in your statement, you can double up {} to serve as an escape character:\n\nspot_smaller |&gt;\n  mutate(statement = str_glue(\"{artist} released {album_name} on {album_release_date} {{according to Spotify}}.\")) |&gt;\n  select(statement)\n\n# A tibble: 10 × 1\n   statement                                                                    \n   &lt;glue&gt;                                                                       \n 1 Alok released Hear Me Now on 2016-01-01 {according to Spotify}.              \n 2 Beyoncé released 4 on 2011-06-24 {according to Spotify}.                     \n 3 Beyoncé released Lemonade on 2016-04-23 {according to Spotify}.              \n 4 Beyoncé released BEYONCÉ [Platinum Edition] on 2014-11-24 {according to Spot…\n 5 Camila Cabello released Romance on 2019-12-06 {according to Spotify}.        \n 6 Freestyle released It's Automatic on 2013-11-28 {according to Spotify}.      \n 7 Kendrick Lamar released good kid, m.A.A.d city (Deluxe) on 2012 {according t…\n 8 Kendrick Lamar released Section.80 on 2011-07-02 {according to Spotify}.     \n 9 Kid Frost released Hispanic Causing Panic on 1990-01-01 {according to Spotif…\n10 Mike WiLL Made-It released Creed II: The Album on 2018-11-16 {according to S…\n\n\n\n\nseparate_wider_delim() and its cousins\nWhen multiple variables are crammed together into a single string, the separate_ functions can be used to extract the pieces are produce additional rows (longer) or columns (wider). We show one such example below, using the optional “too_few” setting to diagnose issues after getting an error the first time.\n\nspot_smaller |&gt;\n  separate_wider_delim(\n    album_release_date,\n    delim = \"-\",\n    names = c(\"year\", \"month\", \"day\"),\n    too_few = \"debug\"\n  ) |&gt;\n  print(width = Inf)\n\nWarning: Debug mode activated: adding variables `album_release_date_ok`,\n`album_release_date_pieces`, and `album_release_date_remainder`.\n\n\n# A tibble: 10 × 12\n   title                                             artist            year \n   &lt;chr&gt;                                             &lt;chr&gt;             &lt;chr&gt;\n 1 Hear Me Now                                       Alok              2016 \n 2 Run the World (Girls)                             Beyoncé           2011 \n 3 Formation                                         Beyoncé           2016 \n 4 7/11                                              Beyoncé           2014 \n 5 My Oh My (feat. DaBaby)                           Camila Cabello    2019 \n 6 It's Automatic                                    Freestyle         2013 \n 7 Poetic Justice                                    Kendrick Lamar    2012 \n 8 A.D.H.D                                           Kendrick Lamar    2011 \n 9 Ya Estuvo                                         Kid Frost         1990 \n10 Runnin (with A$AP Rocky, A$AP Ferg & Nicki Minaj) Mike WiLL Made-It 2018 \n   month day   album_release_date album_release_date_ok\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;lgl&gt;                \n 1 01    01    2016-01-01         TRUE                 \n 2 06    24    2011-06-24         TRUE                 \n 3 04    23    2016-04-23         TRUE                 \n 4 11    24    2014-11-24         TRUE                 \n 5 12    06    2019-12-06         TRUE                 \n 6 11    28    2013-11-28         TRUE                 \n 7 &lt;NA&gt;  &lt;NA&gt;  2012               FALSE                \n 8 07    02    2011-07-02         TRUE                 \n 9 01    01    1990-01-01         TRUE                 \n10 11    16    2018-11-16         TRUE                 \n   album_release_date_pieces album_release_date_remainder\n                       &lt;int&gt; &lt;chr&gt;                       \n 1                         3 \"\"                          \n 2                         3 \"\"                          \n 3                         3 \"\"                          \n 4                         3 \"\"                          \n 5                         3 \"\"                          \n 6                         3 \"\"                          \n 7                         1 \"\"                          \n 8                         3 \"\"                          \n 9                         3 \"\"                          \n10                         3 \"\"                          \n   album_name                      subgenre        \n   &lt;chr&gt;                           &lt;chr&gt;           \n 1 Hear Me Now                     indie poptimism \n 2 4                               post-teen pop   \n 3 Lemonade                        hip pop         \n 4 BEYONCÉ [Platinum Edition]      hip pop         \n 5 Romance                         latin pop       \n 6 It's Automatic                  latin hip hop   \n 7 good kid, m.A.A.d city (Deluxe) hip hop         \n 8 Section.80                      southern hip hop\n 9 Hispanic Causing Panic          latin hip hop   \n10 Creed II: The Album             gangster rap    \n   playlist_name                                                              \n   &lt;chr&gt;                                                                      \n 1 \"Chillout & Remixes \\U0001f49c\"                                            \n 2 \"post-teen alternative, indie, pop (large variety)\"                        \n 3 \"Feeling Accomplished\"                                                     \n 4 \"Feeling Accomplished\"                                                     \n 5 \"2020 Hits & 2019  Hits – Top Global Tracks \\U0001f525\\U0001f525\\U0001f525\"\n 6 \"80's Freestyle/Disco Dance Party (Set Crossfade to 4-Seconds)\"            \n 7 \"Hip Hop Controller\"                                                       \n 8 \"Hip-Hop 'n RnB\"                                                           \n 9 \"HIP-HOP: Latin Rap ['89-present]\"                                         \n10 \"RAP Gangsta\"                                                              \n\nspot_smaller |&gt;\n  separate_wider_delim(\n    album_release_date,\n    delim = \"-\",\n    names = c(\"year\", \"month\", \"day\"),\n    too_few = \"align_start\"\n  )\n\n# A tibble: 10 × 8\n   title              artist year  month day   album_name subgenre playlist_name\n   &lt;chr&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;        \n 1 Hear Me Now        Alok   2016  01    01    Hear Me N… indie p… \"Chillout & …\n 2 Run the World (Gi… Beyon… 2011  06    24    4          post-te… \"post-teen a…\n 3 Formation          Beyon… 2016  04    23    Lemonade   hip pop  \"Feeling Acc…\n 4 7/11               Beyon… 2014  11    24    BEYONCÉ [… hip pop  \"Feeling Acc…\n 5 My Oh My (feat. D… Camil… 2019  12    06    Romance    latin p… \"2020 Hits &…\n 6 It's Automatic     Frees… 2013  11    28    It's Auto… latin h… \"80's Freest…\n 7 Poetic Justice     Kendr… 2012  &lt;NA&gt;  &lt;NA&gt;  good kid,… hip hop  \"Hip Hop Con…\n 8 A.D.H.D            Kendr… 2011  07    02    Section.80 souther… \"Hip-Hop 'n …\n 9 Ya Estuvo          Kid F… 1990  01    01    Hispanic … latin h… \"HIP-HOP: La…\n10 Runnin (with A$AP… Mike … 2018  11    16    Creed II:… gangste… \"RAP Gangsta\"\n\n\nIf there is a definable pattern, but the pattern is a bit weird, we can often use separate_wider_regex() to extract the correct values and build a tidy data set:\n\ndf &lt;- tribble(\n  ~str,\n  \"&lt;Sheryl&gt;-F_34\",\n  \"&lt;Kisha&gt;-F_45\", \n  \"&lt;Brandon&gt;-N_33\",\n  \"&lt;Sharon&gt;-F_38\", \n  \"&lt;Penny&gt;-F_58\",\n  \"&lt;Justin&gt;-M_41\", \n  \"&lt;Patricia&gt;-F_84\", \n)\n\ndf |&gt; \n  separate_wider_regex(\n    str,\n    patterns = c(\n      \"&lt;\", \n      name = \"[A-Za-z]+\", \n      \"&gt;-\", \n      gender = \".\",\n      \"_\",\n      age = \"[0-9]+\"\n    )\n  )\n\n# A tibble: 7 × 3\n  name     gender age  \n  &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;\n1 Sheryl   F      34   \n2 Kisha    F      45   \n3 Brandon  N      33   \n4 Sharon   F      38   \n5 Penny    F      58   \n6 Justin   M      41   \n7 Patricia F      84"
  },
  {
    "objectID": "10_strings_part3.html",
    "href": "10_strings_part3.html",
    "title": "Strings: Extra Practice",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(httr)"
  },
  {
    "objectID": "10_strings_part3.html#on-your-own---extra-practice-with-strings-and-regular-expressions",
    "href": "10_strings_part3.html#on-your-own---extra-practice-with-strings-and-regular-expressions",
    "title": "Strings: Extra Practice",
    "section": "On Your Own - Extra practice with strings and regular expressions",
    "text": "On Your Own - Extra practice with strings and regular expressions\n\nDescribe the equivalents of ?, +, * in {m,n} form.\nDescribe, in words, what the expression “(.)(.)\\2\\1” will match, and provide a word or expression as an example.\nProduce an R string which the regular expression represented by “\\..\\..\\..” matches. In other words, find a string y below that produces a TRUE in str_detect.\nSolve with str_subset(), using the words from stringr::words:\n\n\nFind all words that start or end with x.\nFind all words that start with a vowel and end with a consonant.\nFind all words that start and end with the same letter\n\n\nWhat words in stringr::words have the highest number of vowels? What words have the highest proportion of vowels? (Hint: what is the denominator?) Figure this out using the tidyverse and piping, starting with as_tibble(words) |&gt;.\nFrom the Harvard sentences data, use str_extract to produce a tibble with 3 columns: the sentence, the first word in the sentence, and the first word ending in “ed” (NA if there isn’t one).\nFind and output all contractions (words with apostrophes) in the Harvard sentences, assuming no sentence has multiple contractions.\nCarefully explain what the code below does, both line by line and in general terms.\n\n\ntemp &lt;- str_replace_all(words, \"^([A-Za-z])(.*)([a-z])$\", \"\\\\3\\\\2\\\\1\")\nas_tibble(words) |&gt;\n  semi_join(as_tibble(temp)) |&gt;\n  print(n = Inf)\n\nJoining with `by = join_by(value)`\n\n\n# A tibble: 45 × 1\n   value     \n   &lt;chr&gt;     \n 1 a         \n 2 america   \n 3 area      \n 4 dad       \n 5 dead      \n 6 deal      \n 7 dear      \n 8 depend    \n 9 dog       \n10 educate   \n11 else      \n12 encourage \n13 engine    \n14 europe    \n15 evidence  \n16 example   \n17 excuse    \n18 exercise  \n19 expense   \n20 experience\n21 eye       \n22 god       \n23 health    \n24 high      \n25 knock     \n26 lead      \n27 level     \n28 local     \n29 nation    \n30 no        \n31 non       \n32 on        \n33 rather    \n34 read      \n35 refer     \n36 remember  \n37 serious   \n38 stairs    \n39 test      \n40 tonight   \n41 transport \n42 treat     \n43 trust     \n44 window    \n45 yesterday"
  },
  {
    "objectID": "10_strings_part3.html#coco-and-rotten-tomatoes",
    "href": "10_strings_part3.html#coco-and-rotten-tomatoes",
    "title": "Strings: Extra Practice",
    "section": "Coco and Rotten Tomatoes",
    "text": "Coco and Rotten Tomatoes\nWe will check out the Rotten Tomatoes page for the 2017 movie Coco, scrape information from that page (we’ll get into web scraping in a few weeks!), clean it up into a usable format, and answer some questions using strings and regular expressions.\n\n# used to work\n# coco &lt;- read_html(\"https://www.rottentomatoes.com/m/coco_2017\")\n\nrobotstxt::paths_allowed(\"https://www.rottentomatoes.com/m/coco_2017\")\n\n\n www.rottentomatoes.com                      \n\n\n[1] TRUE\n\nlibrary(polite)\n\nWarning: package 'polite' was built under R version 4.3.3\n\ncoco &lt;- \"https://www.rottentomatoes.com/m/coco_2017\" |&gt;\n  bow() |&gt; scrape()\n\ntop_reviews &lt;- \"https://www.rottentomatoes.com/m/coco_2017/reviews?type=top_critics\" |&gt; \n  bow() |&gt; \n  scrape()\ntop_reviews &lt;- html_nodes(top_reviews, \".review-text\")\ntop_reviews &lt;- html_text(top_reviews)\n\nuser_reviews &lt;- \"https://www.rottentomatoes.com/m/coco_2017/reviews?type=user\" |&gt; \n  bow() |&gt; \n  scrape()\nuser_reviews &lt;- html_nodes(user_reviews, \".js-review-text\")\nuser_reviews &lt;- html_text(user_reviews)\n\n\ntop_reviews is a character vector containing the 20 most recent critic reviews (along with some other junk) for Coco, while user_reviews is a character vector with the 10 most recent user reviews.\n\n\nExplain how the code below helps clean up both user_reviews and top_reviews before we start using them.\n\n\nuser_reviews &lt;- str_trim(user_reviews)\ntop_reviews &lt;- str_trim(top_reviews)\n\n\nPrint out the critic reviews where the reviewer mentions “emotion” or “cry”. Think about various forms (“cried”, “emotional”, etc.) You may want to turn reviews to all lower case before searching for matches.\nIn critic reviews, replace all instances where “Pixar” is used with its full name: “Pixar Animation Studios”.\nFind out how many times each user uses “I” in their review. Remember that it could be used as upper or lower case, at the beginning, middle, or end of a sentence, etc.\nDo critics or users have more complex reviews, as measured by average number of commas used? Be sure your code weeds out commas used in numbers, such as “12,345”."
  },
  {
    "objectID": "12_webscraping.html",
    "href": "12_webscraping.html",
    "title": "Web scraping in R",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button."
  },
  {
    "objectID": "12_webscraping.html#getting-data-from-websites",
    "href": "12_webscraping.html#getting-data-from-websites",
    "title": "Web scraping in R",
    "section": "Getting data from websites",
    "text": "Getting data from websites"
  },
  {
    "objectID": "12_webscraping.html#there-are-typically-four-steps-to-scraping-data-with-functions-in-the-rvest-library",
    "href": "12_webscraping.html#there-are-typically-four-steps-to-scraping-data-with-functions-in-the-rvest-library",
    "title": "Web scraping in R",
    "section": "There are typically four steps to scraping data with functions in the rvest library:",
    "text": "There are typically four steps to scraping data with functions in the rvest library:\n\nrobots_text::paths_allowed() Check if the website allows scraping!\nread_html(). Input the URL containing the data and turn the html code into an XML file (another markup format that’s easier to work with).\nhtml_nodes(). Extract specific nodes from the XML file by using the CSS path that leads to the content of interest. (use css=“table” for tables.)\nhtml_text(). Extract content of interest from nodes. Might also use html_table() etc.\n\nThe CSS selector used in html_nodes() was derived using SelectorGadget (go to selectorgadget.com to install in your browser). You might also consider the fun CSS Selector tutorial at http://flukeout.github.io/.\nBefore scraping, we should always check first whether the website allows scraping. We should also consider if there’s any personal or confidential information, and we should be considerate to not overload the server we’re scraping from.\nSee here for an interesting example: https://techcrunch.com/2021/06/14/supreme-court-revives-linkedin-bid-to-protect-user-data-from-web-scrapers/"
  },
  {
    "objectID": "12_webscraping.html#when-the-data-is-already-in-table-form",
    "href": "12_webscraping.html#when-the-data-is-already-in-table-form",
    "title": "Web scraping in R",
    "section": "When the data is already in table form:",
    "text": "When the data is already in table form:\nIn this example, we will scrape climate data from this website: https://www.usclimatedata.com/climate/minneapolis/minnesota/united-states/usmn0503\nThe website already contains data in table form, so we use html_nodes(. , css = \"table\") and html_table()\n\n# checkout the website below first\nrobotstxt::paths_allowed(\"https://www.usclimatedata.com/climate/minneapolis/minnesota/united-states/usmn0503\")\n\n\n www.usclimatedata.com                      \n\n\n[1] TRUE\n\n# 1: read_html()\nmpls &lt;- read_html(\"https://www.usclimatedata.com/climate/minneapolis/minnesota/united-states/usmn0503\")\n\nUse the option css = \"table\" within html_nodes()\n\n# 2: html_nodes()\ntables &lt;- html_nodes(mpls, css = \"table\") \n\ntables  # have to guesstimate which table contains climate info\n\n{xml_nodeset (8)}\n[1] &lt;table id=\"monthly_table_one\" class=\"table table-hover tablesaw tablesaw- ...\n[2] &lt;table id=\"monthly_table_two\" class=\"table table-hover tablesaw tablesaw- ...\n[3] &lt;table class=\"table table-hover tablesaw tablesaw-mode-swipe mt-4 daily_t ...\n[4] &lt;table class=\"table table-hover tablesaw tablesaw-mode-swipe mt-4 history ...\n[5] &lt;table class=\"table table-striped table-hover tablesaw tablesaw-mode-swip ...\n[6] &lt;table class=\"table table-hover tablesaw geo_table\"&gt;\\n&lt;thead&gt;&lt;tr&gt;\\n&lt;th&gt; &lt; ...\n[7] &lt;table class=\"table table-hover tablesaw datetime_table\" data-tablesaw-hi ...\n[8] &lt;table class=\"table table-hover tablesaw monthly_summary_table\" data-tabl ...\n\n\nUse html_table() rather than html_text()\n\n# 3: html_table()\nhtml_table(tables, header = TRUE, fill = TRUE)    # find the right table\n\n[[1]]\n# A tibble: 6 × 7\n  ``                                    JanJa  FebFe  MarMa  AprAp  MayMa  JunJu\n  &lt;chr&gt;                                 &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Average high in ºF Av. high Hi         24    29     41     58     69     79   \n2 Average low in ºF Av. low Lo            8    13     24     37     49     59   \n3 Days with precipitation Days precip.…   8     7     11      9     11     13   \n4 Hours of sunshine Hours sun. Sun      140   166    200    231    272    302   \n5 Av. precipitation in inch Av. precip…   0.9   0.77   1.89   2.66   3.36   4.25\n6 Av. snowfall in inch Snowfall Sn       12     8     10      3      0      0   \n\n[[2]]\n# A tibble: 6 × 7\n  ``                                     JulJu AugAu  SepSe  OctOc  NovNo  DecDe\n  &lt;chr&gt;                                  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Average high in ºF Av. high Hi         83     80    72     58     41     27   \n2 Average low in ºF Av. low Lo           64     62    52     40     26     12   \n3 Days with precipitation Days precip.…  10     10     9      8      8      8   \n4 Hours of sunshine Hours sun. Sun      343    296   237    193    115    112   \n5 Av. precipitation in inch Av. precip…   4.04   4.3   3.08   2.43   1.77   1.16\n6 Av. snowfall in inch Snowfall Sn        0      0     0      1      9     12   \n\n[[3]]\n# A tibble: 31 × 7\n   Day    HighºF LowºF `Prec/moinch` `Prec/yrinch` `Snow/moinch` `Snow/yrinch`\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n 1 1 Jan    23.8   8.3          0.04          0.04          0.39           1  \n 2 2 Jan    23.7   8.2          0.08          0.08          0.71           1.8\n 3 3 Jan    23.6   8.1          0.12          0.12          1.1            2.8\n 4 4 Jan    23.5   7.9          0.12          0.12          1.5            3.8\n 5 5 Jan    23.5   7.8          0.16          0.16          1.81           4.6\n 6 6 Jan    23.4   7.7          0.2           0.2           2.2            5.6\n 7 7 Jan    23.4   7.6          0.24          0.24          2.6            6.6\n 8 8 Jan    23.3   7.5          0.28          0.28          3.11           7.9\n 9 9 Jan    23.3   7.4          0.28          0.28          3.5            8.9\n10 10 Jan   23.3   7.3          0.31          0.31          3.9            9.9\n# ℹ 21 more rows\n\n[[4]]\n# A tibble: 26 × 6\n   Day    HighºF LowºF Precip.inch Snowinch `Snow d.inch`\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;            &lt;dbl&gt;\n 1 01 Dec   32    19   0.07        1.61                 7\n 2 02 Dec   27    12   0.00        0.00                 6\n 3 03 Dec   37.9  19.9 0.00        0.00                 6\n 4 04 Dec   39    24.1 0.00        0.00                 6\n 5 05 Dec   37    21.9 0.00        0.00                 5\n 6 06 Dec   32    17.1 0.00        0.00                 5\n 7 07 Dec   42.1  21.9 0.00        0.00                 5\n 8 08 Dec   41    30.9 0.00        0.00                 5\n 9 09 Dec   34    -0.9 0.16        2.52                 5\n10 10 Dec    8.1  -4   T           T                    7\n# ℹ 16 more rows\n\n[[5]]\n# A tibble: 9 × 4\n  ``                                          `Dec 19`    ``    Normal     \n  &lt;chr&gt;                                       &lt;chr&gt;       &lt;lgl&gt; &lt;chr&gt;      \n1 \"Average high temperature Av. high temp.\"   \"29.9 ºF\"   NA    \"27 ºF\"    \n2 \"Average low temperature Av. low temp.\"     \"14.6 ºF\"   NA    \"12 ºF\"    \n3 \"Total precipitation Total precip.\"         \"0.39 inch\" NA    \"1.16 inch\"\n4 \"Total snowfall Total snowfall\"             \"6.33 inch\" NA    \"12 inch\"  \n5 \"\"                                          \"\"          NA    \"\"         \n6 \"Highest max temperature Highest max temp.\" \"44.1 ºF\"   NA    \"-\"        \n7 \"Lowest max temperature Lowest max temp.\"   \"8.1 ºF\"    NA    \"-\"        \n8 \"Highest min temperature Highest min temp.\" \"32.0 ºF\"   NA    \"-\"        \n9 \"Lowest min temperature Lowest min temp.\"   \"-5.1 ºF\"   NA    \"-\"        \n\n[[6]]\n# A tibble: 10 × 3\n   ``                   ``                ``   \n   &lt;chr&gt;                &lt;chr&gt;             &lt;lgl&gt;\n 1 Country              United States     NA   \n 2 State                Minnesota         NA   \n 3 County               Hennepin          NA   \n 4 City                 Minneapolis       NA   \n 5 Zip code             55401             NA   \n 6 Longitude            -93.27 dec. degr. NA   \n 7 Latitude             44.98 dec. degr.  NA   \n 8 Altitude - Elevation 840ft             NA   \n 9 ICAO                 -                 NA   \n10 IATA                 -                 NA   \n\n[[7]]\n# A tibble: 6 × 3\n  ``          ``              ``   \n  &lt;chr&gt;       &lt;chr&gt;           &lt;lgl&gt;\n1 Local Time  12:12 AM        NA   \n2 Sunrise     06:11 AM        NA   \n3 Sunset      08:10 PM        NA   \n4 Day / Night Night           NA   \n5 Timezone    Chicago -6:00   NA   \n6 Timezone DB America/Chicago NA   \n\n[[8]]\n# A tibble: 6 × 2\n  ``                         ``        \n  &lt;chr&gt;                      &lt;chr&gt;     \n1 Annual high temperature    55ºF      \n2 Annual low temperature     37ºF      \n3 Days per year with precip. 112 days  \n4 Annual hours of sunshine   2607 hours\n5 Average annual precip.     30.61 inch\n6 Av. annual snowfall        55 inch   \n\nmpls_data1 &lt;- html_table(tables, header = TRUE, fill = TRUE)[[1]]  \nmpls_data1\n\n# A tibble: 6 × 7\n  ``                                    JanJa  FebFe  MarMa  AprAp  MayMa  JunJu\n  &lt;chr&gt;                                 &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Average high in ºF Av. high Hi         24    29     41     58     69     79   \n2 Average low in ºF Av. low Lo            8    13     24     37     49     59   \n3 Days with precipitation Days precip.…   8     7     11      9     11     13   \n4 Hours of sunshine Hours sun. Sun      140   166    200    231    272    302   \n5 Av. precipitation in inch Av. precip…   0.9   0.77   1.89   2.66   3.36   4.25\n6 Av. snowfall in inch Snowfall Sn       12     8     10      3      0      0   \n\nmpls_data2 &lt;- html_table(tables, header = TRUE, fill = TRUE)[[2]]  \nmpls_data2\n\n# A tibble: 6 × 7\n  ``                                     JulJu AugAu  SepSe  OctOc  NovNo  DecDe\n  &lt;chr&gt;                                  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Average high in ºF Av. high Hi         83     80    72     58     41     27   \n2 Average low in ºF Av. low Lo           64     62    52     40     26     12   \n3 Days with precipitation Days precip.…  10     10     9      8      8      8   \n4 Hours of sunshine Hours sun. Sun      343    296   237    193    115    112   \n5 Av. precipitation in inch Av. precip…   4.04   4.3   3.08   2.43   1.77   1.16\n6 Av. snowfall in inch Snowfall Sn        0      0     0      1      9     12   \n\n\nEven after finding the correct tables, there may still be a lot of work to make it tidy!!! What is each line of code doing below?\n\nbind_cols(mpls_data1, mpls_data2) |&gt;\n  as_tibble() |&gt; \n  select(-`...8`) |&gt;\n  mutate(`...1` = str_extract(`...1`, \"[^ ]+ [^ ]+ [^ ]+\")) |&gt;\n  pivot_longer(cols = c(`JanJa`:`DecDe`), \n               names_to = \"month\", values_to = \"weather\") |&gt;\n  pivot_wider(names_from = `...1`, values_from = weather) |&gt;\n  mutate(month = str_sub(month, 1, 3))  |&gt;\n  rename(avg_high = \"Average high in\",\n         avg_low = \"Average low in\")\n\nNew names:\n• `` -&gt; `...1`\n• `` -&gt; `...8`\n\n\n# A tibble: 12 × 7\n   month avg_high avg_low `Days with precipitation` `Hours of sunshine`\n   &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;                     &lt;dbl&gt;               &lt;dbl&gt;\n 1 Jan         24       8                         8                 140\n 2 Feb         29      13                         7                 166\n 3 Mar         41      24                        11                 200\n 4 Apr         58      37                         9                 231\n 5 May         69      49                        11                 272\n 6 Jun         79      59                        13                 302\n 7 Jul         83      64                        10                 343\n 8 Aug         80      62                        10                 296\n 9 Sep         72      52                         9                 237\n10 Oct         58      40                         8                 193\n11 Nov         41      26                         8                 115\n12 Dec         27      12                         8                 112\n# ℹ 2 more variables: `Av. precipitation in` &lt;dbl&gt;, `Av. snowfall in` &lt;dbl&gt;\n\n# Probably want to rename the rest of the variables too!"
  },
  {
    "objectID": "12_webscraping.html#when-the-data-are-not-a-table",
    "href": "12_webscraping.html#when-the-data-are-not-a-table",
    "title": "Web scraping in R",
    "section": "When the data are NOT a table",
    "text": "When the data are NOT a table\nIn this example, we scrape the cast of the movie Coco from IMDb: https://www.imdb.com/title/tt2380307/\n\n# Verfiy site allows scraping\nrobotstxt::paths_allowed(\"https://www.imdb.com/title/tt2380307/\")\n\n\n www.imdb.com                      \n\n\n[1] TRUE\n\n# check if tables works first! \nread_html(\"https://www.imdb.com/title/tt2380307/\", css = \"table\") |&gt; \n  html_table()\n\nlist()\n\n\n\n# 1. Download the HTML and turn it into an XML file with read_html()\n#     - look at Coco page on imdb\ncoco &lt;- read_html(\"https://www.imdb.com/title/tt2380307/\")\n\n# 2. Extract specific nodes with html_nodes()\n#   - finding exact node (e.g. .dGCmsL) is the tricky part.  \n#     Among all the html code used to produce a webpage, where do you \n#     go to grab the content of interest?\n# One solution: selectorGadget (bookmark version; Chrome extension flukey)\ncast &lt;- html_nodes(coco, \".gCQkeh\")\ncast\n\n{xml_nodeset (18)}\n [1] &lt;a data-testid=\"title-cast-item__actor\" href=\"/name/nm5645519/?ref_=tt_c ...\n [2] &lt;a data-testid=\"title-cast-item__actor\" href=\"/name/nm0305558/?ref_=tt_c ...\n [3] &lt;a data-testid=\"title-cast-item__actor\" href=\"/name/nm0000973/?ref_=tt_c ...\n [4] &lt;a data-testid=\"title-cast-item__actor\" href=\"/name/nm0005513/?ref_=tt_c ...\n [5] &lt;a data-testid=\"title-cast-item__actor\" href=\"/name/nm0896149/?ref_=tt_c ...\n [6] &lt;a data-testid=\"title-cast-item__actor\" href=\"/name/nm0131781/?ref_=tt_c ...\n [7] &lt;a data-testid=\"title-cast-item__actor\" href=\"/name/nm0000778/?ref_=tt_c ...\n [8] &lt;a data-testid=\"title-cast-item__actor\" href=\"/name/nm0797567/?ref_=tt_c ...\n [9] &lt;a data-testid=\"title-cast-item__actor\" href=\"/name/nm0407101/?ref_=tt_c ...\n[10] &lt;a data-testid=\"title-cast-item__actor\" href=\"/name/nm0101603/?ref_=tt_c ...\n[11] &lt;a data-testid=\"title-cast-item__actor\" href=\"/name/nm0613872/?ref_=tt_c ...\n[12] &lt;a data-testid=\"title-cast-item__actor\" href=\"/name/nm3086285/?ref_=tt_c ...\n[13] &lt;a data-testid=\"title-cast-item__actor\" href=\"/name/nm0526090/?ref_=tt_c ...\n[14] &lt;a data-testid=\"title-cast-item__actor\" href=\"/name/nm0001579/?ref_=tt_c ...\n[15] &lt;a data-testid=\"title-cast-item__actor\" href=\"/name/nm2016172/?ref_=tt_c ...\n[16] &lt;a data-testid=\"title-cast-item__actor\" href=\"/name/nm3770662/?ref_=tt_c ...\n[17] &lt;a data-testid=\"title-cast-item__actor\" href=\"/name/nm0650969/?ref_=tt_c ...\n[18] &lt;a data-testid=\"title-cast-item__actor\" href=\"/name/nm0883609/?ref_=tt_c ...\n\n# 3. Extract content from nodes with html_text(), html_name(), \n#    html_attrs(), html_children(), html_table()\n# Usually will still need to do some stringr adjustments\nhtml_text(cast)\n\n [1] \"Anthony Gonzalez\"        \"Gael García Bernal\"     \n [3] \"Benjamin Bratt\"          \"Alanna Ubach\"           \n [5] \"Renee Victor\"            \"Jaime Camil\"            \n [7] \"Alfonso Arau\"            \"Herbert Siguenza\"       \n [9] \"Gabriel Iglesias\"        \"Lombardo Boyar\"         \n[11] \"Ana Ofelia Murguía\"      \"Natalia Cordova-Buckley\"\n[13] \"Selene Luna\"             \"Edward James Olmos\"     \n[15] \"Sofía Espinosa\"          \"Carla Medina\"           \n[17] \"Dyana Ortelli\"           \"Luis Valdez\"            \n\n\n\npipe it!\nYou can also write this altogether with a pipe:\n\nrobotstxt::paths_allowed(\"https://www.imdb.com/title/tt2380307/\")   #DON'T FORGET!\n\n\n www.imdb.com                      \n\n\n[1] TRUE\n\nread_html(\"https://www.imdb.com/title/tt2380307/\") |&gt;\n  html_nodes(\".gCQkeh\") |&gt;\n  html_text()\n\n [1] \"Anthony Gonzalez\"        \"Gael García Bernal\"     \n [3] \"Benjamin Bratt\"          \"Alanna Ubach\"           \n [5] \"Renee Victor\"            \"Jaime Camil\"            \n [7] \"Alfonso Arau\"            \"Herbert Siguenza\"       \n [9] \"Gabriel Iglesias\"        \"Lombardo Boyar\"         \n[11] \"Ana Ofelia Murguía\"      \"Natalia Cordova-Buckley\"\n[13] \"Selene Luna\"             \"Edward James Olmos\"     \n[15] \"Sofía Espinosa\"          \"Carla Medina\"           \n[17] \"Dyana Ortelli\"           \"Luis Valdez\""
  },
  {
    "objectID": "12_webscraping.html#another-option-fyi-but-honestly-this-is-harder",
    "href": "12_webscraping.html#another-option-fyi-but-honestly-this-is-harder",
    "title": "Web scraping in R",
    "section": "Another option, FYI, but honestly this is harder:",
    "text": "Another option, FYI, but honestly this is harder:\nDeveloper Tools in your browser (under … &gt; More Tools) can also be used to locate the desired content among the hierarchy of HTML tags, as in cast2. Click on the arrow in the upper left, then click on the desired content in the webpage. An element of html code will be highlighted in the Developer Tools; right click on this element and Copy &gt; XPath, which can finally be pasted into html_nodes() in your R script. Likely quite a bit of cleaning will still need to be done.\n\n# Developer Tools\n\n# cast2fail &lt;- html_nodes(coco, xpath = '//*[@id=\"titleCast\"]/table/tbody/tr[2]/td[2]/a')\n\ncoco |&gt;\n  html_nodes(xpath = '//*[@id=\"__next\"]/main/div/section[1]/div/section/div/div[1]/section[4]/div[2]/div[2]/div[1]/div[2]') |&gt; \n  html_text() \n\n[1] \"Anthony GonzalezMiguel(voice)\""
  },
  {
    "objectID": "12_webscraping.html#putting-multiple-columns-of-data-together.",
    "href": "12_webscraping.html#putting-multiple-columns-of-data-together.",
    "title": "Web scraping in R",
    "section": "Putting multiple columns of data together.",
    "text": "Putting multiple columns of data together.\nUse the rvest package to pull off data from imdb’s top grossing films released in 2017 at https://www.imdb.com/search/title?year=2017&title_type=feature&sort=boxoffice_gross_us,desc\n\nurl &lt;-\"https://www.imdb.com/search/title?year=2017&title_type=feature&sort=boxoffice_gross_us,desc\"\n\nrobotstxt::paths_allowed(url)   #DON'T FORGET!\n\n\n www.imdb.com                      \n\n\n[1] TRUE\n\ntitle &lt;- read_html(url) |&gt;\n  html_nodes(\".ipc-title-link-wrapper .ipc-title__text\") |&gt;\n  html_text()\n\ncontent_rating &lt;- read_html(url) |&gt;\n  html_nodes(\".dli-title-metadata-item:nth-child(3)\") |&gt;\n  html_text()\n\ndescription &lt;- read_html(url) |&gt;\n  html_nodes(\".ipc-html-content-inner-div\") |&gt;\n  html_text() |&gt;\n  str_trim()\n\n# use tibble() to put multiple columns together into a tibble\nmovies2017 &lt;- tibble(title, content_rating, description) |&gt; \n  mutate(year = 2017)\nmovies2017\n\n# A tibble: 50 × 4\n   title                                      content_rating description    year\n   &lt;chr&gt;                                      &lt;chr&gt;          &lt;chr&gt;         &lt;dbl&gt;\n 1 1. Star Wars: Episode VIII - The Last Jedi PG-13          Rey develops…  2017\n 2 2. Beauty and the Beast                    PG             A selfish Pr…  2017\n 3 3. Wonder Woman                            PG-13          When a pilot…  2017\n 4 4. Jumanji: Welcome to the Jungle          PG-13          Four teenage…  2017\n 5 5. Guardians of the Galaxy Vol. 2          PG-13          The Guardian…  2017\n 6 6. Spider-Man: Homecoming                  PG-13          Peter Parker…  2017\n 7 7. It                                      R              In the summe…  2017\n 8 8. Thor: Ragnarok                          PG-13          Imprisoned o…  2017\n 9 9. Despicable Me 3                         PG             Gru meets hi…  2017\n10 10. Justice League                         PG-13          Fueled by hi…  2017\n# ℹ 40 more rows\n\n\nI can just update the url to get another year!\n\nurl &lt;-\"https://www.imdb.com/search/title?year=2018&title_type=feature&sort=boxoffice_gross_us,desc\"\n\nrobotstxt::paths_allowed(url)   #DON'T FORGET!\n\n\n www.imdb.com                      \n\n\n[1] TRUE\n\ntitle &lt;- read_html(url) |&gt;\n  html_nodes(\".ipc-title-link-wrapper .ipc-title__text\") |&gt;\n  html_text()\n\ncontent_rating &lt;- read_html(url) |&gt;\n  html_nodes(\".dli-title-metadata-item:nth-child(3)\") |&gt;\n  html_text()\n\ndescription &lt;- read_html(url) |&gt;\n  html_nodes(\".ipc-html-content-inner-div\") |&gt;\n  html_text() |&gt;\n  str_trim()\n\n\ntibble(title, content_rating, description)\n\n# A tibble: 50 × 3\n   title                             content_rating description                 \n   &lt;chr&gt;                             &lt;chr&gt;          &lt;chr&gt;                       \n 1 1. Black Panther                  PG-13          T'Challa, heir to the hidde…\n 2 2. Avengers: Infinity War         PG-13          The Avengers and their alli…\n 3 3. Incredibles 2                  PG             The Incredibles family take…\n 4 4. Jurassic World: Fallen Kingdom PG-13          When the island's dormant v…\n 5 5. Aquaman                        PG-13          Arthur Curry, the human-bor…\n 6 6. Deadpool 2                     R              Foul-mouthed mutant mercena…\n 7 7. The Grinch                     PG             A grumpy Grinch plots to ru…\n 8 8. Mission: Impossible - Fallout  PG-13          Ethan Hunt and his IMF team…\n 9 9. Ant-Man and the Wasp           PG-13          As Scott Lang balances bein…\n10 10. Bohemian Rhapsody             PG-13          The story of the legendary …\n# ℹ 40 more rows\n\nmovies2018 &lt;- tibble(title, content_rating, description) |&gt; \n  mutate(year = 2018)\nmovies2018\n\n# A tibble: 50 × 4\n   title                             content_rating description             year\n   &lt;chr&gt;                             &lt;chr&gt;          &lt;chr&gt;                  &lt;dbl&gt;\n 1 1. Black Panther                  PG-13          T'Challa, heir to the…  2018\n 2 2. Avengers: Infinity War         PG-13          The Avengers and thei…  2018\n 3 3. Incredibles 2                  PG             The Incredibles famil…  2018\n 4 4. Jurassic World: Fallen Kingdom PG-13          When the island's dor…  2018\n 5 5. Aquaman                        PG-13          Arthur Curry, the hum…  2018\n 6 6. Deadpool 2                     R              Foul-mouthed mutant m…  2018\n 7 7. The Grinch                     PG             A grumpy Grinch plots…  2018\n 8 8. Mission: Impossible - Fallout  PG-13          Ethan Hunt and his IM…  2018\n 9 9. Ant-Man and the Wasp           PG-13          As Scott Lang balance…  2018\n10 10. Bohemian Rhapsody             PG-13          The story of the lege…  2018\n# ℹ 40 more rows\n\nbind_rows(movies2017, movies2018)\n\n# A tibble: 100 × 4\n   title                                      content_rating description    year\n   &lt;chr&gt;                                      &lt;chr&gt;          &lt;chr&gt;         &lt;dbl&gt;\n 1 1. Star Wars: Episode VIII - The Last Jedi PG-13          Rey develops…  2017\n 2 2. Beauty and the Beast                    PG             A selfish Pr…  2017\n 3 3. Wonder Woman                            PG-13          When a pilot…  2017\n 4 4. Jumanji: Welcome to the Jungle          PG-13          Four teenage…  2017\n 5 5. Guardians of the Galaxy Vol. 2          PG-13          The Guardian…  2017\n 6 6. Spider-Man: Homecoming                  PG-13          Peter Parker…  2017\n 7 7. It                                      R              In the summe…  2017\n 8 8. Thor: Ragnarok                          PG-13          Imprisoned o…  2017\n 9 9. Despicable Me 3                         PG             Gru meets hi…  2017\n10 10. Justice League                         PG-13          Fueled by hi…  2017\n# ℹ 90 more rows\n\n\nwhich means… I can write a function to scrape multiple years!\n\nscrape_moviedata &lt;- function(year){\n  url &lt;-str_c(\"https://www.imdb.com/search/title?year=\", year, \"&title_type=feature&sort=boxoffice_gross_us,desc\", sep = \"\")\n\nif(!robotstxt::paths_allowed(url)){stop(\"no scraping allowed!\")}  \n\ntitle &lt;- read_html(url) |&gt;\n  html_nodes(\".ipc-title-link-wrapper .ipc-title__text\") |&gt;\n  html_text()\n\ncontent_rating &lt;- read_html(url) |&gt;\n  html_nodes(\".dli-title-metadata-item:nth-child(3)\") |&gt;\n  html_text()\n\ndescription &lt;- read_html(url) |&gt;\n  html_nodes(\".ipc-html-content-inner-div\") |&gt;\n  html_text() |&gt;\n  str_trim()\n\n\ntibble(title, content_rating, description) |&gt; \n  mutate(year = year)\n}\n\n\nscrape_moviedata(2017)\n\n\n www.imdb.com                      \n\n\n# A tibble: 50 × 4\n   title                                      content_rating description    year\n   &lt;chr&gt;                                      &lt;chr&gt;          &lt;chr&gt;         &lt;dbl&gt;\n 1 1. Star Wars: Episode VIII - The Last Jedi PG-13          Rey develops…  2017\n 2 2. Beauty and the Beast                    PG             A selfish Pr…  2017\n 3 3. Wonder Woman                            PG-13          When a pilot…  2017\n 4 4. Jumanji: Welcome to the Jungle          PG-13          Four teenage…  2017\n 5 5. Guardians of the Galaxy Vol. 2          PG-13          The Guardian…  2017\n 6 6. Spider-Man: Homecoming                  PG-13          Peter Parker…  2017\n 7 7. It                                      R              In the summe…  2017\n 8 8. Thor: Ragnarok                          PG-13          Imprisoned o…  2017\n 9 9. Despicable Me 3                         PG             Gru meets hi…  2017\n10 10. Justice League                         PG-13          Fueled by hi…  2017\n# ℹ 40 more rows\n\nmoviedata &lt;- bind_rows(scrape_moviedata(2017),\n          scrape_moviedata(2018),\n          scrape_moviedata(2019))\n\n\n www.imdb.com                      \n\n\n www.imdb.com                      \n\n\n www.imdb.com                      \n\nmoviedata\n\n# A tibble: 150 × 4\n   title                                      content_rating description    year\n   &lt;chr&gt;                                      &lt;chr&gt;          &lt;chr&gt;         &lt;dbl&gt;\n 1 1. Star Wars: Episode VIII - The Last Jedi PG-13          Rey develops…  2017\n 2 2. Beauty and the Beast                    PG             A selfish Pr…  2017\n 3 3. Wonder Woman                            PG-13          When a pilot…  2017\n 4 4. Jumanji: Welcome to the Jungle          PG-13          Four teenage…  2017\n 5 5. Guardians of the Galaxy Vol. 2          PG-13          The Guardian…  2017\n 6 6. Spider-Man: Homecoming                  PG-13          Peter Parker…  2017\n 7 7. It                                      R              In the summe…  2017\n 8 8. Thor: Ragnarok                          PG-13          Imprisoned o…  2017\n 9 9. Despicable Me 3                         PG             Gru meets hi…  2017\n10 10. Justice League                         PG-13          Fueled by hi…  2017\n# ℹ 140 more rows\n\n\nBe careful about problems in 2020…\nIt looks like a lot of films did not report a gross revenue in 2020, probably because of the weirdness of the pandemic:\n2020 films sorted by popularity: https://www.imdb.com/search/title/?title_type=feature&year=2020-01-01,2020-12-31&sort=num_votes,desc\nIf information is missing, selectorgadget result will just skip it, so your rows won’t line up if you are trying to put multiple vectors together.\n\nscrape_moviedata(2020)   # uh oh\n\nread_html(\"https://www.imdb.com/search/title?year=2020&title_type=feature&sort=boxoffice_gross_us,desc\"\n) |&gt;\n  html_nodes(\".dli-title-metadata-item:nth-child(3)\") |&gt;\n  html_text()\n\n\nExercises\n\nUse the rvest package and html_table to read in the table of data found at the link here and create a scatterplot of land area versus the 2022 estimated population. I give you some starter code below; fill in the “???” and be sure you can explain what EVERY line of code does and why it’s necessary.\n\n\ncity_pop &lt;- read_html(\"https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\")\n\npop &lt;- html_nodes(???)\npop2 &lt;- html_table(pop, header = TRUE, fill = TRUE)[[???]]\npop3 &lt;- as_tibble(pop2[,c(1:6,8)]) |&gt;\n  ???? |&gt;    # removes first row\n  rename(`State` = `ST`,\n         `Estimate2022` = `2022estimate`,\n         `Census` = `2020census`,\n         `Area` = `2020 land area`,\n         `Density` = `2020 density`) |&gt;\n  mutate(Estimate2022 = parse_number(Estimate2022),\n         Census = parse_number(Census),\n         Change = ????,   # need to strip off % but not - sign\n                          # Hint: ifelse()\n         Area = parse_number(Area),\n         Density = parse_number(Density)) |&gt; \n  mutate(City = str_replace(City, \"???\", \"\"))\npop3\n\n# Identify unusual cities you'd like to label\noutliers &lt;- pop3 |&gt;   \n  filter(Estimate2022 &gt; ???? | Area &gt; ????)\n\nggplot(???) +\n  geom_point()  +\n  geom_smooth() +\n  ggrepel::geom_label_repel(data = outliers, aes(label = City))\n\n\nUse SelectorGadget and the rvest package to pull off data from the top 50 grossing films from 2018. Generate a tibble that contains the title, running time, star rating, and metascore for the top 50 films. You’ll have to clean your tibble so that columns are tidy and plot-worthy, titles include no extra characters, running time is in minutes, etc.\n\nCreate a scatterplot of star rating versus metascore. Identify movies that had big discrepancies between reviewers (metascore) and viewers (star rating).\nFor “bonus points”: do longer movies tend to have higher ratings? Answer with an appropriate plot.\n\nUse the httr package to build a data set from the omdb API with a different set of movies and a different set of variables than we used earlier. If you’re feeling ambitious, tidy up the resulting data set."
  },
  {
    "objectID": "github_links.html",
    "href": "github_links.html",
    "title": "GitHub Links",
    "section": "",
    "text": "Here are a few additional GitHub links that I’ve found helpful:\n\nUsing git and GitHub with RStudio (Lisa Lendway)\nGitHub with R projects (Lisa Lendway)\nGitHub starter course – basic terminology\nHappy Git and GitHub for the useR (Jenny Bryan)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MSCS 264: Data Science 2 (Spring 2024)",
    "section": "",
    "text": "Qurato\n\n\n\n\n\n\n\ntidyverse\n\n\n\n\n\n\nKey links for MSCS 264\n\nCourse syllabus\nRStudio server\nmoodle\nGitHub source code for this website"
  },
  {
    "objectID": "miniproject2.html",
    "href": "miniproject2.html",
    "title": "Mini-Project 2: Simulation",
    "section": "",
    "text": "Overview\nYou will conduct a small simulation study which does one of these tasks:\n\ninvestigates the effects of a factor or two on the power of a statistical test\ninvestigates the effects of condition violations on a statistical test, as measured by coverage rate, confidence interval width, type I error rate, etc.\nconducts a randomization-type test by simulating behavior under the null hypothesis\ndevelops a lineup protocol for visual inference, as in this paper by Adam Loy from Carleton\n\nYour simulation study should contain the following elements:\n\nat least 1 function you’ve written\nat least 1 loop (it can be imperative or functional, or you can try it both ways)\nat least 1 illustrative, well-labeled plot\na description of what insights can be gained from your plot(s)\n\n\n\nTimeline\nMini-Project 2 must be submitted on moodle by 11:00 PM on Fri Mar 22. Ideally, you’d add a tab to your quarto webpage with Mini-Project 2, then you can just submit your URL (as long as your code is linked to the page). You can alternatively submit a knitted pdf showing your code and output.\n\n\nAdditional Notes\nYou may choose to work solo or with a partner. If you choose to work with a partner, I ask each of you to hand in your final product (even though it’s likely the same thing) and also to describe your role in the project - was the coding 50/50? who generated ideas at various stages? who designed the final product? etc. If you work with someone else, ideally both of you would upload the final product to your own quarto webpages (with attribution to your co-author)"
  },
  {
    "objectID": "miniproject4.html",
    "href": "miniproject4.html",
    "title": "Mini-Project 4: SQL",
    "section": "",
    "text": "You will use SQL to query the Wideband Acoustic Immittance (WAI) Database hosted by Smith College. WAI measurements are being developed as noninvasive auditory diagnostic tools for people of all ages, and the WAI Database hosts WAI ear measurements that have been published in peer-review articles. The goal of the database is to “enable auditory researchers to share WAI measurements and combine analyses over multiple datasets.”\nYou have two primary goals:\n\nduplicate Figure 1 from a 2019 manuscript by Susan Voss. You will need to query the WAI Database to build a dataset which you can pipe into ggplot() to recreate Figure 1 as closely as possible.\nFind a study where subjects of different sex, race, ethnicity, or age groups were enrolled, and produce plots of frequency vs. mean absorption by group.\n\nYou should be using JOINs in both (1) and (2)."
  },
  {
    "objectID": "miniproject4.html#overview",
    "href": "miniproject4.html#overview",
    "title": "Mini-Project 4: SQL",
    "section": "",
    "text": "You will use SQL to query the Wideband Acoustic Immittance (WAI) Database hosted by Smith College. WAI measurements are being developed as noninvasive auditory diagnostic tools for people of all ages, and the WAI Database hosts WAI ear measurements that have been published in peer-review articles. The goal of the database is to “enable auditory researchers to share WAI measurements and combine analyses over multiple datasets.”\nYou have two primary goals:\n\nduplicate Figure 1 from a 2019 manuscript by Susan Voss. You will need to query the WAI Database to build a dataset which you can pipe into ggplot() to recreate Figure 1 as closely as possible.\nFind a study where subjects of different sex, race, ethnicity, or age groups were enrolled, and produce plots of frequency vs. mean absorption by group.\n\nYou should be using JOINs in both (1) and (2)."
  },
  {
    "objectID": "miniproject4.html#timeline",
    "href": "miniproject4.html#timeline",
    "title": "Mini-Project 4: SQL",
    "section": "Timeline",
    "text": "Timeline\nMini-project 4 is NOT required in Spring 2024. But feel free to give it a shot if it looks fun!"
  },
  {
    "objectID": "miniproject4.html#starter-code",
    "href": "miniproject4.html#starter-code",
    "title": "Mini-Project 4: SQL",
    "section": "Starter Code",
    "text": "Starter Code\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mdsr)\nlibrary(dbplyr)\n\n\nAttaching package: 'dbplyr'\n\nThe following objects are masked from 'package:dplyr':\n\n    ident, sql\n\nlibrary(DBI)\n\n\nlibrary(RMariaDB)\ncon &lt;- dbConnect(\n  MariaDB(), host = \"scidb.smith.edu\",\n  user = \"waiuser\", password = \"smith_waiDB\", \n  dbname = \"wai\"\n)\nMeasurements &lt;- tbl(con, \"Measurements\")\nPI_Info &lt;- tbl(con, \"PI_Info\")\nSubjects &lt;- tbl(con, \"Subjects\")\n\n# collect(Measurements)\n\n\nSHOW TABLES;\n\n\n7 records\n\n\nTables_in_wai\n\n\n\n\nCodebook\n\n\nMeasurements\n\n\nMeasurements_pre2020\n\n\nPI_Info\n\n\nPI_Info_OLD\n\n\nSubjects\n\n\nSubjects_pre2020\n\n\n\n\n\n\nDESCRIBE Measurements;\n\n\nDisplaying records 1 - 10\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nIdentifier\nvarchar(50)\nNO\nPRI\nNA\n\n\n\nSubjectNumber\nint\nNO\nPRI\nNA\n\n\n\nSession\nint\nNO\nPRI\nNA\n\n\n\nEar\nvarchar(50)\nNO\nPRI\n\n\n\n\nInstrument\nvarchar(50)\nNO\nPRI\n\n\n\n\nAge\nfloat\nYES\n\nNA\n\n\n\nAgeCategory\nvarchar(50)\nYES\n\nNA\n\n\n\nEarStatus\nvarchar(50)\nYES\n\nNA\n\n\n\nTPP\nfloat\nYES\n\nNA\n\n\n\nAreaCanal\nfloat\nYES\n\nNA\n\n\n\n\n\n\n::: {.cell}\nSELECT * FROM PI_Info LIMIT 0,10;\n\n\n\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentifier\nYear\nAuthors\nAuthorsShortList\nTitle\nJournal\nURL\nAbstract\nDataSubmitterName\nDataSubmitterEmail\nDateSubmitted\nPI_Notes\n\n\n\n\nAbur_2014\n2014\nDefne Abur, Nicholas J. Horton, and Susan E. Voss\nAbur et al.\nInstrasubject variability in power reflectance\nJ Am Acad Audiol\nhttps://www.ncbi.nlm.nih.gov/pubmed/25257718\n“\n\n\n\n\n\n\n\n\n Purpose:  This study investigates test-retest features of power reflectance, including comparisons of intrasubject versus intersubject variability and how ear-canal measurement location affects measurements.\n\n\n Research design:  Repeated measurements of power reflectance were made at about weekly intervals. The subjects returned for four to eight sessions. Measurements were made at three ear-canal locations: a deep insertion depth (with a foam plug flush at the entrance to the ear canal) and both 3 and 6 mm more lateral to this deep insertion.\n\n\n Study sample: Repeated measurements on seven subjects are reported. All subjects were female, between 19 and 22 yr old, and enrolled at an undergraduate women’s college.\n\n\n Data collection and analysis:  Measurements on both the right and left ears were made at three ear-canal locations during each of four to eight measurement sessions. Random-effects regression models were used for the analysis to account for repeated measures within subjects. The mean power reflectance for each position over all sessions was calculated for each subject.\n\n\n Results:  The comparison of power reflectance from the left and right ears of an individual subject varied greatly over the seven subjects; the difference between the power reflectance measured on the left and that measured on the right was compared at 248 frequencies, and depending on the subject, the percentage of tested frequencies for which the left and right ears differed significantly ranged from 10% to 93% (some with left values greater than right values and others with the opposite pattern). Although the individual subjects showed left-right differences, the overall population generally did not show significant differences between the left and right ears. The mean power reflectance for each measurement position over all sessions depended on the location of the probe in the ear for frequencies of less than 1000 Hz. The standard deviation between subjects’ mean power reflectance after controlling for ear (left or right) was found to be greater than the standard deviation within the individual subject’s mean power reflectance. The intrasubject standard deviation in power reflectance was smallest at the deepest insertion depths.\n\n\n Conclusions:  All subjects had differences in power reflectance between their left and right ears at some frequencies; the percentage of frequencies at which differences occurred varied greatly across subjects. The intrasubject standard deviations were smallest for the deepest probe insertion depths, suggesting clinical measurements should be made with as deep an insertion as practically possible to minimize variability. This deep insertion will reduce both acoustic leaks and the effect of low-frequency ear-canal losses. The within-subject standard deviations were about half the magnitude of the overall standard deviations, quantifying the extent of intrasubject versus intersubject variability.\n\n” |Susan Voss |svoss@smith.edu |24-Aug-2016 |Measurements made on 7 subjects across multiple sessions and 3 probe locations for each subject. Database includes measurements at only the deepest insertion depth (Position 1) and Channel B only. | |Aithal_2013 | 2013|Sreedevi Aithal, Joseph Kei, Carlie Driscoll, and Asaduzzaman Khan |Aithal et al. |Normative wideband reflectance measures in healthy neonates |Int. J. Pediatr. Otorhinolaryngol. |https://pubmed.ncbi.nlm.nih.gov/23047065/ |\n\n Objective:  Presently, normative wideband reflectance data are available for neonates who have passed a distortion product otoacoustic emission test. However, passing the distortion product otoacoustic emission test alone does not ensure normal middle ear function. The objective of this study was to establish normative wideband reflectance data in healthy neonates with normal middle ear function, as justified by passing a battery of tests.\n\n\n Method:  Wideband reflectance was measured in 66 infants (mean age=46.0 h, SD=21.0, range=13.3-116.5h) who passed a test battery that included high frequency (1000 Hz) tympanometry, acoustic stapedial reflex, transient evoked otoacoustic emissions and distortion product otoacoustic emissions.\n\n\n Results:  The analysis of variance (ANOVA) results showed significant variations of reflectance across the frequencies. There was no significant difference between ears and genders. The median reflectance reached a minimum of 0.21-0.24 at 1-2 kHz, but increased to 0.45-0.59 below 1 kHz and 0.24-0.52 above 2 kHz.\n\n\n Conclusions:  The normative reflectance data established in the present study were in agreement with, but marginally smaller than, those of previous normative studies, except for the Keefe et al. (2000) study. While the use of a test battery approach to ensure normal middle ear function in neonates has resulted in slightly reduced reflectance across most frequencies when compared to studies that have used only otoacoustic emissions, further research is needed to accurately determine the middle ear status of neonates using test performance measures.\n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |Sreedevi Aithal   |Sreedevi.aithal@health.qld.gov.au |2-Jan-2023    |                                                                                                                                                                                                      |\n|Aithal_2014 | 2014|Sreedevi Aithal, Joseph Kei, and Carlie Driscoll |Aithal et al. |Wideband Absorbance in Young Infants (0–6 months): A Cross-Sectional Study |J Am Acad Audiol |https://pubmed.ncbi.nlm.nih.gov/25257721/ |\n\n Background:  Wideband acoustic immittance (WAI) studies on infants have shown changes in WAI measures with age. These changes are attributed, at least in part, to developmental effects. However, developmental effects in young infants (0-6 mo) on WAI have not been systematically investigated.\n\n\n Purpose:  The objective of this study was to compare wideband absorbance (WBA) in healthy neonates and infants aged 1, 2, 4, and 6 mo.\n\n\n Research Design:  This was a prospective cross-sectional study. All participants were assessed by using 1-kHz tympanometry, distortion product otoacoustic emission (DPOAE) tests, and WBA tests.\n\n\n Study Sample  Participants included 35 newborns (35 ears), 16 infants aged 1 mo (29 ears), 16 infants aged 2 mo (29 ears), 15 infants aged 4 mo (28 ears), and 14 infants aged 6 mo (27 ears). For each participant, the ears that passed both high-frequency (1-kHz) tympanometry and DPOAE tests were included for analysis.\n\n\n Data collection and analysis:  WBA was recorded at ambient pressure conditions, and the response consisted of 16 data points at 1/3-octave frequencies from 0.25 to 8 kHz. A mixed-model analysis of variance (ANOVA) was applied to the data in each age group to evaluate the effects of sex, ear, and frequency on WBA. WBA was compared between various age groups. In addition, a separate mixed-model ANOVA was applied to WBA data, and post hoc analyses with the Bonferroni correction were performed at each of the 16 data points at 1/3-octave frequencies across age groups to examine the effect of age on WBA.\n\n\n Results:  For all age groups, WBA was highest between 1.5 and 5 kHz and lowest at frequencies of less than 1.5 kHz and greater than 5 kHz. A developmental trend was evident, with both the 0- and 6-mo-old infants being significantly different from other age groups at most frequencies. The WBA results exhibited a multipeaked pattern for infants aged 0 to 2 mo, whereas a single broad peaked pattern for 4- and 6-mo-old infants was observed. The difference in WBA between 0- and 6-mo-old infants was statistically significant across most frequencies. In contrast, the WBA results for 1- and 2-mo-old infants were comparable. There were no significant sex or ear effects on WBA for all age groups.\n\n\n Conclusions:  Developmental effects of WBA were evident for infants during the first 6 mo of life. The WBA data can be used as a reference for detecting disorders in the sound-conductive pathways (outer and middle ear) in young infants. Further development of age-specific normative WBA data in young infants is warranted.\n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |Sreedevi Aithal   |Sreedevi.aithal@health.qld.gov.au |2-Jan-2023    |                                                                                                                                                                                                      |\n|Aithal_2014b | 2014|Sreedevi Aithal, Joseph Kei, and Carlie Driscoll |Aithal et al. |Wideband Absorbance in Australian Aboriginal and Caucasian Neonates |J Am Acad Audiol |https://pubmed.ncbi.nlm.nih.gov/25257722 |\n\n Background:  Despite the high prevalence of otitis media in Australian Aboriginal infants and children, the conductive mechanism of the outer and middle ear of Aboriginal neonates remains unclear. Differences in characteristics of the conductive pathway (outer and middle ear) between Aboriginal and Caucasian neonates have not been systematically investigated by using wideband acoustic immittance measures.\n\n\n Purpose:  The objective of this study was to compare wideband absorbance (WBA) in Australian Aboriginal and Caucasian neonates who passed or failed a screening test battery containing high-frequency tympanometry and distortion product otoacoustic emissions (DPOAEs).\n\n\n Research Design:  A cross-sectional study design was used. The mean WBA as a function of frequency was compared between Aboriginal and non-Aboriginal neonates who passed or failed the test battery.\n\n\n Study Sample:  A total of 59 ears from 32 Aboriginal neonates (mean age, 51.9 h; standard deviation [SD], 18.2 h; range, 22–86 h) and 281 ears from 158 Caucasian neonates (mean age, 42.4 h; SD, 23.0 h; range, 8.1–152 h) who passed or failed 1000-Hz tympanometry and DPOAEs were included in the study.\n\n\n Data Collection and Analysis:    WBAresults were analyzed by using descriptive statistics and t tests with Bonferroni adjustment. An analysis of variance with repeated measures was applied to the data.\n\n\n Results:  Aboriginal and Caucasian neonates had almost identical pass rates of 61%, as determined by the test battery. Despite the apparently equal pass rates, the mean WBA of Aboriginal neonates who passed the test battery was significantly lower than that of their Caucasian counterparts at frequencies between 0.4 and 2 kHz. The mean WBA of Aboriginal neonates who failed the test battery was significantly lower than that of their Caucasian counterparts who also failed the test battery at frequencies between 1.5 and 3 kHz. Both Aboriginal and Caucasian neonates who failed the test battery had significantly lower WBA values than their counterparts who passed the test battery.\n\n\n Conclusions:  This study provided convincing evidence that Aboriginal neonates had significantly lower WBA values than their Caucasian counterparts, although both groups had equal pass rates, as determined by the test battery. Although the two ethnic groups showed significant differences in WBA, the factors contributing to such differences remain undetermined. Further research is warranted to determine the factors that might account for the difference in WBA between the two ethnic groups.\n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |Sreedevi Aithal   |Sreedevi.aithal@health.qld.gov.au |26-Feb-2023   |                                                                                                                                                                                                      |\n|Aithal_2015 | 2015|Sreedevi Aithal, Joseph Kei, Carlie Driscoll, Asaduzzaman Khan, and Andrew Swanston |Aithal et al. |Wideband Absorbance Outcomes in Newborns: A Comparison With High-Frequency Tympanometry, Automated Brainstem Response, and Transient Evoked and Distortion Product Otoacoustic Emissions |Ear Hear |https://pubmed.ncbi.nlm.nih.gov/25951046/ |\n\n Objectives:  The purpose of this study was to evaluate the test performance of wideband absorbance (WBA) in terms of its ability to predict the outer and middle ear status as determined by nine reference standards.\n\n\n Design:  Automated auditory brainstem response (AABR), high-frequency (1000 Hz) tympanometry (HFT), transient evoked otoacoustic emission (TEOAE), and distortion product otoacoustic emission (DPOAE) tests were performed on 298 ears (144 right, 154 left) of 192 (108 males, 84 females) neonates with a mean age of 43.7 hours (SD = 21.3, range = 8.3 to 152.2 hr). WBA was measured from 0.25 to 8 kHz using clicks under ambient pressure conditions. Test performance of WBA was assessed in terms of its ability to identify conductive conditions in neonates when compared with nine reference standards (including four single tests and five test batteries) using the receiver operating characteristic analysis.\n\n\n Results:  The test performance of WBA against the test battery reference standards was better than that against single test reference standards. The area under the receiver operating characteristic curve reached a high value of 0.78 for HFT + TEOAE + DPOAE and AABR + TEOAE + DPOAE reference standards. Within the ears that passed each of the reference standards, there were no significant differences in WBA. However, for the ears that failed each of the test standards, there were significant differences in WBA. The region between 1 and 4 kHz provided the best discriminability to evaluate the conductive status compared with other frequencies.\n\n\n Conclusions:  WBA is a desirable measure of conductive conditions in newborns due to its high performance in classifying ears with conductive loss as determined by the best performing surrogate gold standards (HFT + TEOAE + DPOAE and AABR + TEOAE + DPOAE).\n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |Sreedevi Aithal   |Sreedevi.Aithal@health.qld.gov.au |26-Feb-2023   |                                                                                                                                                                                                      |\n|Aithal_2017a | 2017|Sreedevi Aithal, Joseph Kei, Venkatesh Aithal, Alehandrea Manuel, Joshua Myers, Carlie Driscoll, and Asaduzzaman Khanb |Aithal et al. |Normative Study of Wideband Acoustic Immittance Measures in Newborn Infants |J Speech Lang Hear Res |https://pubmed.ncbi.nlm.nih.gov/28395306 |\n\n Objective:  The purpose of this study was to describe normative aspects of wideband acoustic immittance (WAI) measures obtained from healthy White neonates.\n\n\n Method:  In this cross-sectional study, wideband absorbance (WBA), admittance magnitude, and admittance phase were measured under ambient pressure condition in 326 ears from 203 neonates (M age = 45.9 hr) who passed a battery of tests, including automated auditory brainstem response, high-frequency tympanometry, and distortion product otoacoustic emissions.\n\n\n Results:  Normative WBA data were in agreement with most previous studies. Normative data for both WBA and admittance magnitude revealed double-peaked patterns with the 1st peak at 1.25–2 kHz and the 2nd peak at 5–8 kHz, while normative admittance phase data showed 2 peaks at 0.8 and 4 kHz. There were no significant differences between ears or gender for the 3 WAI measures. Standard deviations for all 3 measures were highest at frequencies above 4 kHz.\n\n\n Conclusions:  The 3 WAI measures between 1 kHz and 4 kHz may provide the most stable response of the outer and middle ear. WAI measures at frequencies above 4 kHz were more variable. The normative data established in the present study may serve as a reference for evaluating outer and middle ear function in neonates.\n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |Sreedevi Aithal   |Sreedevi.Aithal@health.qld.gov.au |17-Apr-2023   |                                                                                                                                                                                                      |\n|Aithal_2019a | 2019|Sreedevi Aithal and Venkatesh Aithal and Joseph Kei and Shane Anderson and Simon Liebenberg |Aithal et al. |Eustachian Tube Dysfunction and Wideband Absorbance Measurements at Tympanometric Peak Pressure and 0 daPa |J Am Acad Audiol |https://pubmed.ncbi.nlm.nih.gov/30424833/ |\n\n Background:  Although wideband absorbance (WBA) provides important information about middle ear function, there is limited research on the use of WBA to evaluate eustachian tube dysfunction (ETD). To date, WBA obtained under pressurized condition has not been used to evaluate ETD.\n\n\n Purpose:  The objective of the study was to compare WBA at 0 daPa and tympanometric peak pressure (TPP) conditions in healthy ears and ears with ETD.\n\n\n Research Design:  A cross-sectional study design was used.\n\n\n Study Sample:  A total of 102 healthy ears from 79 participants (mean age = 10.0 yr) and 43 ears from 32 patients with ETD (mean age = 16.0 yr) were included in this cross-sectional study. WBA was measured at 0 daPa (WBAo) and TPP WBA at TPP (WBATPP).\n\n\n Data collection and analysis:  WBA results were analyzed using descriptive statistics and t-tests with the Bonferroni correction. An analysis of variance with repeated measures was applied to the data.\n\n\n Results:  WBAo was significantly lower in the ETD group than in the control group. The WBAo of the control group demonstrated a broad peak between 1.25 and 4 kHz, whereas the WBAo of the ETD group had a peak between 2.5 and 4 kHz. WBATPP of the ETD group approached values close to that of the control group. In the control group, WBATPP was only 0.06 to 0.09 higher than WBAo, whereas in the ETD group, WBATPP was 0.29 to 0.42 higher than WBAo between 0.6 and 1.5 kHz. A differential pattern of WBA at TPP relative to 0 daPa was observed between ears with ETD and ears with otitis media with effusion (OME) and negative middle ear pressure (NMEP).\n\n\n Conclusions:  Hence, a comparison of WBAo and WBATPP can provide potentially useful diagnostic information, and hence can be used as an adjunct tool to evaluate ETD. This is important especially in young children or some adults who are unable to perform maneuvers such as Toynbee or Valsalva during ETD assessment. Further research is needed to verify the results using test performance measures to determine whether WBAo and WBATPP can objectively determine the presence of ETD or OME with NMEP.\n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |Sreedevi Aithal   |Sreedevi.Aithal@health.qld.gov.au |26-Feb-2023   |None                                                                                                                                                                                                  |\n|Aithal_2019b | 2019|Sreedevi Aithal, Venkatesh Aithal, Joseph Kei,and Alehandrea Manuela |Aithal et al. |Effect of Negative Middle Ear Pressure and Compensated Pressure on Wideband Absorbance and Otoacoustic Emissions in Children |J Speech Lang Hear Res |https://pubmed.ncbi.nlm.nih.gov/31437100/ |Objective This study investigated pressurized transient evoked otoacoustic emission (TEOAE) responses and wideband absorbance (WBA) in healthy ears and ears with negative middle ear pressure (NMEP). Method In this cross-sectional study, TEOAE amplitude, signal-to-noise ratio, and WBA were measured at ambient and tympanometric peak pressure (TPP) in 36 ears from 25 subjects with healthy ears (age range: 3.1-13.0 years) and 88 ears from 76 patients with NMEP (age range: 2.0-13.1 years), divided into 3 groups based on NMEP (Group 1 with TPP between -101 and -200 daPa, Group 2 with TPP between -201 and -300 daPa, and Group 3 with TPP between -301 and -400 daPa). Results Mean TEOAE amplitude, signal-to-noise ratio, and WBA were increased at TPP relative to that measured at ambient pressure between 0.8 and 1.5 kHz. Further decrease in TPP beyond -300 daPa did not result in further increases in the mean TEOAE or WBA at TPP. The correlation between TEOAE and WBA was dependent on the frequency, pressure conditions, and subject group. There was no difference in pass rates between the 2 pressure conditions for the control group, while the 3 NMEP groups demonstrated an improvement in pass rates at TPP. With pressurization, the false alarm rate for TEOAE due to NMEP was reduced by 17.8% for NMEP Group 1, 29.2% for NMEP Group 2, and 15.8% for NMEP Group 3. Conclusion Results demonstrated the feasibility and clinical benefits of measuring TEOAE and WBA under pressurized conditions. Pressurized TEOAE and WBA should be used for assessment of ears with NMEP in hearing screening programs to reduce false alarm rates. |Sreedevi Aithal |Sreedevi.Aithal@health.qld.gov.au |17-Apr-2023 | | |Aithal_2020a | 2020|Sreedevi Aithal, Venkatesh Aithal, Joseph Kei, and Matthew Wilson |Aithal et al. |Predictive Accuracy of Wideband Absorbance at Ambient and Tympanometric Peak Pressure Conditions in Identifying Children with Surgically Confirmed Otitis Media with Effusion |J Am Acad Audiol |https://pubmed.ncbi.nlm.nih.gov/31935192 |\n\n Background:  Wideband absorbance (WBA) measured at ambient pressure (WBAA) does not directly account for middle ear pressure effects. On the other hand, WBA measured at tympanometric peak pressure (TPP) (WBATPP) may compensate for the middle ear pressure effects. To date, there are no studies that have compared WBAA and WBATPP in ears with surgically confirmed otitis media with effusion (OME).\n\n\n Purpose:  The purpose of this study was to compare the predictive accuracy of WBAA and WBATPP in ears with OME.\n\n\n Research Design:  Prospective cross-sectional study.\n\n\n Study Sample:  A total of 60 ears from 38 healthy children (mean age 5 6.5 years, SD 5 1.84 years) and 60 ears from 38 children (mean age 5 5.5 years, SD 5 3.3 years) with confirmed OME during myringotomy were included in this study.\n\n\n Data collection and analysis:  Results were analyzed using descriptive statistics and analysis of variance. The predictive accuracy of WBAA and WBATPP was determined using receiver operating characteristics (ROC) analyses.\n\n\n Results:  Both WBAA and WBATPP were reduced in ears with OME compared with that in healthy ears. The area under the ROC (AROC) curve was 0.92 for WBAA at 1.5 kHz, whereas that for WBATPP at 1.25 kHz was 0.91. In comparison, the AROC for 226-Hz tympanometry based on the static acoustic admittance (Ytm) measure was 0.93.\n\n\n Conclusions:  Both WBAA and WBATPP showed high and similar test performance, but neither test performed significantly better than 226-Hz tympanometry for detection of surgically confirmed OME.\n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |Sreedevi Aithal   |Sreedevi.Aithal@health.qld.gov.au |24-Apr-2023   |                                                                                                                                                                                                      |\n|Aithal_2020b | 2020|Sreedevi Aithal, Venkatesh Aithal, Joseph Kei, and Shane Anderson |Aithal et al. |Wideband Absorbance in Ears with Retraction Pockets and Cholesteatomas: A Preliminary Study |J Am Acad Audiol |https://pubmed.ncbi.nlm.nih.gov/33588510/ |\n\n Objectives:  The objective of this study was to describe wideband absorbance (WBA)\n\n Objectives:  The objective of this study was to describe wideband absorbance (WBA) findings in patients with cholesteatomas and retraction pockets (RPs). Design In this prospective study, tympanometry, audiometry, and wideband tympanometry (WBT) were performed on 27 ears with an RP (eight with epitympanic RP and 19 ears with mesotympanic RP), 39 ears with a cholesteatoma (23 ears with epitympanic and 16 ears with mesotympanic cholesteatomas [MCs]), and 49 healthy ears serving as controls.\n\n\n Results:  Mean WBA at ambient pressure (WBAamb) of both experimental groups was reduced significantly between 0.8 and 5 kHz relative to the control group. The difference between mean WBAamb and mean WBA at tympanometric peak pressure (WBATPP) was greater for the RP (0.12–0.16 between 0.5 and 1.5 kHz) than for the cholesteatoma group (0.03–0.11 between 0.6 and 3kHz). Mean WBAamb of both epitympanic RP (ERP) and epitympanic cholesteatoma (EC) subgroups was significantly lower than that of the control group. Mean WBATPP of the ERP subgroup attained normal levels as per the control group, while mean WBATPP of EC subgroup was significantly lower than that of the control group at 0.8 to 1.5 kHz and 4 to 5 kHz. In contrast, both mesotympanic RP and MC subgroups demonstrated similar mean WBAamb and WBATPP values. No significant differences in WBAamb and WBATPP results between the RP and cholesteatomas groups were observed. Receiver operating characteristic (ROC) analyses indicated that the area under the ROC curve for distinguishing between the RP and cholesteatomas groups ranged from 0.44 to 0.60, indicating low accuracy in separating the two groups.\n\n\n Conclusions:  While it is not possible to distinguish between the RP and cholesteatomas groups based on the WBAamb and WBATPP results, it is potentially feasible to differentiate between the EC and ERP conditions. Further study using a large clinical sample is recommended to determine the sensitivity and specificity of theWBA test to identify the EC and ERP conditions.\n\nfindings in patients with cholesteatomas and retraction pockets (RPs). Design In this prospective study, tympanometry, audiometry, and wideband tympanometry (WBT) were performed on 27 ears with an RP (eight with epitympanic RP and 19 ears with mesotympanic RP), 39 ears with a cholesteatoma (23 ears with epitympanic and 16 ears with mesotympanic cholesteatomas [MCs]), and 49 healthy ears serving as controls.\n\n\n Results:  Mean WBA at ambient pressure (WBAamb) of both experimental groups was reduced significantly between 0.8 and 5 kHz relative to the control group. The difference between mean WBAamb and mean WBA at tympanometric peak pressure (WBATPP) was greater for the RP (0.12–0.16 between 0.5 and 1.5 kHz) than for the cholesteatoma group (0.03–0.11 between 0.6 and 3kHz). Mean WBAamb of both epitympanic RP (ERP) and epitympanic cholesteatoma (EC) subgroups was significantly lower than that of the control group. Mean WBATPP of the ERP subgroup attained normal levels as per the control group, while mean WBATPP of EC subgroup was significantly lower than that of the control group at 0.8 to 1.5 kHz and 4 to 5 kHz. In contrast, both mesotympanic RP and MC subgroups demonstrated similar mean WBAamb and WBATPP values. No significant differences in WBAamb and WBATPP results between the RP and cholesteatomas groups were observed. Receiver operating characteristic (ROC) analyses indicated that the area under the ROC curve for distinguishing between the RP and cholesteatomas groups ranged from 0.44 to 0.60, indicating low accuracy in separating the two groups.\n\n\n Conclusions:  While it is not possible to distinguish between the RP and cholesteatomas groups based on the WBAamb and WBATPP results, it is potentially feasible to differentiate between the EC and ERP conditions. Further study using a large clinical sample is recommended to determine the sensitivity and specificity of theWBA test to identify the EC and ERP conditions.\n\n|Sreedevi Aithal |Sreedevi.Aithal@health.qld.gov.au |17-Apr-2023 | |\n\n:::"
  },
  {
    "objectID": "miniproject4.html#hints",
    "href": "miniproject4.html#hints",
    "title": "Mini-Project 4: SQL",
    "section": "Hints",
    "text": "Hints\n\nParse the caption from Figure 1 carefully to determine what how mean absorbances are calculated: “Mean absorbances for the 12 studies within the WAI database as of July 1, 2019. Noted in the legend are the peer-reviewed publications associated with the datasets, the number of individual ears, and the equipment used in the study. When multiple measurements were made on the same ear, the average from those measurements was used in the calculation across subjects for a given study. Some subjects have measurements on both a right and a left ear, and some subjects have measurements from only one ear; this figure includes every ear in the database and does not control for the effect of number of ears from each subject.”\nfilter for only the 12 studies shown in Figure 1 (and also for frequencies shown in Figure 1)\nstudy the patterns of frequencies. It seems that most researchers used the same set of frequencies for each subject, ear, and session.\nnote the scale of the x-axis\nthe key labels contains AuthorsShortList, Year, and Instrument, in addition to the number of unique ears (I think Werner’s N may be incorrect?)"
  },
  {
    "objectID": "tech_setup.html",
    "href": "tech_setup.html",
    "title": "Tech Setup",
    "section": "",
    "text": "Ideally before class on Thurs Feb 8, and definitely before class on Tues Feb 13, you should follow these instructions to set up the software that we’ll be using throughout the semester. Even if you’ve already downloaded both R and RStudio, you’ll want to re-download to make sure that you have the most current versions.\n\nRequired: Download R and RStudio\n\nFIRST: Download R here.\n\nIn the top section, you will see three links “Download R for …”\nChoose the link that corresponds to your computer.\nAs of Feb 1, 2024, the latest version of R is 4.3.2 (“Eye Holes”).\n\nSECOND: Download RStudio here.\n\nClick the button under step 2 to install the version of RStudio recommended for your computer.\nAs of Feb 1, 2024, the latest version of RStudio is 2023.12.1 (Build 402).\n\nTHIRD: Check that when you go to File &gt; New Project &gt; New Directory, you see “Quarto Website” as an option.\n\n\nSuggested: Watch this video from Lisa Lendway at Macalester describing key configuration options for RStudio.\n\nSuggested: Change the default file download location for your internet browser.\n\nGenerally by default, internet browsers automatically save all files to the Downloads folder on your computer. In that case, you have to grab files from Downloads and move them to a more appropriate storage spot. You can change this option so that your browser asks you where to save each file before downloading it.\nThis page has information on how to do this for the most common browsers.\n\n\nRequired: Install required packages.\n\nAn R package is an extra bit of functionality that will help us in our data analysis efforts in a variety of ways. Many contributors create open source packages that can be added to base R to perform certain tasks in new and better ways.\nFor now, we’ll just make sure the tidyverse package is installed. Open RStudio and click on the Packages tab in the bottom right pane. inside the Console pane (by default, the bottom left pane). Click the Install button and type “tidyverse” (without quotes) in the pop-up box. Click the Install button at the bottom of the pop-up box.\nYou will see a lot of text from status messages appearing in the Console as the packages are being installed. Wait until you see the &gt; again.\nEnter the command library(tidyverse) in the Console and hit Enter.\n\nQuit RStudio. You’re done setting up!\n\n\nOptional: For a refresher on RStudio features, watch this video. It also shows you how to customize the layout and color scheme of RStudio."
  }
]