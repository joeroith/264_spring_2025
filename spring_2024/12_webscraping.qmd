---
title: "Web scraping in R"
format:
  html: default
editor_options: 
  chunk_output_type: console
---
  
You can download this .qmd file from [here](https://github.com/proback/264_fall_2024/blob/main/12_webscraping.qmd).  Just hit the Download Raw File button.

```{r}
#| include: FALSE

library(tidyverse)
library(stringr)
library(rvest)
library(httr)

# Starter steps:
#  - install SelectorGadget in browser
#  - explore imdb.com and omdbapi.com
#  - request API key from omdbapi.com
```

## Getting data from websites

# Option 1: APIs

APIs are Application Programming Interfaces, instructions for how programs should interact with your software.  For our purposes of obtaining data, APIs exist where website developers make data nicely packaged for consumption.  The language HTTP (hypertext transfer protocol) underlies APIs, and the R package `httr()` was written to map closely to HTTP with R.

Essentially you send a request to the website (server) where you want data from, and they send a response, which should contain the data (plus other stuff).

This is a really quick introduction, just to get you started and show you what's possible.  For more information, these links can be somewhat helpful:

- https://cran.r-project.org/web/packages/httr/vignettes/quickstart.html
- https://towardsdatascience.com/functions-with-r-and-rvest-a-laymens-guide-acda42325a77
- https://www.analyticsvidhya.com/blog/2017/03/beginners-guide-on-web-scraping-in-r-using-rvest-with-hands-on-knowledge/
- https://nceas.github.io/oss-lessons/data-liberation/intro-webscraping.html

Plus this website with a list of public APIs:

- https://github.com/toddmotto/public-apis

There are also many R packages that provide a set of functions to communicate with a particular API, such as: ZillowR, rtweet, genius, Rspotify, tidycensus, etc.

Here's an example of getting data from a website that attempts to make imdb movie data available as an API.

```{r}
#| eval: FALSE

myapikey <- "YOUR API KEY"  # enter your API key for omdbapi.com 
                            # (must obtain personal key)

# default is JSON = JavaScript Object Notation, 
#  which is standard data format for APIs
# - URL obtained by searching for Coco in 2017 at omdbapi.com
# - this part is optional: plot=short&r=json&
url <- str_c("http://www.omdbapi.com/?t=Coco&y=2017&apikey=", myapikey)

coco <- GET(url)   # coco holds response from server
coco               # Status of 200 is good!

details <- content(coco, "parse")   
details                             # get a list of 25 pieces of information
details$Year                        # how to access details
details[[2]]                        # since a list, another way to access
```

```{r}
#| eval: FALSE

# Build a data set for a set of movies

# Must figure out pattern in URL for obtaining different movies
#  - try searching for others
movies <- c("Coco", "Wonder+Woman", "Get+Out", 
            "The+Greatest+Showman", "Thor:+Ragnarok")

# Set up empty tibble
omdb <- tibble(Title = character(), Rated = character(), Genre = character(),
       Actors = character(), Metascore = double(), imdbRating = double(),
       BoxOffice = double())

# Use for loop to run through API request process 5 times,
#   each time filling the next row in the tibble
#  - can do max of 1000 GETs per day
for(i in 1:5) {
  url <- str_c("http://www.omdbapi.com/?t=",movies[i],
               "&apikey=", myapikey)
  onemovie <- GET(url)
  details <- content(onemovie, "parse")
  omdb[i,1] <- details$Title
  omdb[i,2] <- details$Rated
  omdb[i,3] <- details$Genre
  omdb[i,4] <- details$Actors
  omdb[i,5] <- parse_number(details$Metascore)
  omdb[i,6] <- parse_number(details$imdbRating)
  omdb[i,7] <- parse_number(details$BoxOffice)   # no $ and ,'s
}

omdb

# Here's what the resulting tibble looks like:

## # A tibble: 5 x 7
##   Title    Rated Genre      Actors          Metascore imdbRating BoxOffice
## * <chr>    <chr> <chr>      <chr>               <dbl>      <dbl>     <dbl>
## 1 Coco     PG    Animation… Anthony Gonzal…        81        8.4 208487719
## 2 Wonder … PG-13 Action, A… Gal Gadot, Chr…        76        7.5 412400625
## 3 Get Out  R     Horror, M… Daniel Kaluuya…        84        7.7 175428355
## 4 The Gre… PG    Biography… Hugh Jackman, …        48        7.7 164616443
## 5 Thor: R… PG-13 Action, A… Chris Hemswort…        74        7.9 314971245

#  could use stringr functions to further organize this data - separate 
#    different genres, different actors, etc.
```

# Option 2: rvest for web scraping

If you would like to assemble data from a website with no API, you can often acquire data using more brute force methods commonly called web scraping.  Typically, this involves finding content inside HTML (Hypertext markup language) code used for creating webpages and web applications and the CSS (Cascading style sheets) language for customizing the appearance of webpages. 
We are used to reading data from .csv files.... but most websites have it stored in XML (like html, but for data). You can read more about it here if you're interested: https://www.w3schools.com/xml/default.asp

XML has a sort of tree or graph-like structure... so we can identify information by which `node` it belongs to (`html_nodes`) and then convert the content into something we can use in R (`html_text` or `html_table`).

## There are typically four steps to scraping data with functions in the `rvest` library:

0. `robots_text::paths_allowed()` Check if the website allows scraping!
1. `read_html()`.  Input the URL containing the data and turn the html code into an XML file (another markup format that's easier to work with).
2. `html_nodes()`.  Extract specific nodes from the XML file by using the CSS path that leads to the content of interest. (use css="table" for tables.)
3. `html_text()`.  Extract content of interest from nodes.  Might also use `html_table()` etc.

The CSS selector used in `html_nodes()` was derived using *SelectorGadget* (go to selectorgadget.com to install in your browser).  You might also consider the fun CSS Selector tutorial at http://flukeout.github.io/.  

Before scraping, we should always check first whether the website allows scraping.  We should also consider if there's any personal or confidential information, and we should be considerate to not overload the server we're scraping from.

See here for an interesting example: https://techcrunch.com/2021/06/14/supreme-court-revives-linkedin-bid-to-protect-user-data-from-web-scrapers/


## When the data is already in table form:

In this example, we will scrape climate data from this website: https://www.usclimatedata.com/climate/minneapolis/minnesota/united-states/usmn0503

The website already contains data in table form, so we use `html_nodes(. , css = "table")` and `html_table()`

```{r}
# checkout the website below first
robotstxt::paths_allowed("https://www.usclimatedata.com/climate/minneapolis/minnesota/united-states/usmn0503")

# 1: read_html()
mpls <- read_html("https://www.usclimatedata.com/climate/minneapolis/minnesota/united-states/usmn0503")
```

Use the option `css = "table"` within html_nodes()

```{r}
# 2: html_nodes()
tables <- html_nodes(mpls, css = "table") 

tables  # have to guesstimate which table contains climate info
```

Use html_table() rather than html_text()

```{r}
# 3: html_table()
html_table(tables, header = TRUE, fill = TRUE)    # find the right table

mpls_data1 <- html_table(tables, header = TRUE, fill = TRUE)[[1]]  
mpls_data1
mpls_data2 <- html_table(tables, header = TRUE, fill = TRUE)[[2]]  
mpls_data2
```

Even after finding the correct tables, there may still be a lot of work to make it tidy!!!  What is each line of code doing below?

```{r}
bind_cols(mpls_data1, mpls_data2) |>
  as_tibble() |> 
  select(-`...8`) |>
  mutate(`...1` = str_extract(`...1`, "[^ ]+ [^ ]+ [^ ]+")) |>
  pivot_longer(cols = c(`JanJa`:`DecDe`), 
               names_to = "month", values_to = "weather") |>
  pivot_wider(names_from = `...1`, values_from = weather) |>
  mutate(month = str_sub(month, 1, 3))  |>
  rename(avg_high = "Average high in",
         avg_low = "Average low in")

# Probably want to rename the rest of the variables too!
```


## When the data are NOT a table

In this example, we scrape the cast of the movie Coco from IMDb: https://www.imdb.com/title/tt2380307/

```{r}
# Verfiy site allows scraping
robotstxt::paths_allowed("https://www.imdb.com/title/tt2380307/")

# check if tables works first! 
read_html("https://www.imdb.com/title/tt2380307/", css = "table") |> 
  html_table()
```

```{r}
# 1. Download the HTML and turn it into an XML file with read_html()
#     - look at Coco page on imdb
coco <- read_html("https://www.imdb.com/title/tt2380307/")

# 2. Extract specific nodes with html_nodes()
#   - finding exact node (e.g. .dGCmsL) is the tricky part.  
#     Among all the html code used to produce a webpage, where do you 
#     go to grab the content of interest?
# One solution: selectorGadget (bookmark version; Chrome extension flukey)
cast <- html_nodes(coco, ".gCQkeh")
cast

# 3. Extract content from nodes with html_text(), html_name(), 
#    html_attrs(), html_children(), html_table()
# Usually will still need to do some stringr adjustments
html_text(cast)
```

### pipe it!

You can also write this altogether with a pipe:

```{r}
robotstxt::paths_allowed("https://www.imdb.com/title/tt2380307/")   #DON'T FORGET!

read_html("https://www.imdb.com/title/tt2380307/") |>
  html_nodes(".gCQkeh") |>
  html_text()
```


##  Another option, FYI, but honestly this is harder:

*Developer Tools* in your browser (under ... > More Tools) can also be used to locate the desired content among the hierarchy of HTML tags, as in cast2.  Click on the arrow in the upper left, then click on the desired content in the webpage.  An element of html code will be highlighted in the Developer Tools; right click on this element and Copy > XPath, which can finally be pasted into html_nodes() in your R script.  Likely quite a bit of cleaning will still need to be done.

```{r}
# Developer Tools

# cast2fail <- html_nodes(coco, xpath = '//*[@id="titleCast"]/table/tbody/tr[2]/td[2]/a')

coco |>
  html_nodes(xpath = '//*[@id="__next"]/main/div/section[1]/div/section/div/div[1]/section[4]/div[2]/div[2]/div[1]/div[2]') |> 
  html_text() 
```


## Putting multiple columns of data together.

Use the `rvest` package to pull off data from imdb's top grossing films released in 2017 at https://www.imdb.com/search/title?year=2017&title_type=feature&sort=boxoffice_gross_us,desc

```{r}
url <-"https://www.imdb.com/search/title?year=2017&title_type=feature&sort=boxoffice_gross_us,desc"

robotstxt::paths_allowed(url)   #DON'T FORGET!

title <- read_html(url) |>
  html_nodes(".ipc-title-link-wrapper .ipc-title__text") |>
  html_text()

content_rating <- read_html(url) |>
  html_nodes(".dli-title-metadata-item:nth-child(3)") |>
  html_text()

description <- read_html(url) |>
  html_nodes(".ipc-html-content-inner-div") |>
  html_text() |>
  str_trim()

# use tibble() to put multiple columns together into a tibble
movies2017 <- tibble(title, content_rating, description) |> 
  mutate(year = 2017)
movies2017
```

I can just update the url to get another year!

```{r}
url <-"https://www.imdb.com/search/title?year=2018&title_type=feature&sort=boxoffice_gross_us,desc"

robotstxt::paths_allowed(url)   #DON'T FORGET!

title <- read_html(url) |>
  html_nodes(".ipc-title-link-wrapper .ipc-title__text") |>
  html_text()

content_rating <- read_html(url) |>
  html_nodes(".dli-title-metadata-item:nth-child(3)") |>
  html_text()

description <- read_html(url) |>
  html_nodes(".ipc-html-content-inner-div") |>
  html_text() |>
  str_trim()


tibble(title, content_rating, description)

movies2018 <- tibble(title, content_rating, description) |> 
  mutate(year = 2018)
movies2018


bind_rows(movies2017, movies2018)
```

which means... I can write a function to scrape multiple years! 

```{r}
scrape_moviedata <- function(year){
  url <-str_c("https://www.imdb.com/search/title?year=", year, "&title_type=feature&sort=boxoffice_gross_us,desc", sep = "")

if(!robotstxt::paths_allowed(url)){stop("no scraping allowed!")}  

title <- read_html(url) |>
  html_nodes(".ipc-title-link-wrapper .ipc-title__text") |>
  html_text()

content_rating <- read_html(url) |>
  html_nodes(".dli-title-metadata-item:nth-child(3)") |>
  html_text()

description <- read_html(url) |>
  html_nodes(".ipc-html-content-inner-div") |>
  html_text() |>
  str_trim()


tibble(title, content_rating, description) |> 
  mutate(year = year)
}


scrape_moviedata(2017)


moviedata <- bind_rows(scrape_moviedata(2017),
          scrape_moviedata(2018),
          scrape_moviedata(2019))

moviedata
```

Be careful about problems in 2020... 

It looks like a lot of films did not report a gross revenue in 2020, probably because of the weirdness of the pandemic:

2020 films sorted by popularity: https://www.imdb.com/search/title/?title_type=feature&year=2020-01-01,2020-12-31&sort=num_votes,desc

If information is missing, selectorgadget result will just skip it, so your rows won't line up if you are trying to put multiple vectors together.

```{r}
#| eval: FALSE

scrape_moviedata(2020)   # uh oh

read_html("https://www.imdb.com/search/title?year=2020&title_type=feature&sort=boxoffice_gross_us,desc"
) |>
  html_nodes(".dli-title-metadata-item:nth-child(3)") |>
  html_text()
```


### Exercises

1. Use the `rvest` package and `html_table` to read in the table of data found at the link [here](https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population) and create a scatterplot of land area versus the 2022 estimated population.  I give you some starter code below; fill in the "???" and be sure you can explain what EVERY line of code does and why it's necessary.

```{r}
#| eval: FALSE

city_pop <- read_html("https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population")

pop <- html_nodes(???)
pop2 <- html_table(pop, header = TRUE, fill = TRUE)[[???]]
pop3 <- as_tibble(pop2[,c(1:6,8)]) |>
  ???? |>    # removes first row
  rename(`State` = `ST`,
         `Estimate2022` = `2022estimate`,
         `Census` = `2020census`,
         `Area` = `2020 land area`,
         `Density` = `2020 density`) |>
  mutate(Estimate2022 = parse_number(Estimate2022),
         Census = parse_number(Census),
         Change = ????,   # need to strip off % but not - sign
                          # Hint: ifelse()
         Area = parse_number(Area),
         Density = parse_number(Density)) |> 
  mutate(City = str_replace(City, "???", ""))
pop3

# Identify unusual cities you'd like to label
outliers <- pop3 |>   
  filter(Estimate2022 > ???? | Area > ????)

ggplot(???) +
  geom_point()  +
  geom_smooth() +
  ggrepel::geom_label_repel(data = outliers, aes(label = City))
```


2. Use SelectorGadget and the `rvest` package to pull off data from the top 50 grossing films from 2018. Generate a tibble that contains the title, running time, star rating, and metascore for the top 50 films.  You'll have to clean your tibble so that columns are tidy and plot-worthy, titles include no extra characters, running time is in minutes, etc.

Create a scatterplot of star rating versus metascore.  Identify movies that had big discrepancies between reviewers (metascore) and viewers (star rating).

For "bonus points": do longer movies tend to have higher ratings?  Answer with an appropriate plot.


3. Use the `httr` package to build a data set from the omdb API with a different set of movies and a different set of variables than we used earlier.  If you're feeling ambitious, tidy up the resulting data set.
