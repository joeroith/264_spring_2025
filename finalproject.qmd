---
title: "Final Project: Shiny App"
format:
  html: default
---

## Overview

In this project, you will find data on the web, scrape it, tidy it, visualize it, and then publish it to tell a data-driven story.  


## Groups

TBD – I will assign pairs with input from you.  In certain cases, I may approve a group of 3 if you can convince me all 3 will contribute fully to a product that exceeds what a pair could do


## Timeline

|                               | Tentative Due Date	|Points |
|-------------------------------|---------------------|------:|
|Stage 0: Partner Preferences		|	Thurs Apr 25		    | 1     |
|Stage I: Proposal			        |	Wed May 1		        | 8     |
|Stage II: Progress Report	    |	Wed May 8		        | 8     |
|Stage III:	Peer Review				  | Tues May 14		      | 8     |
|Stage IV: Project Submission		|	Sat May 18		      | 75    |
|-------------------------------|---------------------|-------|
|Total									        |                     |100    |


## Key definitions

- “*Scrape it*”.  You can find a site with an API and acquire data with httr; you can find data on a website, in table form or scattered, and acquire it with rvest; or, you can find multiple files in a format like .csv and merge them together with tidyr.  You cannot start with a single, nicely-formatted file.
- “*Tidy it*”.  Create a tidy tibble that will allow you to perform desired analyses.  This might involve stringr, mutate, filter, spread, parse_, etc.
- “*Visualize it*”.  Create plots using ggplot() that effectively tell a story about what insights can be gained from your data.
- “*Publish it*”.  This should take the form of a Shiny app published with GitHub Pages (or possibly somewhere like shinyapps.io or rconnect.stolaf.edu).


## Examples of good past projects

See Project folder on the RStudio server.


## Stage I:  Proposal

A max two-page document describing:

- Website(s) you plan to scrape or data sets you plan to merge
- Variables you plan to acquire, including variables you plan to generate from your raw data
- Questions you plan to address
- Format of your final product and visualizations you envision (ideally include photos of plot sketches here).


## Stage II:  Progress Report

A one-page document (not including sketches) describing:

- An update on how your data scraping / acquisition is coming; you should have already put together your primary data set(s) well before May 8
- Updates to your proposal – what has changed in terms of websites scraped, variables acquired and generated, questions addressed, sketches of planned visualizations, and format of your final product
- Names of files and links where I can find your R code.  At a minimum, you should create 2 qmd files – one for scraping and/or creating a tidy data set (that gets written using write_csv) and one doing plots and analyses (after reading tidy data in with read_csv).


## Stage III:  Peer Review

Come prepared to share a draft version of your final product with another team during class on Tuesday, May 14.  As a reviewer, come prepared to offer comments on how the plots you see could be improved to more effectively convey messages in the data.  You might also be called on to share R code or hints on how to modify R code to achieve certain goals.


## Stage IV:  Project Submission

Your submission (one per group) will be simply a link to a Shiny app or website containing your final product, along with directions for accessing your source code in a public GitHub repo.  Your final submission can also have section on “tribulations and victories of data acquisition and wrangling” – i.e., data wrangling challenges and how you overcame them.

Your project score will be based on the quality of your team’s Final Product, and your individual contribution to your team, as assessed by all team members (and me).  A high quality Final Product will show your abilities to scrape, tidy, and visualize data; illustrate creativity and strong effort; and tell a compelling story with excellent written descriptions and graphics.

